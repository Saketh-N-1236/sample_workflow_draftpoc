`code.txt`

```

```

`deterministic/.env`

```
# Database Configuration
# Fill in your actual database credentials below

# PostgreSQL Host (usually localhost)
DB_HOST=localhost

# PostgreSQL Port (default is 5432)
DB_PORT=5432

# Database Name (planon as specified)
DB_NAME=planon

# Schema Name (planon1 as specified)
DB_SCHEMA=planon1

# Database Username
# TODO: Replace with your actual PostgreSQL username
DB_USER=postgres

# Database Password
# TODO: Replace with your actual PostgreSQL password
DB_PASSWORD=root

```

`deterministic/01_create_tables.py`

```python
"""
Step 1: Create Database Tables

This script creates all necessary database tables in the planon1 schema.
It handles table creation, indexes, and foreign key constraints.

What it does:
1. Connects to PostgreSQL database (planon)
2. Creates schema if it doesn't exist
3. Creates all tables in planon1 schema:
   - test_registry (all tests)
   - test_dependencies (test -> production code)
   - reverse_index (production code -> tests)
   - test_metadata (test descriptions, markers)
   - test_structure (directory structure)
4. Creates indexes for fast queries
5. Sets up foreign key relationships
6. Displays creation progress
7. Verifies tables created successfully

Run this script:
    python deterministic/01_create_tables.py
"""

import sys
from pathlib import Path

# Add current directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from db_connection import get_connection, test_connection, get_db_config
from utils.output_formatter import print_header, print_section, print_item
import os
from dotenv import load_dotenv
from pathlib import Path

# Load environment variables to get schema
env_path = Path(__file__).parent / ".env"
if not env_path.exists():
    env_path = Path(__file__).parent.parent / ".env"
load_dotenv(env_path)

# Get schema from environment (same as db_connection.py)
SCHEMA = os.getenv('DB_SCHEMA', 'planon1')


def create_schema_if_not_exists(conn):
    """
    Create the schema if it doesn't exist.
    
    Args:
        conn: Database connection object
    
    This ensures the schema exists before creating tables.
    """
    with conn.cursor() as cursor:
        # Create schema if it doesn't exist
        cursor.execute(f"CREATE SCHEMA IF NOT EXISTS {SCHEMA}")
        conn.commit()
        print(f"[OK] Schema '{SCHEMA}' ready")


def create_test_registry_table(conn):
    """
    Create the test_registry table.
    
    This table stores all test information:
    - test_id: Unique identifier for each test
    - file_path: Path to the test file
    - class_name: Test class name (if any)
    - method_name: Test method name
    - test_type: Type of test (unit, integration, e2e)
    - line_number: Line number in file (optional)
    """
    with conn.cursor() as cursor:
        # Drop table if exists (for re-running)
        cursor.execute(f"DROP TABLE IF EXISTS {SCHEMA}.test_registry CASCADE")
        
        # Create table
        cursor.execute(f"""
            CREATE TABLE {SCHEMA}.test_registry (
                test_id VARCHAR(50) PRIMARY KEY,
                file_path TEXT NOT NULL,
                class_name VARCHAR(255),
                method_name VARCHAR(255) NOT NULL,
                test_type VARCHAR(50),
                line_number INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # Create indexes for fast lookups
        cursor.execute(f"""
            CREATE INDEX idx_test_registry_file 
            ON {SCHEMA}.test_registry(file_path)
        """)
        
        cursor.execute(f"""
            CREATE INDEX idx_test_registry_class 
            ON {SCHEMA}.test_registry(class_name)
        """)
        
        cursor.execute(f"""
            CREATE INDEX idx_test_registry_type 
            ON {SCHEMA}.test_registry(test_type)
        """)
        
        conn.commit()
        print(f"[OK] Created table: {SCHEMA}.test_registry")
        print(f"  - Primary key: test_id")
        print(f"  - Indexes: file_path, class_name, test_type")


def create_test_dependencies_table(conn):
    """
    Create the test_dependencies table.
    
    This table stores test-to-production-code mappings:
    - test_id: Foreign key to test_registry
    - referenced_class: Production class/module referenced by test
    - import_type: Type of import (direct, from_import, etc.)
    """
    with conn.cursor() as cursor:
        # Drop table if exists
        cursor.execute(f"DROP TABLE IF EXISTS {SCHEMA}.test_dependencies CASCADE")
        
        # Create table
        cursor.execute(f"""
            CREATE TABLE {SCHEMA}.test_dependencies (
                id SERIAL PRIMARY KEY,
                test_id VARCHAR(50) NOT NULL,
                referenced_class TEXT NOT NULL,
                import_type VARCHAR(50),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (test_id) REFERENCES {SCHEMA}.test_registry(test_id) ON DELETE CASCADE
            )
        """)
        
        # Create indexes
        cursor.execute(f"""
            CREATE INDEX idx_dependencies_test 
            ON {SCHEMA}.test_dependencies(test_id)
        """)
        
        cursor.execute(f"""
            CREATE INDEX idx_dependencies_class 
            ON {SCHEMA}.test_dependencies(referenced_class)
        """)
        
        conn.commit()
        print(f"[OK] Created table: {SCHEMA}.test_dependencies")
        print(f"  - Foreign key: test_id -> test_registry")
        print(f"  - Indexes: test_id, referenced_class")


def create_reverse_index_table(conn):
    """
    Create the reverse_index table.
    
    This table stores production-code-to-tests mappings for fast lookup:
    - production_class: Production class/module name
    - test_id: Foreign key to test_registry
    - test_file_path: Path to test file (denormalized for performance)
    """
    with conn.cursor() as cursor:
        # Drop table if exists
        cursor.execute(f"DROP TABLE IF EXISTS {SCHEMA}.reverse_index CASCADE")
        
        # Create table
        cursor.execute(f"""
            CREATE TABLE {SCHEMA}.reverse_index (
                id SERIAL PRIMARY KEY,
                production_class TEXT NOT NULL,
                test_id VARCHAR(50) NOT NULL,
                test_file_path TEXT,
                reference_type VARCHAR(20) DEFAULT 'direct_import',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (test_id) REFERENCES {SCHEMA}.test_registry(test_id) ON DELETE CASCADE
            )
        """)
        
        # Create indexes for fast lookups
        cursor.execute(f"""
            CREATE INDEX idx_reverse_class 
            ON {SCHEMA}.reverse_index(production_class)
        """)
        
        cursor.execute(f"""
            CREATE INDEX idx_reverse_test 
            ON {SCHEMA}.reverse_index(test_id)
        """)
        
        # Composite index for common query pattern
        cursor.execute(f"""
            CREATE INDEX idx_reverse_class_test 
            ON {SCHEMA}.reverse_index(production_class, test_id)
        """)
        
        # Index for reference_type to support filtering
        cursor.execute(f"""
            CREATE INDEX idx_reverse_reference_type 
            ON {SCHEMA}.reverse_index(reference_type)
        """)
        
        conn.commit()
        print(f"[OK] Created table: {SCHEMA}.reverse_index")
        print(f"  - Foreign key: test_id -> test_registry")
        print(f"  - Columns: production_class, test_id, reference_type")
        print(f"  - Indexes: production_class, test_id, reference_type, (production_class, test_id)")


def create_test_metadata_table(conn):
    """
    Create the test_metadata table.
    
    This table stores test metadata:
    - test_id: Foreign key to test_registry (unique)
    - description: Test description/docstring
    - markers: Pytest markers (stored as JSONB)
    - is_async: Whether test is async
    - is_parameterized: Whether test is parameterized
    - pattern: Test naming pattern
    """
    with conn.cursor() as cursor:
        # Drop table if exists
        cursor.execute(f"DROP TABLE IF EXISTS {SCHEMA}.test_metadata CASCADE")
        
        # Create table
        cursor.execute(f"""
            CREATE TABLE {SCHEMA}.test_metadata (
                id SERIAL PRIMARY KEY,
                test_id VARCHAR(50) UNIQUE NOT NULL,
                description TEXT,
                markers JSONB,
                is_async BOOLEAN DEFAULT FALSE,
                is_parameterized BOOLEAN DEFAULT FALSE,
                pattern VARCHAR(50),
                line_number INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (test_id) REFERENCES {SCHEMA}.test_registry(test_id) ON DELETE CASCADE
            )
        """)
        
        # Create indexes
        cursor.execute(f"""
            CREATE INDEX idx_metadata_test 
            ON {SCHEMA}.test_metadata(test_id)
        """)
        
        cursor.execute(f"""
            CREATE INDEX idx_metadata_pattern 
            ON {SCHEMA}.test_metadata(pattern)
        """)
        
        # GIN index for JSONB markers (for fast JSON queries)
        cursor.execute(f"""
            CREATE INDEX idx_metadata_markers 
            ON {SCHEMA}.test_metadata USING GIN (markers)
        """)
        
        conn.commit()
        print(f"[OK] Created table: {SCHEMA}.test_metadata")
        print(f"  - Foreign key: test_id -> test_registry (unique)")
        print(f"  - Indexes: test_id, pattern, markers (GIN)")


def create_test_structure_table(conn):
    """
    Create the test_structure table.
    
    This table stores test repository structure information:
    - directory_path: Path to directory
    - category: Test category (unit, integration, e2e)
    - file_count: Number of files in directory
    - total_lines: Total lines of code
    """
    with conn.cursor() as cursor:
        # Drop table if exists
        cursor.execute(f"DROP TABLE IF EXISTS {SCHEMA}.test_structure CASCADE")
        
        # Create table
        cursor.execute(f"""
            CREATE TABLE {SCHEMA}.test_structure (
                id SERIAL PRIMARY KEY,
                directory_path TEXT NOT NULL,
                category VARCHAR(50),
                file_count INTEGER,
                total_lines INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # Create indexes
        cursor.execute(f"""
            CREATE INDEX idx_structure_category 
            ON {SCHEMA}.test_structure(category)
        """)
        
        cursor.execute(f"""
            CREATE INDEX idx_structure_path 
            ON {SCHEMA}.test_structure(directory_path)
        """)
        
        conn.commit()
        print(f"[OK] Created table: {SCHEMA}.test_structure")
        print(f"  - Indexes: category, directory_path")


def create_test_function_mapping_table(conn):
    """
    Create the test_function_mapping table.
    
    This table stores function-level mappings:
    - test_id: Foreign key to test_registry
    - module_name: Production module name (e.g., agent.langgraph_agent)
    - function_name: Function name (e.g., initialize)
    - call_type: Type of call (direct_call, method_call, patch_ref)
    - source: Source of mapping (method_call, patch_ref)
    """
    with conn.cursor() as cursor:
        # Drop table if exists
        cursor.execute(f"DROP TABLE IF EXISTS {SCHEMA}.test_function_mapping CASCADE")
        
        # Create table
        cursor.execute(f"""
            CREATE TABLE {SCHEMA}.test_function_mapping (
                id SERIAL PRIMARY KEY,
                test_id VARCHAR(50) NOT NULL,
                module_name TEXT NOT NULL,
                function_name VARCHAR(255) NOT NULL,
                call_type VARCHAR(50),
                source VARCHAR(50),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (test_id) REFERENCES {SCHEMA}.test_registry(test_id) ON DELETE CASCADE
            )
        """)
        
        # Create composite index for fast lookups (module_name, function_name)
        cursor.execute(f"""
            CREATE INDEX idx_func_mapping_module_func 
            ON {SCHEMA}.test_function_mapping(module_name, function_name)
        """)
        
        # Create index on test_id for reverse lookups
        cursor.execute(f"""
            CREATE INDEX idx_func_mapping_test 
            ON {SCHEMA}.test_function_mapping(test_id)
        """)
        
        # Create index on function_name alone for broader searches
        cursor.execute(f"""
            CREATE INDEX idx_func_mapping_function 
            ON {SCHEMA}.test_function_mapping(function_name)
        """)
        
        conn.commit()
        print(f"[OK] Created table: {SCHEMA}.test_function_mapping")
        print(f"  - Foreign key: test_id -> test_registry")
        print(f"  - Indexes: (module_name, function_name), test_id, function_name")


def verify_tables_created(conn):
    """
    Verify all tables were created successfully.
    
    Args:
        conn: Database connection object
    
    Returns:
        Dictionary with table verification results
    """
    with conn.cursor() as cursor:
        # Query to get all tables in schema
        cursor.execute("""
            SELECT table_name 
            FROM information_schema.tables 
            WHERE table_schema = %s
            ORDER BY table_name
        """, (SCHEMA,))
        
        tables = [row[0] for row in cursor.fetchall()]
        
        expected_tables = [
            'test_registry',
            'test_dependencies',
            'reverse_index',
            'test_metadata',
            'test_structure',
            'test_function_mapping'
        ]
        
        result = {
            'tables_found': tables,
            'expected_tables': expected_tables,
            'all_present': all(table in tables for table in expected_tables),
            'missing_tables': [t for t in expected_tables if t not in tables]
        }
        
        return result


def main():
    """Main function to create all database tables."""
    print_header("Step 1: Creating Database Tables")
    print()
    
    # Test connection first
    print_section("Testing database connection...")
    if not test_connection():
        print()
        print("ERROR: Cannot connect to database!")
        print("Please check your .env file and database configuration.")
        return
    
    print()
    
    try:
        with get_connection() as conn:
            # Step 1: Create schema if needed
            print_section("Creating schema if needed...")
            create_schema_if_not_exists(conn)
            print()
            
            # Step 2: Create all tables
            print_section("Creating tables...")
            print()
            
            create_test_registry_table(conn)
            print()
            
            create_test_dependencies_table(conn)
            print()
            
            create_reverse_index_table(conn)
            print()
            
            create_test_metadata_table(conn)
            print()
            
            create_test_structure_table(conn)
            print()
            
            create_test_function_mapping_table(conn)
            print()
            
            # Step 3: Verify tables
            print_section("Verifying tables...")
            verification = verify_tables_created(conn)
            
            if verification['all_present']:
                print("[OK] All tables created successfully!")
                print()
                print("Tables created:")
                for table in verification['tables_found']:
                    print(f"  - {SCHEMA}.{table}")
            else:
                print("[ERROR] Some tables are missing!")
                print(f"Missing: {verification['missing_tables']}")
                return
            
            print()
            print_header("Step 1 Complete!")
            print(f"All tables created in schema: {SCHEMA}")
            print("Ready to load data in next steps.")
            
    except Exception as e:
        print(f"ERROR: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

```

`deterministic/02_load_test_registry.py`

```python
"""
Step 2: Load Test Registry

This script loads test registry data from the test analysis JSON output
into the PostgreSQL database.

What it does:
1. Reads test registry data from test_analysis/outputs/03_test_registry.json
2. Connects to PostgreSQL database
3. Inserts all tests into test_registry table
4. Handles duplicates (updates if test_id already exists)
5. Displays loading progress
6. Shows summary statistics

Run this script:
    python deterministic/02_load_test_registry.py
"""

import sys
import json
from pathlib import Path

# Add current directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from db_connection import get_connection, test_connection
from utils.db_helpers import batch_insert_test_registry, count_table_records
from utils.output_formatter import print_header, print_section, print_item

# Path to test registry JSON file
TEST_ANALYSIS_DIR = Path(__file__).parent.parent / "test_analysis" / "outputs"
TEST_REGISTRY_FILE = TEST_ANALYSIS_DIR / "03_test_registry.json"


def load_test_registry_json() -> dict:
    """
    Load test registry data from JSON file.
    
    Returns:
        Dictionary with test registry data
    
    Raises:
        FileNotFoundError: If JSON file doesn't exist
        json.JSONDecodeError: If JSON is invalid
    """
    if not TEST_REGISTRY_FILE.exists():
        raise FileNotFoundError(
            f"Test registry file not found: {TEST_REGISTRY_FILE}\n"
            f"Please run test_analysis/03_build_test_registry.py first."
        )
    
    with open(TEST_REGISTRY_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
        # Extract data from the nested structure
        return data.get('data', data)


def prepare_test_data(registry_data: dict) -> list:
    """
    Prepare test data for database insertion.
    
    Args:
        registry_data: Dictionary from JSON file
    
    Returns:
        List of test dictionaries ready for database insertion
    """
    tests = registry_data.get('tests', [])
    
    # Ensure all required fields are present
    prepared_tests = []
    for test in tests:
        prepared_test = {
            'test_id': test['test_id'],
            'file_path': test['file_path'],
            'class_name': test.get('class_name'),
            'method_name': test['method_name'],
            'test_type': test.get('test_type'),
            'line_number': test.get('line_number')
        }
        prepared_tests.append(prepared_test)
    
    return prepared_tests


def load_tests_to_database(conn, tests: list, batch_size: int = 50) -> dict:
    """
    Load tests into database in batches.
    
    Args:
        conn: Database connection
        tests: List of test dictionaries
        batch_size: Number of tests to insert per batch (default: 50)
    
    Returns:
        Dictionary with loading statistics
    """
    total_tests = len(tests)
    loaded_count = 0
    failed_count = 0
    
    print_section(f"Loading {total_tests} tests in batches of {batch_size}...")
    print()
    
    # Process in batches
    for i in range(0, total_tests, batch_size):
        batch = tests[i:i + batch_size]
        batch_num = (i // batch_size) + 1
        total_batches = (total_tests + batch_size - 1) // batch_size
        
        print_progress(i + len(batch), total_tests, "tests")
        
        # Insert batch
        inserted = batch_insert_test_registry(conn, batch)
        
        if inserted == len(batch):
            loaded_count += inserted
        else:
            failed_count += (len(batch) - inserted)
            loaded_count += inserted
    
    print()  # New line after progress
    
    return {
        'total': total_tests,
        'loaded': loaded_count,
        'failed': failed_count
    }


def print_progress(current: int, total: int, item_name: str = "items") -> None:
    """
    Print a progress indicator.
    
    Args:
        current: Current item number
        total: Total number of items
        item_name: Name of the items being processed
    """
    percentage = (current / total * 100) if total > 0 else 0
    print(f"Processing: {current}/{total} {item_name} ({percentage:.1f}%)", end='\r')
    
    # Print newline when complete
    if current == total:
        print()  # Move to next line


def main():
    """Main function to load test registry."""
    print_header("Step 2: Loading Test Registry")
    print()
    
    # Step 1: Test database connection
    print_section("Testing database connection...")
    if not test_connection():
        print()
        print("ERROR: Cannot connect to database!")
        return
    print()
    
    # Step 2: Load JSON data
    print_section("Loading test registry data from JSON...")
    try:
        registry_data = load_test_registry_json()
        print_item("JSON file loaded:", str(TEST_REGISTRY_FILE))
        print_item("Total tests in file:", registry_data.get('total_tests', 0))
    except FileNotFoundError as e:
        print(f"ERROR: {e}")
        return
    except json.JSONDecodeError as e:
        print(f"ERROR: Invalid JSON file: {e}")
        return
    except Exception as e:
        print(f"ERROR: {e}")
        return
    
    print()
    
    # Step 3: Prepare test data
    print_section("Preparing test data...")
    tests = prepare_test_data(registry_data)
    print_item("Tests prepared:", len(tests))
    print()
    
    # Step 4: Load into database
    try:
        with get_connection() as conn:
            # Check current count
            initial_count = count_table_records(conn, "test_registry")
            print_item("Tests in database (before):", initial_count)
            print()
            
            # Load tests
            stats = load_tests_to_database(conn, tests)
            
            # Check final count
            final_count = count_table_records(conn, "test_registry")
            print()
            
            # Step 5: Display summary
            print_section("Loading Summary:")
            print_item("Total tests in JSON:", stats['total'])
            print_item("Successfully loaded:", stats['loaded'])
            if stats['failed'] > 0:
                print_item("Failed to load:", stats['failed'])
            print_item("Tests in database (after):", final_count)
            print()
            
            # Step 6: Display sample tests
            print_section("Sample Tests (first 5):")
            with conn.cursor() as cursor:
                cursor.execute("""
                    SELECT test_id, class_name, method_name, test_type
                    FROM test_registry
                    ORDER BY test_id
                    LIMIT 5
                """)
                for row in cursor.fetchall():
                    test_id, class_name, method_name, test_type = row
                    test_desc = f"{class_name}.{method_name}" if class_name else method_name
                    print_item(f"{test_id}:", f"{test_desc} ({test_type})")
            print()
            
            print_header("Step 2 Complete!")
            print(f"Loaded {stats['loaded']} tests into database")
            
    except Exception as e:
        print(f"ERROR: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

```

`deterministic/03_load_dependencies.py`

```python
"""
Step 3: Load Test Dependencies

This script loads test dependencies (test → production code mappings)
from the test analysis JSON output into the PostgreSQL database.

What it does:
1. Reads dependency data from test_analysis/outputs/04_static_dependencies.json
2. Connects to PostgreSQL database
3. Inserts test dependencies into test_dependencies table
4. Links dependencies to test_registry via test_id
5. Displays loading progress and statistics

Run this script:
    python deterministic/03_load_dependencies.py
"""

import sys
import json
from pathlib import Path

# Add current directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from db_connection import get_connection, test_connection
from utils.db_helpers import batch_insert_test_dependencies, count_table_records
from utils.output_formatter import print_header, print_section, print_item

# Path to dependencies JSON file
TEST_ANALYSIS_DIR = Path(__file__).parent.parent / "test_analysis" / "outputs"
DEPENDENCIES_FILE = TEST_ANALYSIS_DIR / "04_static_dependencies.json"


def load_dependencies_json() -> dict:
    """
    Load test dependencies data from JSON file.
    
    Returns:
        Dictionary with dependency data
    
    Raises:
        FileNotFoundError: If JSON file doesn't exist
    """
    if not DEPENDENCIES_FILE.exists():
        raise FileNotFoundError(
            f"Dependencies file not found: {DEPENDENCIES_FILE}\n"
            f"Please run test_analysis/04_extract_static_dependencies.py first."
        )
    
    with open(DEPENDENCIES_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
        return data.get('data', data)


def prepare_dependency_data(dependencies_data: dict) -> list:
    """
    Prepare dependency data for database insertion.
    
    Args:
        dependencies_data: Dictionary from JSON file
    
    Returns:
        List of dependency dictionaries ready for database insertion
    """
    test_dependencies = dependencies_data.get('test_dependencies', [])
    
    # Flatten the dependencies: each test can have multiple referenced classes
    all_dependencies = []
    
    for test_dep in test_dependencies:
        test_id = test_dep['test_id']
        referenced_classes = test_dep.get('referenced_classes', [])
        
        # Create one dependency record per referenced class
        for ref_class in referenced_classes:
            dependency = {
                'test_id': test_id,
                'referenced_class': ref_class,
                'import_type': None  # Could be enhanced to detect import type
            }
            all_dependencies.append(dependency)
    
    return all_dependencies


def load_dependencies_to_database(conn, dependencies: list, batch_size: int = 100) -> dict:
    """
    Load dependencies into database in batches.
    
    Args:
        conn: Database connection
        dependencies: List of dependency dictionaries
        batch_size: Number of dependencies to insert per batch
    
    Returns:
        Dictionary with loading statistics
    """
    total_deps = len(dependencies)
    loaded_count = 0
    failed_count = 0
    
    print_section(f"Loading {total_deps} dependencies in batches of {batch_size}...")
    
    # Process in batches
    for i in range(0, total_deps, batch_size):
        batch = dependencies[i:i + batch_size]
        
        # Show progress
        current = min(i + batch_size, total_deps)
        percentage = (current / total_deps * 100) if total_deps > 0 else 0
        print(f"Processing: {current}/{total_deps} dependencies ({percentage:.1f}%)", end='\r')
        
        # Insert batch
        inserted = batch_insert_test_dependencies(conn, batch)
        
        if inserted == len(batch):
            loaded_count += inserted
        else:
            failed_count += (len(batch) - inserted)
            loaded_count += inserted
    
    print()  # New line after progress
    
    return {
        'total': total_deps,
        'loaded': loaded_count,
        'failed': failed_count
    }


def get_dependency_statistics(conn) -> dict:
    """
    Get statistics about loaded dependencies.
    
    Args:
        conn: Database connection
    
    Returns:
        Dictionary with statistics
    """
    with conn.cursor() as cursor:
        # Total dependencies
        cursor.execute("SELECT COUNT(*) FROM test_dependencies")
        total_deps = cursor.fetchone()[0]
        
        # Unique production classes referenced
        cursor.execute("SELECT COUNT(DISTINCT referenced_class) FROM test_dependencies")
        unique_classes = cursor.fetchone()[0]
        
        # Tests with dependencies
        cursor.execute("SELECT COUNT(DISTINCT test_id) FROM test_dependencies")
        tests_with_deps = cursor.fetchone()[0]
        
        # Most referenced classes
        cursor.execute("""
            SELECT referenced_class, COUNT(*) as count
            FROM test_dependencies
            GROUP BY referenced_class
            ORDER BY count DESC
            LIMIT 10
        """)
        top_classes = cursor.fetchall()
        
        return {
            'total_dependencies': total_deps,
            'unique_classes': unique_classes,
            'tests_with_dependencies': tests_with_deps,
            'top_classes': [{'class': row[0], 'count': row[1]} for row in top_classes]
        }


def main():
    """Main function to load test dependencies."""
    print_header("Step 3: Loading Test Dependencies")
    print()
    
    # Step 1: Test database connection
    print_section("Testing database connection...")
    if not test_connection():
        print()
        print("ERROR: Cannot connect to database!")
        return
    print()
    
    # Step 2: Load JSON data
    print_section("Loading dependencies data from JSON...")
    try:
        dependencies_data = load_dependencies_json()
        print_item("JSON file loaded:", str(DEPENDENCIES_FILE))
        print_item("Total tests with dependencies:", 
                  dependencies_data.get('tests_with_dependencies', 0))
    except FileNotFoundError as e:
        print(f"ERROR: {e}")
        return
    except Exception as e:
        print(f"ERROR: {e}")
        return
    
    print()
    
    # Step 3: Prepare dependency data
    print_section("Preparing dependency data...")
    dependencies = prepare_dependency_data(dependencies_data)
    print_item("Dependencies prepared:", len(dependencies))
    print()
    
    # Step 4: Load into database
    try:
        with get_connection() as conn:
            # Check current count
            initial_count = count_table_records(conn, "test_dependencies")
            print_item("Dependencies in database (before):", initial_count)
            print()
            
            # Load dependencies
            stats = load_dependencies_to_database(conn, dependencies)
            print()
            
            # Check final count
            final_count = count_table_records(conn, "test_dependencies")
            
            # Step 5: Get statistics
            dep_stats = get_dependency_statistics(conn)
            print()
            
            # Step 6: Display summary
            print_section("Loading Summary:")
            print_item("Total dependencies in JSON:", stats['total'])
            print_item("Successfully loaded:", stats['loaded'])
            if stats['failed'] > 0:
                print_item("Failed to load:", stats['failed'])
            print_item("Dependencies in database (after):", final_count)
            print()
            
            print_section("Dependency Statistics:")
            print_item("Unique production classes:", dep_stats['unique_classes'])
            print_item("Tests with dependencies:", dep_stats['tests_with_dependencies'])
            print()
            
            # Step 7: Display top referenced classes
            print_section("Most Referenced Production Classes (Top 10):")
            for i, class_info in enumerate(dep_stats['top_classes'], 1):
                print_item(f"{i}. {class_info['class']}:", f"{class_info['count']} references")
            print()
            
            print_header("Step 3 Complete!")
            print(f"Loaded {stats['loaded']} dependencies into database")
            
    except Exception as e:
        print(f"ERROR: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

```

`deterministic/04b_load_function_mappings.py`

```python
"""
Step 4b: Load Function Mappings

This script loads function-level mappings (test → module.function)
from the test analysis JSON output into the PostgreSQL database.

What it does:
1. Reads function mapping data from test_analysis/outputs/04b_function_calls.json
2. Connects to PostgreSQL database
3. Inserts function mappings into test_function_mapping table
4. Links to test_registry via test_id
5. Displays loading progress and statistics

The function mapping allows precise lookup: "Which tests call this specific function?"

Run this script:
    python deterministic/04b_load_function_mappings.py
"""

import sys
import json
from pathlib import Path

# Add current directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from db_connection import get_connection, test_connection
from utils.db_helpers import batch_insert_test_function_mapping, count_table_records
from utils.output_formatter import print_header, print_section, print_item

# Path to function calls JSON file
TEST_ANALYSIS_DIR = Path(__file__).parent.parent / "test_analysis" / "outputs"
FUNCTION_CALLS_FILE = TEST_ANALYSIS_DIR / "04b_function_calls.json"


def load_function_calls_json() -> dict:
    """
    Load function calls data from JSON file.
    
    Returns:
        Dictionary with function calls data
    
    Raises:
        FileNotFoundError: If JSON file doesn't exist
    """
    if not FUNCTION_CALLS_FILE.exists():
        raise FileNotFoundError(
            f"Function calls file not found: {FUNCTION_CALLS_FILE}\n"
            f"Please run test_analysis/04b_extract_function_calls.py first."
        )
    
    with open(FUNCTION_CALLS_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
        return data.get('data', data)


def prepare_function_mapping_data(function_calls_data: dict) -> list:
    """
    Prepare function mapping data for database insertion.
    
    Filters out mappings without module_name (can't query without module).
    
    Args:
        function_calls_data: Dictionary from JSON file
    
    Returns:
        List of function mapping dictionaries ready for database insertion
    """
    test_function_mappings = function_calls_data.get('test_function_mappings', [])
    
    # Filter to only include mappings with module_name (required for queries)
    valid_mappings = []
    
    for mapping in test_function_mappings:
        # Only include if we have both module_name and function_name
        if mapping.get('module_name') and mapping.get('function_name'):
            entry = {
                'test_id': mapping['test_id'],
                'module_name': mapping['module_name'],
                'function_name': mapping['function_name'],
                'call_type': mapping.get('call_type'),
                'source': mapping.get('source', 'method_call')
            }
            valid_mappings.append(entry)
    
    return valid_mappings


def load_function_mappings_to_database(conn, mappings: list, batch_size: int = 100) -> dict:
    """
    Load function mappings into database in batches.
    
    Args:
        conn: Database connection
        mappings: List of function mapping dictionaries
        batch_size: Number of mappings to insert per batch
    
    Returns:
        Dictionary with loading statistics
    """
    total_mappings = len(mappings)
    loaded_count = 0
    failed_count = 0
    
    print_section(f"Loading {total_mappings} function mappings in batches of {batch_size}...")
    
    # Process in batches
    for i in range(0, total_mappings, batch_size):
        batch = mappings[i:i + batch_size]
        
        # Show progress
        current = min(i + batch_size, total_mappings)
        percentage = (current / total_mappings * 100) if total_mappings > 0 else 0
        print(f"Processing: {current}/{total_mappings} mappings ({percentage:.1f}%)", end='\r')
        
        # Insert batch
        inserted = batch_insert_test_function_mapping(conn, batch)
        
        if inserted == len(batch):
            loaded_count += inserted
        else:
            failed_count += (len(batch) - inserted)
            loaded_count += inserted
    
    print()  # New line after progress
    
    return {
        'total': total_mappings,
        'loaded': loaded_count,
        'failed': failed_count
    }


def get_function_mapping_statistics(conn) -> dict:
    """
    Get statistics about loaded function mappings.
    
    Args:
        conn: Database connection
    
    Returns:
        Dictionary with statistics
    """
    with conn.cursor() as cursor:
        # Total mappings
        cursor.execute("SELECT COUNT(*) FROM test_function_mapping")
        total_mappings = cursor.fetchone()[0]
        
        # Unique tests
        cursor.execute("SELECT COUNT(DISTINCT test_id) FROM test_function_mapping")
        unique_tests = cursor.fetchone()[0]
        
        # Unique module.function combinations
        cursor.execute("""
            SELECT COUNT(DISTINCT module_name || '.' || function_name) 
            FROM test_function_mapping
        """)
        unique_functions = cursor.fetchone()[0]
        
        # Most called functions
        cursor.execute("""
            SELECT module_name, function_name, COUNT(*) as test_count
            FROM test_function_mapping
            GROUP BY module_name, function_name
            ORDER BY test_count DESC
            LIMIT 10
        """)
        top_functions = cursor.fetchall()
        
        # Count by source type
        cursor.execute("""
            SELECT source, COUNT(*) as count
            FROM test_function_mapping
            GROUP BY source
        """)
        by_source = cursor.fetchall()
        
        return {
            'total_mappings': total_mappings,
            'unique_tests': unique_tests,
            'unique_functions': unique_functions,
            'top_functions': [
                {'module': row[0], 'function': row[1], 'test_count': row[2]} 
                for row in top_functions
            ],
            'by_source': {row[0]: row[1] for row in by_source}
        }


def main():
    """Main function to load function mappings."""
    print_header("Step 4b: Loading Function Mappings")
    print()
    
    # Step 1: Test database connection
    print_section("Testing database connection...")
    if not test_connection():
        print()
        print("ERROR: Cannot connect to database!")
        return
    print()
    
    # Step 2: Load JSON data
    print_section("Loading function calls data from JSON...")
    try:
        function_calls_data = load_function_calls_json()
        print_item("JSON file loaded:", str(FUNCTION_CALLS_FILE))
        print_item("Total mappings in file:", 
                  function_calls_data.get('total_mappings', 0))
    except FileNotFoundError as e:
        print(f"ERROR: {e}")
        return
    except Exception as e:
        print(f"ERROR: {e}")
        return
    
    print()
    
    # Step 3: Prepare function mapping data
    print_section("Preparing function mapping data...")
    mappings = prepare_function_mapping_data(function_calls_data)
    print_item("Valid function mappings prepared:", len(mappings))
    print_item("(Filtered out mappings without module_name)")
    print()
    
    # Step 4: Load into database
    try:
        with get_connection() as conn:
            # Check current count
            initial_count = count_table_records(conn, "test_function_mapping")
            print_item("Function mappings in database (before):", initial_count)
            print()
            
            # Load function mappings
            stats = load_function_mappings_to_database(conn, mappings)
            print()
            
            # Check final count
            final_count = count_table_records(conn, "test_function_mapping")
            
            # Step 5: Get statistics
            func_stats = get_function_mapping_statistics(conn)
            print()
            
            # Step 6: Display summary
            print_section("Loading Summary:")
            print_item("Total mappings in JSON:", stats['total'])
            print_item("Successfully loaded:", stats['loaded'])
            if stats['failed'] > 0:
                print_item("Failed to load:", stats['failed'])
            print_item("Mappings in database (after):", final_count)
            print()
            
            print_section("Function Mapping Statistics:")
            print_item("Unique tests with function calls:", func_stats['unique_tests'])
            print_item("Unique module.function combinations:", func_stats['unique_functions'])
            print_item("Average mappings per test:", 
                      round(func_stats['total_mappings'] / func_stats['unique_tests'], 2) 
                      if func_stats['unique_tests'] > 0 else 0)
            print()
            
            # Step 7: Display by source
            if func_stats['by_source']:
                print_section("Mappings by Source Type:")
                for source, count in sorted(func_stats['by_source'].items(), key=lambda x: x[1], reverse=True):
                    print_item(f"{source}:", count)
                print()
            
            # Step 8: Display top functions
            print_section("Most Called Functions (Top 10):")
            for i, func_info in enumerate(func_stats['top_functions'], 1):
                func_name = f"{func_info['module']}.{func_info['function']}"
                print_item(f"{i}. {func_name}:", f"{func_info['test_count']} tests")
            print()
            
            # Step 9: Test a sample query
            print_section("Testing Function Mapping Query:")
            if func_stats['top_functions']:
                sample_func = func_stats['top_functions'][0]
                with conn.cursor() as cursor:
                    cursor.execute("""
                        SELECT DISTINCT tr.test_id, tr.class_name, tr.method_name
                        FROM test_function_mapping tfm
                        JOIN test_registry tr ON tfm.test_id = tr.test_id
                        WHERE tfm.module_name = %s
                        AND tfm.function_name = %s
                        LIMIT 5
                    """, (sample_func['module'], sample_func['function']))
                    sample_tests = cursor.fetchall()
                    func_name = f"{sample_func['module']}.{sample_func['function']}"
                    print_item(f"Sample: Tests for '{func_name}':", len(sample_tests))
                    if sample_tests:
                        test_desc = f"{sample_tests[0][1]}.{sample_tests[0][2]}" if sample_tests[0][1] else sample_tests[0][2]
                        print_item("  First test:", test_desc)
            print()
            
            print_header("Step 4b Complete!")
            print(f"Loaded {stats['loaded']} function mappings into database")
            print("Function mappings are ready for precise function-level test selection!")
            
    except Exception as e:
        print(f"ERROR: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

```

`deterministic/04_load_reverse_index.py`

```python
"""
Step 4: Load Reverse Index

This script loads the reverse index (production code → tests mapping)
from the test analysis JSON output into the PostgreSQL database.

What it does:
1. Reads reverse index data from test_analysis/outputs/06_reverse_index.json
2. Connects to PostgreSQL database
3. Inserts reverse index entries into reverse_index table
4. Links to test_registry via test_id
5. Creates indexes for fast queries
6. Displays loading progress and statistics

The reverse index allows fast lookup: "Which tests reference this production class?"

Run this script:
    python deterministic/04_load_reverse_index.py
"""

import sys
import json
from pathlib import Path

# Add current directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from db_connection import get_connection, test_connection
from utils.db_helpers import batch_insert_reverse_index, count_table_records
from utils.output_formatter import print_header, print_section, print_item

# Path to reverse index JSON file
TEST_ANALYSIS_DIR = Path(__file__).parent.parent / "test_analysis" / "outputs"
REVERSE_INDEX_FILE = TEST_ANALYSIS_DIR / "06_reverse_index.json"


def load_reverse_index_json() -> dict:
    """
    Load reverse index data from JSON file.
    
    Returns:
        Dictionary with reverse index data
    
    Raises:
        FileNotFoundError: If JSON file doesn't exist
    """
    if not REVERSE_INDEX_FILE.exists():
        raise FileNotFoundError(
            f"Reverse index file not found: {REVERSE_INDEX_FILE}\n"
            f"Please run test_analysis/06_build_reverse_index.py first."
        )
    
    with open(REVERSE_INDEX_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
        return data.get('data', data)


def prepare_reverse_index_data(reverse_index_data: dict) -> list:
    """
    Prepare reverse index data for database insertion.
    
    Args:
        reverse_index_data: Dictionary from JSON file
    
    Returns:
        List of reverse index dictionaries ready for database insertion
    """
    reverse_index = reverse_index_data.get('reverse_index', {})
    
    # Flatten the reverse index: each production class maps to multiple tests
    all_entries = []
    
    for production_class, tests in reverse_index.items():
        # Each test in the list
        for test_info in tests:
            entry = {
                'production_class': production_class,
                'test_id': test_info['test_id'],
                'test_file_path': test_info.get('file_path'),  # Denormalized for performance
                'reference_type': test_info.get('reference_type', 'direct_import')  # NEW: Include reference type
            }
            all_entries.append(entry)
    
    return all_entries


def load_reverse_index_to_database(conn, entries: list, batch_size: int = 100) -> dict:
    """
    Load reverse index entries into database in batches.
    
    Args:
        conn: Database connection
        entries: List of reverse index dictionaries
        batch_size: Number of entries to insert per batch
    
    Returns:
        Dictionary with loading statistics
    """
    total_entries = len(entries)
    loaded_count = 0
    failed_count = 0
    
    print_section(f"Loading {total_entries} reverse index entries in batches of {batch_size}...")
    
    # Process in batches
    for i in range(0, total_entries, batch_size):
        batch = entries[i:i + batch_size]
        
        # Show progress
        current = min(i + batch_size, total_entries)
        percentage = (current / total_entries * 100) if total_entries > 0 else 0
        print(f"Processing: {current}/{total_entries} entries ({percentage:.1f}%)", end='\r')
        
        # Insert batch
        inserted = batch_insert_reverse_index(conn, batch)
        
        if inserted == len(batch):
            loaded_count += inserted
        else:
            failed_count += (len(batch) - inserted)
            loaded_count += inserted
    
    print()  # New line after progress
    
    return {
        'total': total_entries,
        'loaded': loaded_count,
        'failed': failed_count
    }


def get_reverse_index_statistics(conn) -> dict:
    """
    Get statistics about loaded reverse index.
    
    Args:
        conn: Database connection
    
    Returns:
        Dictionary with statistics
    """
    with conn.cursor() as cursor:
        # Total entries
        cursor.execute("SELECT COUNT(*) FROM reverse_index")
        total_entries = cursor.fetchone()[0]
        
        # Unique production classes
        cursor.execute("SELECT COUNT(DISTINCT production_class) FROM reverse_index")
        unique_classes = cursor.fetchone()[0]
        
        # Unique tests
        cursor.execute("SELECT COUNT(DISTINCT test_id) FROM reverse_index")
        unique_tests = cursor.fetchone()[0]
        
        # Most referenced classes (classes with most tests)
        cursor.execute("""
            SELECT production_class, COUNT(*) as test_count
            FROM reverse_index
            GROUP BY production_class
            ORDER BY test_count DESC
            LIMIT 10
        """)
        top_classes = cursor.fetchall()
        
        return {
            'total_entries': total_entries,
            'unique_production_classes': unique_classes,
            'unique_tests': unique_tests,
            'top_classes': [{'class': row[0], 'test_count': row[1]} for row in top_classes]
        }


def main():
    """Main function to load reverse index."""
    print_header("Step 4: Loading Reverse Index")
    print()
    
    # Step 1: Test database connection
    print_section("Testing database connection...")
    if not test_connection():
        print()
        print("ERROR: Cannot connect to database!")
        return
    print()
    
    # Step 2: Load JSON data
    print_section("Loading reverse index data from JSON...")
    try:
        reverse_index_data = load_reverse_index_json()
        print_item("JSON file loaded:", str(REVERSE_INDEX_FILE))
        print_item("Total production classes:", 
                  reverse_index_data.get('total_production_classes', 0))
    except FileNotFoundError as e:
        print(f"ERROR: {e}")
        return
    except Exception as e:
        print(f"ERROR: {e}")
        return
    
    print()
    
    # Step 3: Prepare reverse index data
    print_section("Preparing reverse index data...")
    entries = prepare_reverse_index_data(reverse_index_data)
    print_item("Reverse index entries prepared:", len(entries))
    print()
    
    # Step 4: Load into database
    try:
        with get_connection() as conn:
            # Check current count
            initial_count = count_table_records(conn, "reverse_index")
            print_item("Reverse index entries in database (before):", initial_count)
            print()
            
            # Load reverse index
            stats = load_reverse_index_to_database(conn, entries)
            print()
            
            # Check final count
            final_count = count_table_records(conn, "reverse_index")
            
            # Step 5: Get statistics
            rev_stats = get_reverse_index_statistics(conn)
            print()
            
            # Step 6: Display summary
            print_section("Loading Summary:")
            print_item("Total entries in JSON:", stats['total'])
            print_item("Successfully loaded:", stats['loaded'])
            if stats['failed'] > 0:
                print_item("Failed to load:", stats['failed'])
            print_item("Entries in database (after):", final_count)
            print()
            
            print_section("Reverse Index Statistics:")
            print_item("Unique production classes:", rev_stats['unique_production_classes'])
            print_item("Unique tests:", rev_stats['unique_tests'])
            print_item("Average tests per class:", 
                      round(rev_stats['total_entries'] / rev_stats['unique_production_classes'], 2) 
                      if rev_stats['unique_production_classes'] > 0 else 0)
            print()
            
            # Step 7: Display top classes
            print_section("Most Referenced Production Classes (Top 10):")
            for i, class_info in enumerate(rev_stats['top_classes'], 1):
                print_item(f"{i}. {class_info['class']}:", 
                          f"{class_info['test_count']} tests")
            print()
            
            # Step 8: Test a sample query
            print_section("Testing Reverse Index Query:")
            if rev_stats['top_classes']:
                sample_class = rev_stats['top_classes'][0]['class']
                from utils.db_helpers import get_tests_for_production_class
                sample_tests = get_tests_for_production_class(conn, sample_class)
                print_item(f"Sample: Tests for '{sample_class}':", len(sample_tests))
                if sample_tests:
                    print_item("  First test:", 
                              f"{sample_tests[0]['class_name']}.{sample_tests[0]['method_name']}" 
                              if sample_tests[0]['class_name'] 
                              else sample_tests[0]['method_name'])
            print()
            
            print_header("Step 4 Complete!")
            print(f"Loaded {stats['loaded']} reverse index entries into database")
            print("Reverse index is ready for fast code → tests lookups!")
            
    except Exception as e:
        print(f"ERROR: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

```

`deterministic/05_load_metadata.py`

```python
"""
Step 5: Load Test Metadata

This script loads test metadata (descriptions, markers, patterns)
from the test analysis JSON output into the PostgreSQL database.

What it does:
1. Reads metadata from test_analysis/outputs/05_test_metadata.json
2. Connects to PostgreSQL database
3. Inserts test metadata into test_metadata table
4. Stores markers as JSONB for flexible querying
5. Links to test_registry via test_id
6. Displays loading progress and statistics

Run this script:
    python deterministic/05_load_metadata.py
"""

import sys
import json
from pathlib import Path

# Add current directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from db_connection import get_connection, test_connection
from utils.db_helpers import count_table_records
from utils.output_formatter import print_header, print_section, print_item
from psycopg2.extras import execute_values

# Path to metadata JSON file
TEST_ANALYSIS_DIR = Path(__file__).parent.parent / "test_analysis" / "outputs"
METADATA_FILE = TEST_ANALYSIS_DIR / "05_test_metadata.json"


def load_metadata_json() -> dict:
    """
    Load test metadata from JSON file.
    
    Returns:
        Dictionary with metadata data
    
    Raises:
        FileNotFoundError: If JSON file doesn't exist
    """
    if not METADATA_FILE.exists():
        raise FileNotFoundError(
            f"Metadata file not found: {METADATA_FILE}\n"
            f"Please run test_analysis/05_extract_test_metadata.py first."
        )
    
    with open(METADATA_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
        return data.get('data', data)


def prepare_metadata_data(metadata_data: dict) -> list:
    """
    Prepare metadata data for database insertion.
    
    Args:
        metadata_data: Dictionary from JSON file
    
    Returns:
        List of metadata dictionaries ready for database insertion
    """
    test_metadata_list = metadata_data.get('test_metadata', [])
    
    prepared_metadata = []
    for meta in test_metadata_list:
        # Convert markers list to JSON string (will be stored as JSONB)
        markers_json = json.dumps(meta.get('markers', [])) if meta.get('markers') else None
        
        prepared_meta = {
            'test_id': meta['test_id'],
            'description': meta.get('description'),
            'markers': markers_json,  # JSON string, will be converted to JSONB
            'is_async': meta.get('is_async', False),
            'is_parameterized': meta.get('is_parameterized', False),
            'pattern': meta.get('pattern'),
            'line_number': meta.get('line_number')
        }
        prepared_metadata.append(prepared_meta)
    
    return prepared_metadata


def load_metadata_to_database(conn, metadata_list: list, batch_size: int = 50) -> dict:
    """
    Load metadata into database in batches.
    
    Args:
        conn: Database connection
        metadata_list: List of metadata dictionaries
        batch_size: Number of records to insert per batch
    
    Returns:
        Dictionary with loading statistics
    """
    total_metadata = len(metadata_list)
    loaded_count = 0
    failed_count = 0
    
    print_section(f"Loading {total_metadata} metadata records in batches of {batch_size}...")
    
    # Process in batches
    for i in range(0, total_metadata, batch_size):
        batch = metadata_list[i:i + batch_size]
        
        # Show progress
        current = min(i + batch_size, total_metadata)
        percentage = (current / total_metadata * 100) if total_metadata > 0 else 0
        print(f"Processing: {current}/{total_metadata} records ({percentage:.1f}%)", end='\r')
        
        try:
            with conn.cursor() as cursor:
                # Prepare values for batch insert
                values = [
                    (
                        m['test_id'],
                        m['description'],
                        m['markers'],  # Will be converted to JSONB
                        m['is_async'],
                        m['is_parameterized'],
                        m['pattern'],
                        m['line_number']
                    )
                    for m in batch
                ]
                
                # Use execute_values for efficient batch insert
                execute_values(
                    cursor,
                    """
                    INSERT INTO test_metadata 
                    (test_id, description, markers, is_async, is_parameterized, pattern, line_number)
                    VALUES %s
                    ON CONFLICT (test_id) DO UPDATE SET
                        description = EXCLUDED.description,
                        markers = EXCLUDED.markers,
                        is_async = EXCLUDED.is_async,
                        is_parameterized = EXCLUDED.is_parameterized,
                        pattern = EXCLUDED.pattern,
                        line_number = EXCLUDED.line_number
                    """,
                    values
                )
                conn.commit()
                loaded_count += len(batch)
        except Exception as e:
            conn.rollback()
            print(f"\nError in batch insert: {e}")
            failed_count += len(batch)
    
    print()  # New line after progress
    
    return {
        'total': total_metadata,
        'loaded': loaded_count,
        'failed': failed_count
    }


def get_metadata_statistics(conn) -> dict:
    """
    Get statistics about loaded metadata.
    
    Args:
        conn: Database connection
    
    Returns:
        Dictionary with statistics
    """
    with conn.cursor() as cursor:
        # Total metadata records
        cursor.execute("SELECT COUNT(*) FROM test_metadata")
        total_records = cursor.fetchone()[0]
        
        # Tests with descriptions
        cursor.execute("SELECT COUNT(*) FROM test_metadata WHERE description IS NOT NULL AND description != ''")
        with_descriptions = cursor.fetchone()[0]
        
        # Tests with markers
        cursor.execute("SELECT COUNT(*) FROM test_metadata WHERE markers IS NOT NULL AND markers != '[]'::jsonb")
        with_markers = cursor.fetchone()[0]
        
        # Async tests
        cursor.execute("SELECT COUNT(*) FROM test_metadata WHERE is_async = TRUE")
        async_tests = cursor.fetchone()[0]
        
        # Parameterized tests
        cursor.execute("SELECT COUNT(*) FROM test_metadata WHERE is_parameterized = TRUE")
        parameterized_tests = cursor.fetchone()[0]
        
        # Pattern distribution
        cursor.execute("""
            SELECT pattern, COUNT(*) as count
            FROM test_metadata
            WHERE pattern IS NOT NULL
            GROUP BY pattern
            ORDER BY count DESC
        """)
        patterns = cursor.fetchall()
        
        return {
            'total_records': total_records,
            'with_descriptions': with_descriptions,
            'with_markers': with_markers,
            'async_tests': async_tests,
            'parameterized_tests': parameterized_tests,
            'patterns': [{'pattern': row[0], 'count': row[1]} for row in patterns]
        }


def main():
    """Main function to load test metadata."""
    print_header("Step 5: Loading Test Metadata")
    print()
    
    # Step 1: Test database connection
    print_section("Testing database connection...")
    if not test_connection():
        print()
        print("ERROR: Cannot connect to database!")
        return
    print()
    
    # Step 2: Load JSON data
    print_section("Loading metadata data from JSON...")
    try:
        metadata_data = load_metadata_json()
        print_item("JSON file loaded:", str(METADATA_FILE))
        print_item("Total tests with metadata:", metadata_data.get('total_tests', 0))
    except FileNotFoundError as e:
        print(f"ERROR: {e}")
        return
    except Exception as e:
        print(f"ERROR: {e}")
        return
    
    print()
    
    # Step 3: Prepare metadata data
    print_section("Preparing metadata data...")
    metadata_list = prepare_metadata_data(metadata_data)
    print_item("Metadata records prepared:", len(metadata_list))
    print()
    
    # Step 4: Load into database
    try:
        with get_connection() as conn:
            # Check current count
            initial_count = count_table_records(conn, "test_metadata")
            print_item("Metadata records in database (before):", initial_count)
            print()
            
            # Load metadata
            stats = load_metadata_to_database(conn, metadata_list)
            print()
            
            # Check final count
            final_count = count_table_records(conn, "test_metadata")
            
            # Step 5: Get statistics
            meta_stats = get_metadata_statistics(conn)
            print()
            
            # Step 6: Display summary
            print_section("Loading Summary:")
            print_item("Total records in JSON:", stats['total'])
            print_item("Successfully loaded:", stats['loaded'])
            if stats['failed'] > 0:
                print_item("Failed to load:", stats['failed'])
            print_item("Records in database (after):", final_count)
            print()
            
            print_section("Metadata Statistics:")
            print_item("Tests with descriptions:", meta_stats['with_descriptions'])
            print_item("Tests with markers:", meta_stats['with_markers'])
            print_item("Async tests:", meta_stats['async_tests'])
            print_item("Parameterized tests:", meta_stats['parameterized_tests'])
            print()
            
            # Step 7: Display pattern distribution
            if meta_stats['patterns']:
                print_section("Test Naming Patterns:")
                for pattern_info in meta_stats['patterns']:
                    print_item(f"{pattern_info['pattern']}:", pattern_info['count'])
            print()
            
            print_header("Step 5 Complete!")
            print(f"Loaded {stats['loaded']} metadata records into database")
            
    except Exception as e:
        print(f"ERROR: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

```

`deterministic/06_load_structure.py`

```python
"""
Step 6: Load Test Structure

This script loads test repository structure information
from the test analysis JSON output into the PostgreSQL database.

What it does:
1. Reads structure data from test_analysis/outputs/07_test_structure.json
2. Connects to PostgreSQL database
3. Inserts directory structure data into test_structure table
4. Stores category information and file counts
5. Displays loading progress and statistics

Run this script:
    python deterministic/06_load_structure.py
"""

import sys
import json
from pathlib import Path

# Add current directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from db_connection import get_connection, test_connection
from utils.db_helpers import count_table_records
from utils.output_formatter import print_header, print_section, print_item
from psycopg2.extras import execute_values

# Path to structure JSON file
TEST_ANALYSIS_DIR = Path(__file__).parent.parent / "test_analysis" / "outputs"
STRUCTURE_FILE = TEST_ANALYSIS_DIR / "07_test_structure.json"


def load_structure_json() -> dict:
    """
    Load test structure data from JSON file.
    
    Returns:
        Dictionary with structure data
    
    Raises:
        FileNotFoundError: If JSON file doesn't exist
    """
    if not STRUCTURE_FILE.exists():
        raise FileNotFoundError(
            f"Structure file not found: {STRUCTURE_FILE}\n"
            f"Please run test_analysis/07_map_test_structure.py first."
        )
    
    with open(STRUCTURE_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
        return data.get('data', data)


def prepare_structure_data(structure_data: dict) -> list:
    """
    Prepare structure data for database insertion.
    
    Args:
        structure_data: Dictionary from JSON file
    
    Returns:
        List of structure dictionaries ready for database insertion
    """
    directory_structure = structure_data.get('directory_structure', {})
    directories = directory_structure.get('directories', {})
    files_by_directory = directory_structure.get('files_by_directory', {})
    
    structure_list = []
    
    # Process each directory category
    for category, stats in directories.items():
        # Get files for this category
        files = files_by_directory.get(category, [])
        
        # Create structure entry
        structure_entry = {
            'directory_path': category,  # Category name as directory path
            'category': category,
            'file_count': stats.get('file_count', len(files)),
            'total_lines': stats.get('total_lines', 0)
        }
        structure_list.append(structure_entry)
    
    # Also process package structure if available
    package_structure = directory_structure.get('package_structure', {})
    for package, info in package_structure.items():
        # Check if we already have this directory
        existing = [s for s in structure_list if s['directory_path'] == package]
        if not existing:
            structure_entry = {
                'directory_path': package,
                'category': None,  # Will be determined from files
                'file_count': len(info.get('files', [])),
                'total_lines': 0  # Could be calculated if needed
            }
            structure_list.append(structure_entry)
    
    return structure_list


def load_structure_to_database(conn, structure_list: list) -> dict:
    """
    Load structure data into database.
    
    Args:
        conn: Database connection
        structure_list: List of structure dictionaries
    
    Returns:
        Dictionary with loading statistics
    """
    total_records = len(structure_list)
    loaded_count = 0
    failed_count = 0
    
    print_section(f"Loading {total_records} structure records...")
    
    try:
        with conn.cursor() as cursor:
            # Prepare values for batch insert
            values = [
                (
                    s['directory_path'],
                    s['category'],
                    s['file_count'],
                    s['total_lines']
                )
                for s in structure_list
            ]
            
            # Use execute_values for efficient batch insert
            execute_values(
                cursor,
                """
                INSERT INTO test_structure 
                (directory_path, category, file_count, total_lines)
                VALUES %s
                ON CONFLICT DO NOTHING
                """,
                values
            )
            conn.commit()
            loaded_count = total_records
            
    except Exception as e:
        conn.rollback()
        print(f"Error loading structure: {e}")
        failed_count = total_records
    
    return {
        'total': total_records,
        'loaded': loaded_count,
        'failed': failed_count
    }


def get_structure_statistics(conn) -> dict:
    """
    Get statistics about loaded structure data.
    
    Args:
        conn: Database connection
    
    Returns:
        Dictionary with statistics
    """
    with conn.cursor() as cursor:
        # Total structure records
        cursor.execute("SELECT COUNT(*) FROM test_structure")
        total_records = cursor.fetchone()[0]
        
        # Total files across all directories
        cursor.execute("SELECT SUM(file_count) FROM test_structure")
        total_files = cursor.fetchone()[0] or 0
        
        # Total lines across all directories
        cursor.execute("SELECT SUM(total_lines) FROM test_structure")
        total_lines = cursor.fetchone()[0] or 0
        
        # Structure by category
        cursor.execute("""
            SELECT category, COUNT(*) as dir_count, 
                   SUM(file_count) as total_files, 
                   SUM(total_lines) as total_lines
            FROM test_structure
            WHERE category IS NOT NULL
            GROUP BY category
            ORDER BY category
        """)
        by_category = cursor.fetchall()
        
        return {
            'total_records': total_records,
            'total_files': total_files,
            'total_lines': total_lines,
            'by_category': [
                {
                    'category': row[0],
                    'directory_count': row[1],
                    'file_count': row[2],
                    'line_count': row[3]
                }
                for row in by_category
            ]
        }


def main():
    """Main function to load test structure."""
    print_header("Step 6: Loading Test Structure")
    print()
    
    # Step 1: Test database connection
    print_section("Testing database connection...")
    if not test_connection():
        print()
        print("ERROR: Cannot connect to database!")
        return
    print()
    
    # Step 2: Load JSON data
    print_section("Loading structure data from JSON...")
    try:
        structure_data = load_structure_json()
        print_item("JSON file loaded:", str(STRUCTURE_FILE))
    except FileNotFoundError as e:
        print(f"ERROR: {e}")
        return
    except Exception as e:
        print(f"ERROR: {e}")
        return
    
    print()
    
    # Step 3: Prepare structure data
    print_section("Preparing structure data...")
    structure_list = prepare_structure_data(structure_data)
    print_item("Structure records prepared:", len(structure_list))
    print()
    
    # Step 4: Load into database
    try:
        with get_connection() as conn:
            # Check current count
            initial_count = count_table_records(conn, "test_structure")
            print_item("Structure records in database (before):", initial_count)
            print()
            
            # Load structure
            stats = load_structure_to_database(conn, structure_list)
            print()
            
            # Check final count
            final_count = count_table_records(conn, "test_structure")
            
            # Step 5: Get statistics
            struct_stats = get_structure_statistics(conn)
            print()
            
            # Step 6: Display summary
            print_section("Loading Summary:")
            print_item("Total records in JSON:", stats['total'])
            print_item("Successfully loaded:", stats['loaded'])
            if stats['failed'] > 0:
                print_item("Failed to load:", stats['failed'])
            print_item("Records in database (after):", final_count)
            print()
            
            print_section("Structure Statistics:")
            print_item("Total directories:", struct_stats['total_records'])
            print_item("Total files:", struct_stats['total_files'])
            print_item("Total lines:", struct_stats['total_lines'])
            print()
            
            # Step 7: Display by category
            if struct_stats['by_category']:
                print_section("Structure by Category:")
                for cat_info in struct_stats['by_category']:
                    print_item(f"{cat_info['category']}:", 
                              f"{cat_info['file_count']} files, {cat_info['line_count']} lines")
            print()
            
            print_header("Step 6 Complete!")
            print(f"Loaded {stats['loaded']} structure records into database")
            
    except Exception as e:
        print(f"ERROR: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

```

`deterministic/07_verify_data.py`

```python
"""
Step 7: Verify Data

This script verifies that all data was loaded correctly into the database.
It runs sample queries, checks data integrity, and displays a comprehensive report.

What it does:
1. Connects to PostgreSQL database
2. Counts records in each table
3. Verifies foreign key relationships
4. Runs sample queries to test functionality
5. Displays comprehensive verification report
6. Tests query performance

Run this script:
    python deterministic/07_verify_data.py
"""

import sys
from pathlib import Path

# Add current directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from db_connection import get_connection, test_connection, get_db_config
from utils.db_helpers import count_table_records, get_tests_for_production_class
from utils.output_formatter import print_header, print_section, print_item
import os
from dotenv import load_dotenv
from pathlib import Path

# Load environment variables to get schema
env_path = Path(__file__).parent / ".env"
if not env_path.exists():
    env_path = Path(__file__).parent.parent / ".env"
load_dotenv(env_path)

# Get schema from environment (same as db_connection.py)
SCHEMA = os.getenv('DB_SCHEMA', 'planon1')


def count_all_tables(conn) -> dict:
    """
    Count records in all tables.
    
    Args:
        conn: Database connection
    
    Returns:
        Dictionary with table counts
    """
    tables = [
        'test_registry',
        'test_dependencies',
        'reverse_index',
        'test_metadata',
        'test_structure',
        'test_function_mapping'
    ]
    
    counts = {}
    for table in tables:
        counts[table] = count_table_records(conn, table)
    
    return counts


def verify_foreign_keys(conn) -> dict:
    """
    Verify foreign key relationships are intact.
    
    Args:
        conn: Database connection
    
    Returns:
        Dictionary with verification results
    """
    results = {}
    
    with conn.cursor() as cursor:
        # Check test_dependencies -> test_registry
        cursor.execute("""
            SELECT COUNT(*) 
            FROM test_dependencies td
            LEFT JOIN test_registry tr ON td.test_id = tr.test_id
            WHERE tr.test_id IS NULL
        """)
        orphaned_deps = cursor.fetchone()[0]
        results['test_dependencies'] = {
            'orphaned': orphaned_deps,
            'valid': orphaned_deps == 0
        }
        
        # Check reverse_index -> test_registry
        cursor.execute("""
            SELECT COUNT(*) 
            FROM reverse_index ri
            LEFT JOIN test_registry tr ON ri.test_id = tr.test_id
            WHERE tr.test_id IS NULL
        """)
        orphaned_reverse = cursor.fetchone()[0]
        results['reverse_index'] = {
            'orphaned': orphaned_reverse,
            'valid': orphaned_reverse == 0
        }
        
        # Check test_metadata -> test_registry
        cursor.execute("""
            SELECT COUNT(*) 
            FROM test_metadata tm
            LEFT JOIN test_registry tr ON tm.test_id = tr.test_id
            WHERE tr.test_id IS NULL
        """)
        orphaned_metadata = cursor.fetchone()[0]
        results['test_metadata'] = {
            'orphaned': orphaned_metadata,
            'valid': orphaned_metadata == 0
        }
        
        # Check test_function_mapping -> test_registry
        cursor.execute("""
            SELECT COUNT(*) 
            FROM test_function_mapping tfm
            LEFT JOIN test_registry tr ON tfm.test_id = tr.test_id
            WHERE tr.test_id IS NULL
        """)
        orphaned_function_mapping = cursor.fetchone()[0]
        results['test_function_mapping'] = {
            'orphaned': orphaned_function_mapping,
            'valid': orphaned_function_mapping == 0
        }
    
    return results


def run_sample_queries(conn) -> dict:
    """
    Run sample queries to test functionality.
    
    Args:
        conn: Database connection
    
    Returns:
        Dictionary with query results
    """
    results = {}
    
    with conn.cursor() as cursor:
        # Query 1: Get tests for a production class (using reverse index)
        cursor.execute("""
            SELECT production_class, COUNT(*) as test_count
            FROM reverse_index
            GROUP BY production_class
            ORDER BY test_count DESC
            LIMIT 1
        """)
        top_class = cursor.fetchone()
        if top_class:
            class_name = top_class[0]
            test_count = top_class[1]
            results['top_production_class'] = {
                'class': class_name,
                'test_count': test_count
            }
            
            # Get actual tests for this class
            tests = get_tests_for_production_class(conn, class_name)
            results['top_production_class']['sample_tests'] = tests[:3]
        
        # Query 2: Tests by type
        cursor.execute("""
            SELECT test_type, COUNT(*) as count
            FROM test_registry
            WHERE test_type IS NOT NULL
            GROUP BY test_type
            ORDER BY count DESC
        """)
        by_type = cursor.fetchall()
        results['tests_by_type'] = [
            {'type': row[0], 'count': row[1]} for row in by_type
        ]
        
        # Query 3: Tests with most dependencies
        cursor.execute("""
            SELECT tr.test_id, tr.method_name, COUNT(td.id) as dep_count
            FROM test_registry tr
            LEFT JOIN test_dependencies td ON tr.test_id = td.test_id
            GROUP BY tr.test_id, tr.method_name
            ORDER BY dep_count DESC
            LIMIT 5
        """)
        top_deps = cursor.fetchall()
        results['tests_with_most_dependencies'] = [
            {
                'test_id': row[0],
                'method_name': row[1],
                'dependency_count': row[2]
            }
            for row in top_deps
        ]
        
        # Query 4: Metadata coverage
        cursor.execute("""
            SELECT 
                COUNT(DISTINCT tr.test_id) as total_tests,
                COUNT(DISTINCT tm.test_id) as tests_with_metadata,
                ROUND(100.0 * COUNT(DISTINCT tm.test_id) / COUNT(DISTINCT tr.test_id), 2) as coverage_pct
            FROM test_registry tr
            LEFT JOIN test_metadata tm ON tr.test_id = tm.test_id
        """)
        coverage = cursor.fetchone()
        results['metadata_coverage'] = {
            'total_tests': coverage[0],
            'tests_with_metadata': coverage[1],
            'coverage_percentage': coverage[2]
        }
        
        # Query 5: Function mapping coverage and sample
        cursor.execute("""
            SELECT 
                COUNT(DISTINCT tr.test_id) as total_tests,
                COUNT(DISTINCT tfm.test_id) as tests_with_function_calls,
                ROUND(100.0 * COUNT(DISTINCT tfm.test_id) / COUNT(DISTINCT tr.test_id), 2) as coverage_pct
            FROM test_registry tr
            LEFT JOIN test_function_mapping tfm ON tr.test_id = tfm.test_id
        """)
        func_coverage = cursor.fetchone()
        results['function_mapping_coverage'] = {
            'total_tests': func_coverage[0],
            'tests_with_function_calls': func_coverage[1],
            'coverage_percentage': func_coverage[2]
        }
        
        # Get most called function
        cursor.execute("""
            SELECT module_name, function_name, COUNT(*) as test_count
            FROM test_function_mapping
            GROUP BY module_name, function_name
            ORDER BY test_count DESC
            LIMIT 1
        """)
        top_function = cursor.fetchone()
        if top_function:
            results['top_function'] = {
                'module': top_function[0],
                'function': top_function[1],
                'test_count': top_function[2]
            }
            
            # Get sample tests for this function
            cursor.execute("""
                SELECT DISTINCT tr.test_id, tr.class_name, tr.method_name
                FROM test_function_mapping tfm
                JOIN test_registry tr ON tfm.test_id = tr.test_id
                WHERE tfm.module_name = %s
                AND tfm.function_name = %s
                LIMIT 3
            """, (top_function[0], top_function[1]))
            sample_tests = cursor.fetchall()
            results['top_function']['sample_tests'] = [
                {
                    'test_id': row[0],
                    'class_name': row[1],
                    'method_name': row[2]
                }
                for row in sample_tests
            ]
    
    return results


def main():
    """Main function to verify data."""
    print_header("Step 7: Verifying Data")
    print()
    
    # Step 1: Test database connection
    print_section("Testing database connection...")
    if not test_connection():
        print()
        print("ERROR: Cannot connect to database!")
        return
    print()
    
    try:
        with get_connection() as conn:
            # Step 2: Count all tables
            print_section("Counting records in all tables...")
            counts = count_all_tables(conn)
            
            for table, count in counts.items():
                print_item(f"{table}:", count)
            print()
            
            # Step 3: Verify foreign keys
            print_section("Verifying foreign key relationships...")
            fk_results = verify_foreign_keys(conn)
            
            all_valid = True
            for table, result in fk_results.items():
                if result['valid']:
                    print_item(f"{table}:", "[OK] All foreign keys valid")
                else:
                    print_item(f"{table}:", f"[ERROR] {result['orphaned']} orphaned records")
                    all_valid = False
            
            if all_valid:
                print()
                print_item("All foreign key relationships are valid!", "")
            else:
                print()
                print_item("WARNING: Some foreign key relationships have issues!", "")
            print()
            
            # Step 4: Run sample queries
            print_section("Running sample queries...")
            query_results = run_sample_queries(conn)
            
            # Display query results
            if 'top_production_class' in query_results:
                top_class = query_results['top_production_class']
                print_item("Most referenced production class:", 
                          f"{top_class['class']} ({top_class['test_count']} tests)")
                if top_class.get('sample_tests'):
                    print_item("  Sample tests:", "")
                    for test in top_class['sample_tests'][:3]:
                        test_name = f"{test['class_name']}.{test['method_name']}" if test['class_name'] else test['method_name']
                        print_item(f"    - {test['test_id']}:", test_name)
            print()
            
            if 'tests_by_type' in query_results:
                print_item("Tests by type:", "")
                for type_info in query_results['tests_by_type']:
                    print_item(f"  {type_info['type']}:", type_info['count'])
            print()
            
            if 'tests_with_most_dependencies' in query_results:
                print_item("Tests with most dependencies:", "")
                for test_info in query_results['tests_with_most_dependencies']:
                    print_item(f"  {test_info['test_id']} ({test_info['method_name']}):", 
                              f"{test_info['dependency_count']} dependencies")
            print()
            
            if 'metadata_coverage' in query_results:
                coverage = query_results['metadata_coverage']
                print_item("Metadata coverage:", 
                          f"{coverage['coverage_percentage']}% "
                          f"({coverage['tests_with_metadata']}/{coverage['total_tests']} tests)")
            print()
            
            if 'function_mapping_coverage' in query_results:
                func_coverage = query_results['function_mapping_coverage']
                print_item("Function mapping coverage:", 
                          f"{func_coverage['coverage_percentage']}% "
                          f"({func_coverage['tests_with_function_calls']}/{func_coverage['total_tests']} tests)")
            print()
            
            if 'top_function' in query_results:
                top_func = query_results['top_function']
                func_name = f"{top_func['module']}.{top_func['function']}"
                print_item("Most called function:", 
                          f"{func_name} ({top_func['test_count']} tests)")
                if top_func.get('sample_tests'):
                    print_item("  Sample tests:", "")
                    for test in top_func['sample_tests']:
                        test_name = f"{test['class_name']}.{test['method_name']}" if test['class_name'] else test['method_name']
                        print_item(f"    - {test['test_id']}:", test_name)
            print()
            
            # Step 5: Summary
            print_section("Verification Summary:")
            total_records = sum(counts.values())
            print_item("Total records across all tables:", total_records)
            print_item("Foreign key integrity:", "[OK] Valid" if all_valid else "[ERROR] Issues found")
            print_item("Data loaded successfully:", "[OK] Yes" if total_records > 0 else "[ERROR] No")
            print()
            
            print_header("Step 7 Complete!")
            print("Data verification complete!")
            print()
            print("Database is ready for:")
            print("  - Deterministic test selection queries")
            print("  - Fast code -> tests lookups")
            print("  - Function-level test selection (precise matching)")
            print("  - Test metadata queries")
            print("  - Future semantic/vector data integration")
            
    except Exception as e:
        print(f"ERROR: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

```

`deterministic/db_connection.py`

```python
"""
Database Connection Module

This module provides database connection utilities for PostgreSQL.
It handles connection management, environment variable loading, and provides
a context manager for safe database operations.

Key Features:
- Loads database credentials from .env file
- Creates database connections with proper error handling
- Sets schema search path to planon1
- Provides connection context manager for automatic cleanup
- Includes test connection function

Usage:
    from db_connection import get_connection, test_connection
    
    # Test connection
    if test_connection():
        print("Database connection successful!")
    
    # Use connection
    with get_connection() as conn:
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM planon1.test_registry")
        results = cursor.fetchall()
"""

import os
import psycopg2
from psycopg2 import pool
from psycopg2 import OperationalError
from contextlib import contextmanager
from dotenv import load_dotenv
from pathlib import Path

# Load environment variables from .env file
# Look for .env file in the deterministic directory or parent directory
env_path = Path(__file__).parent / ".env"
if not env_path.exists():
    env_path = Path(__file__).parent.parent / ".env"
load_dotenv(env_path)


# Database configuration from environment variables
# Schema is stored separately as it's not a connection parameter
DB_SCHEMA = os.getenv('DB_SCHEMA', 'planon1')

# Connection parameters (schema is NOT included - it's set via SQL after connection)
DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost'),
    'port': os.getenv('DB_PORT', '5432'),
    'database': os.getenv('DB_NAME', 'planon'),
    'user': os.getenv('DB_USER'),
    'password': os.getenv('DB_PASSWORD')
}


# Connection pool (created on first use)
_connection_pool = None


def get_db_config():
    """
    Get database configuration dictionary.
    
    Returns:
        Dictionary with database connection parameters
    
    Example:
        >>> config = get_db_config()
        >>> print(config['database'])
        planon
    """
    return DB_CONFIG.copy()


def create_connection_pool(minconn=1, maxconn=5):
    """
    Create a connection pool for database connections.
    
    Args:
        minconn: Minimum number of connections in pool (default: 1)
        maxconn: Maximum number of connections in pool (default: 5)
    
    Returns:
        Connection pool object
    
    Note:
        Connection pooling allows reuse of database connections,
        which is more efficient than creating new connections each time.
    """
    global _connection_pool
    
    if _connection_pool is None:
        try:
            _connection_pool = pool.SimpleConnectionPool(
                minconn,
                maxconn,
                **DB_CONFIG
            )
            print(f"Connection pool created successfully")
        except Exception as e:
            print(f"Error creating connection pool: {e}")
            raise
    
    return _connection_pool


@contextmanager
def get_connection():
    """
    Get a database connection from the pool with automatic cleanup.
    
    This is a context manager that:
    1. Gets a connection from the pool
    2. Sets the schema search path to planon1
    3. Yields the connection for use
    4. Returns the connection to the pool when done
    
    Usage:
        with get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM planon1.test_registry")
            results = cursor.fetchall()
    
    Yields:
        psycopg2.connection: Database connection object
    
    Raises:
        Exception: If connection cannot be established
    """
    pool = create_connection_pool()
    conn = None
    
    try:
        # Get connection from pool
        conn = pool.getconn()
        
        if conn is None:
            raise Exception("Failed to get connection from pool")
        
        # Set schema search path to planon1
        # This allows us to use table names without schema prefix
        with conn.cursor() as cursor:
            cursor.execute(f"SET search_path TO {DB_SCHEMA}, public")
            conn.commit()
        
        # Yield connection for use
        yield conn
        
    except Exception as e:
        if conn:
            conn.rollback()
        print(f"Database connection error: {e}")
        raise
    finally:
        # Return connection to pool
        if conn:
            pool.putconn(conn)


def test_connection():
    """
    Test database connection and schema access.
    
    This function:
    1. Attempts to connect to the database
    2. Verifies the schema exists
    3. Returns True if successful, False otherwise
    
    Returns:
        bool: True if connection successful, False otherwise
    
    Example:
        >>> if test_connection():
        ...     print("Database is ready!")
        ... else:
        ...     print("Database connection failed!")
    """
    try:
        with get_connection() as conn:
            with conn.cursor() as cursor:
                # Test basic query
                cursor.execute("SELECT current_database(), current_schema()")
                result = cursor.fetchone()
                db_name, schema_name = result
                
                # Verify schema exists
                cursor.execute("""
                    SELECT schema_name 
                    FROM information_schema.schemata 
                    WHERE schema_name = %s
                """, (DB_SCHEMA,))
                
                schema_exists = cursor.fetchone() is not None
                
                if schema_exists:
                    print(f"[OK] Connected to database: {db_name}")
                    print(f"[OK] Using schema: {DB_SCHEMA}")
                    print(f"[OK] Schema exists and is accessible")
                    return True
                else:
                    print(f"[ERROR] Schema '{DB_SCHEMA}' does not exist!")
                    print(f"  Please create the schema in pgAdmin or using SQL:")
                    print(f"  CREATE SCHEMA IF NOT EXISTS {DB_SCHEMA};")
                    return False
                    
    except OperationalError as e:
        print(f"[ERROR] Connection failed: {e}")
        print(f"  Please check:")
        print(f"  - Database '{DB_CONFIG['database']}' exists")
        print(f"  - Host: {DB_CONFIG['host']}, Port: {DB_CONFIG['port']}")
        print(f"  - Username and password are correct")
        print(f"  - .env file is configured properly")
        return False
    except Exception as e:
        print(f"[ERROR] Unexpected error: {e}")
        return False


def close_connection_pool():
    """
    Close all connections in the pool.
    
    Call this when you're done with all database operations
    to properly clean up resources.
    """
    global _connection_pool
    
    if _connection_pool:
        _connection_pool.closeall()
        _connection_pool = None
        print("Connection pool closed")


if __name__ == "__main__":
    """
    Test the database connection when run directly.
    
    Run this file to test your database connection:
        python deterministic/db_connection.py
    """
    print("=" * 50)
    print("Testing Database Connection")
    print("=" * 50)
    print()
    print(f"Database: {DB_CONFIG['database']}")
    print(f"Schema: {DB_SCHEMA}")
    print(f"Host: {DB_CONFIG['host']}:{DB_CONFIG['port']}")
    print(f"User: {DB_CONFIG['user']}")
    print()
    
    if test_connection():
        print()
        print("=" * 50)
        print("Connection test PASSED!")
        print("=" * 50)
    else:
        print()
        print("=" * 50)
        print("Connection test FAILED!")
        print("=" * 50)
        print()
        print("Please check your .env file configuration.")

```

`deterministic/requirements.txt`

```
# Requirements for Deterministic Database Implementation

# PostgreSQL database adapter
psycopg2-binary>=2.9.0

# Environment variable management
python-dotenv>=1.0.0

# Note: psycopg2-binary is recommended for beginners as it includes
# all dependencies and doesn't require compilation.
# For production, you might want to use psycopg2 instead.

```

`deterministic/utils/db_helpers.py`

```python
"""
Database Helper Functions

This module provides reusable database helper functions for common operations
like inserting data, querying, and batch operations.

These functions make it easier to work with the database and reduce code duplication.
"""

from typing import List, Dict, Any, Optional
from psycopg2.extras import execute_batch, execute_values


def insert_test_registry(conn, test_data: Dict[str, Any]) -> bool:
    """
    Insert a single test into test_registry table.
    
    Args:
        conn: Database connection
        test_data: Dictionary with test data:
            - test_id: Unique test identifier
            - file_path: Path to test file
            - class_name: Test class name (optional)
            - method_name: Test method name
            - test_type: Type of test (unit, integration, e2e)
            - line_number: Line number (optional)
    
    Returns:
        True if successful, False otherwise
    """
    try:
        with conn.cursor() as cursor:
            cursor.execute("""
                INSERT INTO test_registry 
                (test_id, file_path, class_name, method_name, test_type, line_number)
                VALUES (%s, %s, %s, %s, %s, %s)
                ON CONFLICT (test_id) DO UPDATE SET
                    file_path = EXCLUDED.file_path,
                    class_name = EXCLUDED.class_name,
                    method_name = EXCLUDED.method_name,
                    test_type = EXCLUDED.test_type,
                    line_number = EXCLUDED.line_number
            """, (
                test_data['test_id'],
                test_data['file_path'],
                test_data.get('class_name'),
                test_data['method_name'],
                test_data.get('test_type'),
                test_data.get('line_number')
            ))
            conn.commit()
            return True
    except Exception as e:
        conn.rollback()
        print(f"Error inserting test {test_data.get('test_id')}: {e}")
        return False


def batch_insert_test_registry(conn, tests: List[Dict[str, Any]]) -> int:
    """
    Insert multiple tests in batch (more efficient than one-by-one).
    
    Args:
        conn: Database connection
        tests: List of test dictionaries
    
    Returns:
        Number of tests successfully inserted
    """
    if not tests:
        return 0
    
    try:
        with conn.cursor() as cursor:
            # Prepare data for batch insert
            values = [
                (
                    t['test_id'],
                    t['file_path'],
                    t.get('class_name'),
                    t['method_name'],
                    t.get('test_type'),
                    t.get('line_number')
                )
                for t in tests
            ]
            
            # Use execute_values for efficient batch insert
            execute_values(
                cursor,
                """
                INSERT INTO test_registry 
                (test_id, file_path, class_name, method_name, test_type, line_number)
                VALUES %s
                ON CONFLICT (test_id) DO UPDATE SET
                    file_path = EXCLUDED.file_path,
                    class_name = EXCLUDED.class_name,
                    method_name = EXCLUDED.method_name,
                    test_type = EXCLUDED.test_type,
                    line_number = EXCLUDED.line_number
                """,
                values
            )
            conn.commit()
            return len(tests)
    except Exception as e:
        conn.rollback()
        print(f"Error in batch insert: {e}")
        return 0


def insert_test_dependency(conn, test_id: str, referenced_class: str, 
                          import_type: Optional[str] = None) -> bool:
    """
    Insert a test dependency (test → production code mapping).
    
    Args:
        conn: Database connection
        test_id: Test identifier
        referenced_class: Production class/module name
        import_type: Type of import (optional)
    
    Returns:
        True if successful, False otherwise
    """
    try:
        with conn.cursor() as cursor:
            cursor.execute("""
                INSERT INTO test_dependencies 
                (test_id, referenced_class, import_type)
                VALUES (%s, %s, %s)
            """, (test_id, referenced_class, import_type))
            conn.commit()
            return True
    except Exception as e:
        conn.rollback()
        print(f"Error inserting dependency: {e}")
        return False


def batch_insert_test_dependencies(conn, dependencies: List[Dict[str, Any]]) -> int:
    """
    Insert multiple test dependencies in batch.
    
    Args:
        conn: Database connection
        dependencies: List of dependency dictionaries with:
            - test_id: Test identifier
            - referenced_class: Production class name
            - import_type: Type of import (optional)
    
    Returns:
        Number of dependencies inserted
    """
    if not dependencies:
        return 0
    
    try:
        with conn.cursor() as cursor:
            values = [
                (
                    d['test_id'],
                    d['referenced_class'],
                    d.get('import_type')
                )
                for d in dependencies
            ]
            
            execute_values(
                cursor,
                """
                INSERT INTO test_dependencies 
                (test_id, referenced_class, import_type)
                VALUES %s
                """,
                values
            )
            conn.commit()
            return len(dependencies)
    except Exception as e:
        conn.rollback()
        print(f"Error in batch insert dependencies: {e}")
        return 0


def insert_reverse_index(conn, production_class: str, test_id: str, 
                        test_file_path: Optional[str] = None,
                        reference_type: str = 'direct_import') -> bool:
    """
    Insert a reverse index entry (production code → test mapping).
    
    Args:
        conn: Database connection
        production_class: Production class/module name
        test_id: Test identifier
        test_file_path: Path to test file (optional, for performance)
        reference_type: Type of reference ('direct_import', 'string_ref', 'indirect')
    
    Returns:
        True if successful, False otherwise
    """
    try:
        with conn.cursor() as cursor:
            cursor.execute("""
                INSERT INTO reverse_index 
                (production_class, test_id, test_file_path, reference_type)
                VALUES (%s, %s, %s, %s)
            """, (production_class, test_id, test_file_path, reference_type))
            conn.commit()
            return True
    except Exception as e:
        conn.rollback()
        print(f"Error inserting reverse index: {e}")
        return False


def batch_insert_reverse_index(conn, reverse_entries: List[Dict[str, Any]]) -> int:
    """
    Insert multiple reverse index entries in batch.
    
    Args:
        conn: Database connection
        reverse_entries: List of reverse index dictionaries
    
    Returns:
        Number of entries inserted
    """
    if not reverse_entries:
        return 0
    
    try:
        with conn.cursor() as cursor:
            values = [
                (
                    e['production_class'],
                    e['test_id'],
                    e.get('test_file_path'),
                    e.get('reference_type', 'direct_import')
                )
                for e in reverse_entries
            ]
            
            execute_values(
                cursor,
                """
                INSERT INTO reverse_index 
                (production_class, test_id, test_file_path, reference_type)
                VALUES %s
                """,
                values
            )
            conn.commit()
            return len(reverse_entries)
    except Exception as e:
        conn.rollback()
        print(f"Error in batch insert reverse index: {e}")
        return 0


def get_tests_for_production_class(conn, production_class: str, schema: str = 'planon1') -> List[Dict[str, Any]]:
    """
    Get all tests that reference a production class (using reverse index).
    
    This function matches both:
    - Exact matches: production_class = 'api.routes'
    - Sub-path matches: production_class LIKE 'api.routes.%'
    
    This is important because string references in patch() calls often use
    sub-paths like 'api.routes.get_agent' instead of just 'api.routes'.
    
    Args:
        conn: Database connection
        production_class: Production class/module name
        schema: Database schema name (default: planon1)
    
    Returns:
        List of test dictionaries with reference_type included
    """
    with conn.cursor() as cursor:
        # Get all matching tests, prioritizing exact matches and string references
        # Use a CTE to handle deduplication properly
        cursor.execute(f"""
            WITH ranked_tests AS (
                SELECT 
                    r.test_id, 
                    r.test_file_path, 
                    t.class_name, 
                    t.method_name, 
                    r.reference_type,
                    ROW_NUMBER() OVER (
                        PARTITION BY r.test_id 
                        ORDER BY 
                            CASE WHEN r.production_class = %s THEN 1 ELSE 2 END,
                            CASE WHEN r.reference_type = 'string_ref' THEN 1 ELSE 2 END
                    ) as rn
                FROM {schema}.reverse_index r
                JOIN {schema}.test_registry t ON r.test_id = t.test_id
                WHERE r.production_class = %s
                   OR r.production_class LIKE %s
            )
            SELECT 
                test_id, 
                test_file_path, 
                class_name, 
                method_name, 
                reference_type
            FROM ranked_tests
            WHERE rn = 1
            ORDER BY 
                CASE WHEN reference_type = 'string_ref' THEN 1 ELSE 2 END,
                test_id
        """, (production_class, production_class, f"{production_class}.%"))
        
        results = cursor.fetchall()
        return [
            {
                'test_id': row[0],
                'test_file_path': row[1],
                'class_name': row[2],
                'method_name': row[3],
                'reference_type': row[4] or 'direct_import'
            }
            for row in results
        ]


def batch_insert_test_function_mapping(conn, mappings: List[Dict[str, Any]]) -> int:
    """
    Insert multiple test function mappings in batch.
    
    Args:
        conn: Database connection
        mappings: List of function mapping dictionaries with:
            - test_id: Test identifier
            - module_name: Production module name (e.g., agent.langgraph_agent)
            - function_name: Function name (e.g., initialize)
            - call_type: Type of call (direct_call, method_call, patch_ref)
            - source: Source of mapping (method_call, patch_ref)
    
    Returns:
        Number of mappings inserted
    """
    if not mappings:
        return 0
    
    try:
        with conn.cursor() as cursor:
            values = [
                (
                    m['test_id'],
                    m['module_name'],
                    m['function_name'],
                    m.get('call_type'),
                    m.get('source')
                )
                for m in mappings
            ]
            
            execute_values(
                cursor,
                """
                INSERT INTO test_function_mapping 
                (test_id, module_name, function_name, call_type, source)
                VALUES %s
                ON CONFLICT DO NOTHING
                """,
                values
            )
            conn.commit()
            return len(mappings)
    except Exception as e:
        conn.rollback()
        print(f"Error in batch insert function mappings: {e}")
        return 0


def count_table_records(conn, table_name: str) -> int:
    """
    Count records in a table.
    
    Args:
        conn: Database connection
        table_name: Name of the table (without schema prefix)
    
    Returns:
        Number of records
    """
    with conn.cursor() as cursor:
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        return cursor.fetchone()[0]

```

`deterministic/utils/output_formatter.py`

```python
"""
Output formatting utilities for console output.

This module provides functions for formatted console output,
similar to the test_analysis utilities but simplified for database operations.
"""

from datetime import datetime


def print_header(title: str, width: int = 50) -> None:
    """
    Print a formatted header for console output.
    
    Args:
        title: The title text to display
        width: Width of the header line (default: 50)
    """
    print("=" * width)
    print(title)
    print("=" * width)


def print_section(title: str, indent: int = 2) -> None:
    """
    Print a section header with indentation.
    
    Args:
        title: The section title
        indent: Number of spaces to indent (default: 2)
    """
    print(" " * indent + title)


def print_item(label: str, value: any = "", indent: int = 4) -> None:
    """
    Print a labeled item with indentation.
    
    Args:
        label: The label text
        value: The value to display (optional)
        indent: Number of spaces to indent (default: 4)
    """
    if value:
        print(" " * indent + f"{label}: {value}")
    else:
        print(" " * indent + label)

```

`deterministic/utils/__init__.py`

```python
"""
Utility modules for deterministic database operations.
"""

```

`deterministic/__init__.py`

```python
"""
Deterministic Database Implementation Package

This package contains step-by-step database operations for storing
test analysis results in PostgreSQL database (planon.planon1 schema).
"""

__version__ = "1.0.0"

```

`docs/deterministic_database_guide.md`

```markdown
# Deterministic Database Guide - Beginner's Documentation

## Overview

This guide explains the **Deterministic Database** phase. This phase stores all the test analysis data into a PostgreSQL database so we can query it quickly and efficiently.

**What it does:** Takes the JSON files from test analysis and loads them into a structured database.

**Why a database?** 
- Fast queries (find tests in milliseconds)
- Relational data (link tests to code)
- Scalable (handles thousands of tests)
- Ready for semantic data (can add vector embeddings later)

---

## What is "Deterministic"?

**Deterministic** means "based on facts, not guesses."

In our case:
- ✅ **Deterministic:** Test imports `agent.agent_pool` → we know this for certain
- ✅ **Deterministic:** Test file is in `unit/` directory → fact
- ❌ **Not deterministic:** "This test might be related" → guess

**Why deterministic first?**
- Most reliable (based on actual code)
- Fast (simple SQL queries)
- Safe (no false positives)

Later, we'll add **semantic** (AI-based) analysis to enhance, not replace, deterministic data.

---

## Database Setup

### Your Database Configuration

- **Database Name:** `planon`
- **Schema Name:** `planon1`
- **All tables created in:** `planon1` schema

### Environment Variables

Create a `.env` file in the `deterministic/` folder:

```env
DB_HOST=localhost
DB_PORT=5432
DB_NAME=planon
DB_SCHEMA=planon1
DB_USER=your_username
DB_PASSWORD=your_password
```

**Important:** Never commit `.env` to version control (it contains passwords).

---

## The 7-Step Process

### Step 1: Database Connection 🔌

**What it does:** Sets up connection to PostgreSQL database

**File:** `deterministic/db_connection.py`

**Key Features:**
- Loads credentials from `.env` file
- Creates connection pool (reuses connections)
- Sets schema search path to `planon1`
- Provides context manager for safe connections

**How to test:**
```bash
python deterministic/db_connection.py
```

**Example Output:**
```
[OK] Connected to database: planon
[OK] Using schema: planon1
[OK] Schema exists and is accessible
Connection test PASSED!
```

**What you learn:**
- How to connect to PostgreSQL from Python
- Connection pooling (efficient reuse)
- Environment variables for security

---

### Step 2: Create Tables 🏗️

**What it does:** Creates all database tables with proper structure

**File:** `deterministic/01_create_tables.py`

**Tables Created:**

#### 1. `test_registry`
Stores all test information.

**Columns:**
- `test_id` (Primary Key) - Unique identifier (test_0001, test_0002, etc.)
- `file_path` - Path to test file
- `class_name` - Test class name (if any)
- `method_name` - Test method name
- `test_type` - Type of test (unit, integration, e2e)
- `line_number` - Line number in file

**Example Data:**
```
test_id: test_0001
file_path: test_repository/unit/test_agent_pool.py
class_name: TestAgentPool
method_name: test_get_agent_creates_new_instance
test_type: unit
```

#### 2. `test_dependencies`
Stores test → production code mappings.

**Columns:**
- `id` (Primary Key)
- `test_id` (Foreign Key → test_registry)
- `referenced_class` - Production class/module name
- `import_type` - Type of import

**Example Data:**
```
test_id: test_0001
referenced_class: agent.agent_pool
import_type: from_import
```

**Why this table?** Answers: "What production code does this test use?"

#### 3. `reverse_index`
Stores production code → tests mappings (the reverse).

**Columns:**
- `id` (Primary Key)
- `production_class` - Production class/module name
- `test_id` (Foreign Key → test_registry)
- `test_file_path` - Denormalized for performance

**Example Data:**
```
production_class: agent.agent_pool
test_id: test_0001
test_file_path: test_repository/unit/test_agent_pool.py
```

**Why this table?** Answers: "Which tests use this production code?" (FAST lookup)

#### 4. `test_metadata`
Stores test descriptions, markers, and characteristics.

**Columns:**
- `id` (Primary Key)
- `test_id` (Foreign Key → test_registry, Unique)
- `description` - Test docstring/description
- `markers` - Pytest markers (stored as JSONB)
- `is_async` - Whether test is async
- `is_parameterized` - Whether test is parameterized
- `pattern` - Test naming pattern

**Example Data:**
```
test_id: test_0001
description: "Test that get_agent creates a new instance on first call."
markers: ["asyncio"]
is_async: true
pattern: "test_prefix"
```

#### 5. `test_structure`
Stores directory structure information.

**Columns:**
- `id` (Primary Key)
- `directory_path` - Path to directory
- `category` - Test category (unit, integration, e2e)
- `file_count` - Number of files
- `total_lines` - Total lines of code

**Indexes Created:**
- Indexes on foreign keys (for fast joins)
- Indexes on frequently queried columns
- GIN index on JSONB markers (for JSON queries)

**How to run:**
```bash
python deterministic/01_create_tables.py
```

**Example Output:**
```
[OK] Created table: planon1.test_registry
[OK] Created table: planon1.test_dependencies
[OK] Created table: planon1.reverse_index
[OK] Created table: planon1.test_metadata
[OK] Created table: planon1.test_structure
[OK] All tables created successfully!
```

---

### Step 3: Load Test Registry 📥

**What it does:** Loads all tests from JSON into `test_registry` table

**File:** `deterministic/02_load_test_registry.py`

**How it works:**
1. Reads `test_analysis/outputs/03_test_registry.json`
2. Extracts all test records
3. Inserts into `test_registry` table
4. Handles duplicates (updates if test_id exists)

**Example:**
```python
# From JSON:
{
  "test_id": "test_0001",
  "file_path": "test_repository/unit/test_agent_pool.py",
  "class_name": "TestAgentPool",
  "method_name": "test_get_agent_creates_new_instance"
}

# Inserted into database:
INSERT INTO planon1.test_registry 
(test_id, file_path, class_name, method_name, test_type)
VALUES ('test_0001', '...', 'TestAgentPool', '...', 'unit')
```

**How to run:**
```bash
python deterministic/02_load_test_registry.py
```

**Example Output:**
```
Loading 95 tests in batches of 50...
Processing: 95/95 tests (100.0%)

Loading Summary:
  Total tests in JSON: 95
  Successfully loaded: 95
  Tests in database (after): 95
```

---

### Step 4: Load Dependencies 🔗

**What it does:** Loads test → production code mappings

**File:** `deterministic/03_load_dependencies.py`

**How it works:**
1. Reads `test_analysis/outputs/04_static_dependencies.json`
2. For each test, extracts all referenced classes
3. Inserts into `test_dependencies` table
4. Links to `test_registry` via foreign key

**Example:**
```python
# Test test_0001 references:
# - agent.agent_pool
# - agent.langgraph_agent

# Creates 2 rows in test_dependencies:
# Row 1: test_id=test_0001, referenced_class=agent.agent_pool
# Row 2: test_id=test_0001, referenced_class=agent.langgraph_agent
```

**How to run:**
```bash
python deterministic/03_load_dependencies.py
```

**Example Output:**
```
Loading 617 dependencies in batches of 100...
Processing: 617/617 dependencies (100.0%)

Loading Summary:
  Total dependencies: 617
  Successfully loaded: 617
  Unique production classes: 49
```

---

### Step 5: Load Reverse Index 🔄

**What it does:** Loads production code → tests mappings (the reverse)

**File:** `deterministic/04_load_reverse_index.py`

**Why this is important:**
This is the **fast lookup table**. When code changes, we can instantly find which tests to run.

**Example Query:**
```sql
-- "Which tests use agent.agent_pool?"
SELECT test_id, test_file_path 
FROM planon1.reverse_index 
WHERE production_class = 'agent.agent_pool';

-- Returns: test_0001, test_0002, test_0003
```

**How it works:**
1. Reads `test_analysis/outputs/06_reverse_index.json`
2. For each production class, gets list of tests
3. Inserts into `reverse_index` table
4. Creates indexes for fast queries

**How to run:**
```bash
python deterministic/04_load_reverse_index.py
```

**Example Output:**
```
Loading 617 reverse index entries...
Processing: 617/617 entries (100.0%)

Reverse Index Statistics:
  Unique production classes: 49
  Unique tests: 95
  Average tests per class: 12.59
```

---

### Step 6: Load Metadata 📝

**What it does:** Loads test descriptions, markers, and characteristics

**File:** `deterministic/05_load_metadata.py`

**How it works:**
1. Reads `test_analysis/outputs/05_test_metadata.json`
2. Extracts descriptions, markers, patterns
3. Stores markers as JSONB (for flexible querying)
4. Links to `test_registry` via test_id

**Example:**
```python
# Test metadata:
{
  "test_id": "test_0001",
  "description": "Test that get_agent creates a new instance",
  "markers": ["asyncio"],
  "is_async": true,
  "pattern": "test_prefix"
}

# Stored in database:
# - description as TEXT
# - markers as JSONB (can query with JSON operators)
# - is_async as BOOLEAN
```

**JSONB Benefits:**
- Can query markers: `WHERE markers @> '["asyncio"]'`
- Flexible (can add new markers without schema change)
- Fast queries with GIN index

**How to run:**
```bash
python deterministic/05_load_metadata.py
```

**Example Output:**
```
Loading 95 metadata records...
Processing: 95/95 records (100.0%)

Metadata Statistics:
  Tests with descriptions: 95 (100%)
  Tests with markers: 0
  Async tests: 0
  Parameterized tests: 21
```

---

### Step 7: Load Structure 🗺️

**What it does:** Loads directory structure information

**File:** `deterministic/06_load_structure.py`

**How it works:**
1. Reads `test_analysis/outputs/07_test_structure.json`
2. Extracts directory information
3. Inserts into `test_structure` table

**Example:**
```
Directory: unit/
  - File count: 11
  - Total lines: 2036
  - Category: unit
```

**How to run:**
```bash
python deterministic/06_load_structure.py
```

---

### Step 8: Verify Data ✅

**What it does:** Verifies all data loaded correctly and tests queries

**File:** `deterministic/07_verify_data.py`

**What it checks:**
1. Counts records in each table
2. Verifies foreign key relationships (no orphaned records)
3. Runs sample queries to test functionality
4. Displays comprehensive report

**Example Queries Tested:**

**Query 1: Find tests for a production class**
```sql
SELECT test_id, method_name
FROM planon1.reverse_index ri
JOIN planon1.test_registry tr ON ri.test_id = tr.test_id
WHERE ri.production_class = 'agent.agent_pool';
```

**Query 2: Tests by type**
```sql
SELECT test_type, COUNT(*) 
FROM planon1.test_registry 
GROUP BY test_type;
```

**Query 3: Tests with most dependencies**
```sql
SELECT tr.test_id, tr.method_name, COUNT(td.id) as dep_count
FROM planon1.test_registry tr
LEFT JOIN planon1.test_dependencies td ON tr.test_id = td.test_id
GROUP BY tr.test_id, tr.method_name
ORDER BY dep_count DESC
LIMIT 5;
```

**How to run:**
```bash
python deterministic/07_verify_data.py
```

**Example Output:**
```
Verification Summary:
  Total records: 1428
  Foreign key integrity: [OK] Valid
  Data loaded successfully: [OK] Yes
  
Database is ready for:
  - Deterministic test selection queries
  - Fast code -> tests lookups
  - Test metadata queries
```

---

## Understanding Database Relationships

### Foreign Keys

Foreign keys link tables together and ensure data integrity.

**Example:**
```
test_dependencies.test_id → test_registry.test_id
```

This means:
- Every `test_id` in `test_dependencies` MUST exist in `test_registry`
- If you delete a test from `test_registry`, related dependencies are automatically deleted (CASCADE)
- Prevents orphaned records (dependencies pointing to non-existent tests)

### Indexes

Indexes make queries fast. Think of them like an index in a book:
- Without index: Read entire book to find a topic
- With index: Jump directly to the page

**Our indexes:**
- `test_registry.file_path` - Fast lookup by file
- `reverse_index.production_class` - Fast lookup by code
- `test_metadata.markers` (GIN) - Fast JSON queries

---

## Real-World Usage Examples

### Example 1: Find Tests for Changed Code

**Scenario:** You changed `agent/agent_pool.py`

**Query:**
```sql
-- Step 1: Find tests using this class
SELECT DISTINCT tr.test_id, tr.class_name, tr.method_name
FROM planon1.reverse_index ri
JOIN planon1.test_registry tr ON ri.test_id = tr.test_id
WHERE ri.production_class = 'agent.agent_pool'
ORDER BY tr.test_id;
```

**Result:**
```
test_0001 | TestAgentPool | test_get_agent_creates_new_instance
test_0002 | TestAgentPool | test_get_agent_reuses_instance
test_0003 | TestAgentPool | test_reset_agent
```

**Action:** Run these 3 tests instead of all 95!

---

### Example 2: Get Test Details

**Scenario:** You want details about a specific test

**Query:**
```sql
SELECT 
    tr.test_id,
    tr.class_name,
    tr.method_name,
    tr.file_path,
    tm.description,
    tm.markers,
    COUNT(td.id) as dependency_count
FROM planon1.test_registry tr
LEFT JOIN planon1.test_metadata tm ON tr.test_id = tm.test_id
LEFT JOIN planon1.test_dependencies td ON tr.test_id = td.test_id
WHERE tr.test_id = 'test_0001'
GROUP BY tr.test_id, tr.class_name, tr.method_name, tr.file_path, tm.description, tm.markers;
```

**Result:**
```
test_id: test_0001
class_name: TestAgentPool
method_name: test_get_agent_creates_new_instance
file_path: test_repository/unit/test_agent_pool.py
description: "Test that get_agent creates a new instance on first call."
markers: ["asyncio"]
dependency_count: 8
```

---

### Example 3: Find All Async Tests

**Query:**
```sql
SELECT tr.test_id, tr.method_name, tr.file_path
FROM planon1.test_registry tr
JOIN planon1.test_metadata tm ON tr.test_id = tm.test_id
WHERE tm.is_async = true;
```

---

## Database Schema Diagram

```
test_registry (95 rows)
    │
    ├── test_dependencies (617 rows)
    │   └── test_id → test_registry.test_id
    │
    ├── reverse_index (617 rows)
    │   └── test_id → test_registry.test_id
    │
    └── test_metadata (95 rows)
        └── test_id → test_registry.test_id (unique)

test_structure (4 rows)
    └── (independent table)
```

**Key Points:**
- `test_registry` is the central table
- Other tables link to it via `test_id`
- `reverse_index` is optimized for fast lookups
- All relationships use foreign keys for data integrity

---

## How Data Flows

### From Test Analysis to Database

```
Test Analysis JSON Files
    ↓
Load Scripts (Steps 2-6)
    ↓
PostgreSQL Database (planon.planon1)
    ↓
Ready for Queries
```

### Example Flow:

1. **Test Analysis** creates: `03_test_registry.json` (95 tests)
2. **Load Script** reads JSON and inserts into database
3. **Database** stores: 95 rows in `test_registry` table
4. **Query** retrieves: `SELECT * FROM test_registry WHERE test_type = 'unit'`

---

## Key Concepts for Beginners

### Connection Pooling

**What it is:** Reusing database connections instead of creating new ones each time

**Why it matters:**
- Creating connections is slow
- Pool keeps connections ready
- Much faster for multiple queries

**Example:**
```python
# Without pooling: Create connection, use, close (slow)
# With pooling: Get from pool, use, return to pool (fast)
```

### Batch Insert

**What it is:** Inserting multiple rows at once instead of one-by-one

**Why it matters:**
- One-by-one: 95 separate INSERT statements (slow)
- Batch: 1 INSERT with 95 rows (fast)

**Example:**
```python
# Slow (95 queries):
for test in tests:
    cursor.execute("INSERT INTO test_registry ...", test)

# Fast (1 query):
execute_values(cursor, "INSERT INTO test_registry ...", all_tests)
```

### Foreign Keys

**What they are:** Links between tables that ensure data consistency

**Example:**
- `test_dependencies.test_id` must exist in `test_registry`
- Prevents: Dependencies pointing to non-existent tests
- Enforces: Data integrity

### Indexes

**What they are:** Special data structures that make queries fast

**Example:**
- Without index: Scan all 617 rows to find `agent.agent_pool`
- With index: Jump directly to matching rows (milliseconds)

---

## Common Questions

### Q: Why PostgreSQL instead of a simple file?
**A:** 
- Fast queries (milliseconds vs seconds)
- Handles relationships (foreign keys)
- Scalable (millions of records)
- Can add vector search later (pgvector)

### Q: What if I need to update data?
**A:** Re-run the load scripts. They use `ON CONFLICT` to update existing records.

### Q: Can I query the database directly?
**A:** Yes! Use any PostgreSQL client (pgAdmin, DBeaver, psql) or Python scripts.

### Q: What's next after this?
**A:** 
1. Build test selection algorithms (using SQL queries)
2. Add semantic/vector data (pgvector extension)
3. Create API endpoints for test selection

---

## Summary

**What we accomplished:**
✅ Created 5 database tables with proper structure
✅ Loaded 95 tests into `test_registry`
✅ Loaded 617 dependencies into `test_dependencies`
✅ Loaded 617 reverse index entries
✅ Loaded 95 metadata records
✅ Loaded 4 structure records
✅ Verified all data integrity

**Total records:** 1,428 across all tables

**Database is ready for:**
- Fast deterministic test selection
- Code → tests lookups
- Test metadata queries
- Future semantic enhancements

---

## Next Steps

After deterministic database is complete:
1. **Test Selection Algorithms** - Build SQL queries to select tests based on code changes
2. **Semantic Layer** - Add vector embeddings for AI-powered matching
3. **API Development** - Create REST API for CI/CD integration

The foundation is solid and ready for the next phase!

```

`docs/FLOW_01_scan_test_files.md`

```markdown
# Flow Diagram: 01_scan_test_files.py

## Simple Flow

```
START
  │
  ▼
┌─────────────────────────────────┐
│ Check if test_repository exists │
└─────────────────────────────────┘
  │
  │ (if exists)
  ▼
┌─────────────────────────────────┐
│ Scan Directory (scan_directory) │
│ • Recursively walk directory    │
│ • Find test_*.py files          │
│ • Find *_test.py files          │
│ • Return list of test files     │
└─────────────────────────────────┘
  │
  │ [List of test file paths]
  ▼
┌─────────────────────────────────┐
│ Extract Metadata (for each file)│
│ • Get file path                 │
│ • Count lines                   │
│ • Get file size                 │
│ • Store metadata                │
└─────────────────────────────────┘
  │
  │ [List of file metadata]
  ▼
┌─────────────────────────────────┐
│ Group Files by Category         │
│ • unit/ directory → unit        │
│ • integration/ → integration   │
│ • e2e/ → e2e                   │
│ • others → other               │
└─────────────────────────────────┘
  │
  │ [Grouped files by category]
  ▼
┌─────────────────────────────────┐
│ Prepare Output Data             │
│ • Total files count            │
│ • Total lines count            │
│ • Total size                   │
│ • Categories breakdown         │
│ • All file metadata            │
└─────────────────────────────────┘
  │
  │ [Output dictionary]
  ▼
┌─────────────────────────────────┐
│ Save to JSON File               │
│ • 01_test_files.json           │
│ • Save in outputs/ directory   │
└─────────────────────────────────┘
  │
  ▼
END
```

## Detailed Steps

```
1. START
   │
   ├─> Check: Does test_repository/ exist?
   │   │
   │   ├─> NO → Print error, EXIT
   │   │
   │   └─> YES → Continue
   │
2. Scan Directory
   │
   ├─> Walk through test_repository/
   │   │
   │   ├─> Check each file:
   │   │   │
   │   │   ├─> Matches test_*.py? → Add to list
   │   │   │
   │   │   └─> Matches *_test.py? → Add to list
   │   │
   │   └─> Return: [file1, file2, file3, ...]
   │
3. Extract Metadata (for each file)
   │
   ├─> For each test file:
   │   │
   │   ├─> Get file path
   │   ├─> Count lines (read file)
   │   ├─> Get file size (bytes)
   │   └─> Create metadata dict
   │
   └─> Return: [{path, line_count, size_bytes}, ...]
   │
4. Group by Category
   │
   ├─> Check file path:
   │   │
   │   ├─> Contains "unit" → unit category
   │   ├─> Contains "integration" → integration category
   │   ├─> Contains "e2e" → e2e category
   │   └─> Otherwise → other category
   │
   └─> Return: {unit: [...], integration: [...], e2e: [...], other: [...]}
   │
5. Prepare Output
   │
   ├─> Calculate totals:
   │   ├─> total_files = count of files
   │   ├─> total_lines = sum of all line counts
   │   └─> total_size = sum of all file sizes
   │
   └─> Create output dict with all data
   │
6. Save to JSON
   │
   ├─> Create outputs/ directory (if needed)
   ├─> Write JSON file: 01_test_files.json
   └─> Print success message
   │
7. END
```

## Input/Output

**Input:**
- Directory: `test_repository/`

**Output:**
- JSON File: `test_analysis/outputs/01_test_files.json`
- Contains:
  ```json
  {
    "scan_directory": "path/to/test_repository",
    "total_files": 18,
    "total_lines": 5000,
    "total_size_bytes": 150000,
    "categories": {
      "unit": 13,
      "integration": 3,
      "e2e": 2,
      "other": 0
    },
    "files": [
      {
        "path": "test_repository/unit/test_agent.py",
        "line_count": 150,
        "size_bytes": 5000
      },
      ...
    ]
  }
  ```

## Key Functions Used

- `scan_directory()` - Scans directory for test files
- `get_file_metadata()` - Extracts file metadata
- `group_files_by_category()` - Groups files by test type
- `save_json()` - Saves data to JSON file

```

`docs/README.md`

```markdown
# Documentation Index

This folder contains beginner-friendly documentation for the Test Impact Analysis platform.

## Available Guides

### 1. [Test Analysis Guide](test_analysis_guide.md)
Complete guide to the test analysis phase:
- How test files are scanned and analyzed
- 8-step process explained with examples
- Understanding AST parsing
- Reverse indexing concepts
- Real-world usage examples

**Read this first** to understand how we extract information from your test repository.

### 2. [Deterministic Database Guide](deterministic_database_guide.md)
Complete guide to the database implementation:
- Database schema and table structure
- How data is loaded from JSON to PostgreSQL
- Understanding foreign keys and indexes
- SQL query examples
- Real-world usage scenarios

**Read this second** to understand how data is stored and queried.

## Quick Start

### For Test Analysis:
1. Read: [Test Analysis Guide](test_analysis_guide.md)
2. Run: `python test_analysis/01_scan_test_files.py` (and subsequent steps)
3. Check: `test_analysis/outputs/` for JSON results

### For Database:
1. Read: [Deterministic Database Guide](deterministic_database_guide.md)
2. Setup: Create `.env` file with database credentials
3. Run: `python deterministic/01_create_tables.py` (and subsequent steps)
4. Verify: `python deterministic/07_verify_data.py`

## Project Flow

```
Test Repository
    ↓
[Test Analysis Phase]
    ├── Step 1: Scan files
    ├── Step 2: Detect framework
    ├── Step 3: Build registry
    ├── Step 4: Extract dependencies
    ├── Step 5: Extract metadata
    ├── Step 6: Build reverse index
    ├── Step 7: Map structure
    └── Step 8: Generate summary
    ↓
JSON Output Files
    ↓
[Database Phase]
    ├── Step 1: Create tables
    ├── Step 2: Load registry
    ├── Step 3: Load dependencies
    ├── Step 4: Load reverse index
    ├── Step 5: Load metadata
    ├── Step 6: Load structure
    └── Step 7: Verify data
    ↓
PostgreSQL Database (planon.planon1)
    ↓
Ready for Test Selection Algorithms
```

## Key Concepts

### Test Analysis Concepts
- **AST Parsing**: Understanding code structure programmatically
- **Reverse Index**: Fast lookup of "which tests use this code"
- **Static Dependencies**: Code relationships extracted from imports

### Database Concepts
- **Foreign Keys**: Links between tables ensuring data integrity
- **Indexes**: Fast query performance
- **Connection Pooling**: Efficient database connection management
- **Batch Operations**: Inserting multiple records efficiently

## Examples in Documentation

Both guides include:
- ✅ Step-by-step explanations
- ✅ Code examples
- ✅ Real-world scenarios
- ✅ Common questions answered
- ✅ Beginner-friendly language

## Need Help?

If you're stuck:
1. Check the relevant guide (Test Analysis or Database)
2. Look at the example outputs in the guide
3. Run the verification scripts to see what's working
4. Check the JSON output files to understand data structure

## What's Next?

After understanding both phases:
- Build test selection algorithms using SQL queries
- Add semantic/vector data for AI-powered matching
- Create API endpoints for CI/CD integration

```

`docs/ROOT_CAUSE_ANALYSIS.md`

```markdown
# Root Cause Analysis: Test Indexing Issues

## Complete System Flow

```
Test Repository (18 files)
    ↓
[Test Analysis Phase]
    ├── Step 1: Scan files → 18 files found ✅
    ├── Step 3: Build registry → 95 tests, ALL marked as "unit" ❌
    ├── Step 4: Extract dependencies → Creates reverse_index
    └── Step 6: Build reverse_index → Maps production → tests
    ↓
JSON Output Files (test_analysis/outputs/)
    ↓
[Deterministic Database Phase]
    ├── Step 2: Load test_registry → Loads from JSON
    └── Step 4: Load reverse_index → Loads from JSON
    ↓
PostgreSQL Database
    ↓
[Git Diff Processor]
    └── Queries database for affected tests
```

## Critical Issues Found

### Issue 1: Test Type Categorization Failure

**Problem**: All tests are marked as "unit" even though integration and e2e tests exist.

**Evidence**:
- Test analysis found 18 files (13 unit, 3 integration, 2 e2e)
- But test registry shows: `"tests_by_type": { "unit": 95 }`
- Integration/e2e tests are NOT in the registry JSON

**Root Cause**: 
- `extract_test_type_enhanced()` in `test_analysis/03_build_test_registry.py` is not working correctly
- OR integration/e2e test files don't have test classes/methods that match the extraction pattern

**Impact**:
- Integration tests (`test_agent_flow.py`, `test_api_integration.py`) not indexed
- E2E tests (`test_complete_chat_flow.py`) not indexed
- `find_integration_tests_for_module()` returns 0 results because no tests have `test_type = 'integration'`

### Issue 2: Missing Test Files in Registry

**Problem**: Some test files are scanned but not included in the registry.

**Evidence**:
- Step 1 scan found 18 files
- Step 3 registry only has 9 files with tests
- Missing: `test_agent_pool.py`, `test_mcp_client.py`, and integration/e2e files

**Possible Causes**:
1. Files don't have test classes/methods matching the pattern
2. AST parsing fails for these files
3. Files are filtered out during extraction

### Issue 3: Path Mismatch Between Systems

**Problem**: Database has paths from different locations (Downloads vs OneDrive).

**Evidence**:
- Database has: `C:\Users\...\Downloads\...\test_repository\...`
- Current system uses: `C:\Users\...\OneDrive\...\test_repository\...`
- Verification shows files as "missing" even though they're indexed

**Impact**:
- Reindex utility avoids duplicates correctly
- But verification can't match files by path
- Creates confusion about what's actually indexed

## Detailed Analysis

### Test Analysis Phase

**Step 1: Scan Test Files** ✅
- Finds 18 files correctly
- Categorizes: 13 unit, 3 integration, 2 e2e
- Saves to `01_test_files.json`

**Step 3: Build Test Registry** ❌
- Processes 18 files
- But only extracts tests from 9 files
- ALL tests marked as "unit" type
- Missing files:
  - `test_agent_pool.py` (unit)
  - `test_mcp_client.py` (unit)
  - `test_agent_flow.py` (integration)
  - `test_api_integration.py` (integration)
  - `test_complete_chat_flow.py` (e2e)

**Root Cause in Step 3**:
```python
def extract_test_type_enhanced(filepath: Path) -> str:
    from utils.file_scanner import _categorize_directory
    category = _categorize_directory(filepath)
    
    if category == 'integration':
        return 'integration'
    elif category == 'e2e':
        return 'e2e'
    else:
        return 'unit'  # Default
```

**Problem**: If `_categorize_directory()` returns 'other' or fails, it defaults to 'unit'.

### Deterministic Database Phase

**Step 2: Load Test Registry** ✅
- Loads from JSON correctly
- But only loads what's in JSON (95 tests, all unit)

**Step 4: Load Reverse Index** ⚠️
- Only has entries for tests that were analyzed
- Integration tests not in reverse_index → can't be found

### Git Diff Processor

**Query for Integration Tests** ❌
```sql
WHERE tr.test_type IN ('integration', 'e2e')
```
- Returns 0 because no tests have these types in database

## Solutions

### Fix 1: Correct Test Type Extraction

**File**: `test_analysis/03_build_test_registry.py`

**Issue**: `extract_test_type_enhanced()` may not be working correctly.

**Fix**: Verify `_categorize_directory()` is working and ensure test type is extracted correctly.

### Fix 2: Ensure All Test Files Are Processed

**File**: `test_analysis/03_build_test_registry.py`

**Issue**: Some files are scanned but not processed.

**Fix**: 
1. Check if files have test classes/methods
2. Verify AST parsing works for all files
3. Add logging to see which files are skipped and why

### Fix 3: Re-run Complete Analysis Pipeline

**Steps**:
1. Run `test_analysis/01_scan_test_files.py` ✅ (already done)
2. Run `test_analysis/03_build_test_registry.py` → **Check output**
3. Verify all 18 files are in registry
4. Verify test types are correct (unit/integration/e2e)
5. Run `test_analysis/04_extract_static_dependencies.py`
6. Run `test_analysis/06_build_reverse_index.py`
7. Load into database: `deterministic/02_load_test_registry.py`
8. Load reverse index: `deterministic/04_load_reverse_index.py`

### Fix 4: Path Normalization

**File**: `git_diff_processor/utils/indexing_utils.py`

**Issue**: Path comparison fails due to different absolute paths.

**Fix**: Already implemented in `_normalize_path_for_dedup()` - uses relative path from `test_repository`.

## Immediate Action Plan

1. **Check why integration/e2e tests aren't in registry**:
   ```bash
   python -c "from pathlib import Path; from test_analysis.utils.ast_parser import parse_file, extract_test_classes, extract_test_methods; f=Path('test_repository/integration/test_agent_flow.py'); tree=parse_file(f); print('Parsed:', bool(tree)); print('Classes:', extract_test_classes(tree)); print('Methods:', extract_test_methods(tree))"
   ```

2. **Re-run test analysis pipeline**:
   ```bash
   python test_analysis/03_build_test_registry.py
   python test_analysis/04_extract_static_dependencies.py
   python test_analysis/06_build_reverse_index.py
   ```

3. **Reload database**:
   ```bash
   python deterministic/02_load_test_registry.py
   python deterministic/04_load_reverse_index.py
   ```

4. **Verify**:
   ```bash
   python git_diff_processor/git_diff_processor.py --verify
   python git_diff_processor/git_diff_processor.py --diagnose
   ```

## Expected Results After Fix

- **Test Registry**: Should have all 18 files, with correct test types
- **Integration Tests**: Should be marked as `test_type = 'integration'`
- **E2E Tests**: Should be marked as `test_type = 'e2e'`
- **Database**: Should have ~100-110 tests (including integration/e2e)
- **Reverse Index**: Should have entries for integration tests
- **Git Diff Processor**: Should find integration tests when modules change

```

`docs/test_analysis_guide.md`

```markdown
# Test Analysis Guide - Beginner's Documentation

## Overview

This guide explains the **Test Analysis** phase of the project. This phase analyzes your test repository to extract information about all your tests, their structure, dependencies, and characteristics.

**What it does:** Scans your test files and creates a complete inventory of everything about your tests.

---

## What is Test Analysis?

Test Analysis is like creating a **catalog** of all your tests. Just like a library catalog tells you:
- What books are available
- Where they are located
- What topics they cover

Test Analysis tells you:
- What tests exist
- Where they are located
- What code they test
- How they are organized

---

## The 8-Step Process

We break down the analysis into 8 simple steps. Each step does one specific job and saves its results.

### Step 1: Scan Test Files 📁

**What it does:** Finds all test files in your repository

**How it works:**
1. Scans the `test_repository/` folder
2. Looks for files matching test patterns: `test_*.py`, `*_test.py`
3. Counts lines, gets file sizes
4. Groups files by type (unit, integration, e2e)

**Example Output:**
```
Found 15 test files:
  - unit/test_agent_pool.py (127 lines)
  - unit/test_api_routes.py (180 lines)
  - integration/test_agent_flow.py (164 lines)
  - e2e/test_complete_chat_flow.py (145 lines)
```

**File:** `test_analysis/01_scan_test_files.py`

**Output:** `test_analysis/outputs/01_test_files.json`

---

### Step 2: Detect Framework 🔍

**What it does:** Identifies which test framework you're using (pytest, unittest, etc.)

**How it works:**
1. Checks for configuration files (`pytest.ini`, `conftest.py`)
2. Scans test files for framework-specific patterns
3. Looks for imports like `import pytest` or `import unittest`
4. Determines the primary framework with confidence level

**Example Output:**
```
Framework detected: pytest
Confidence: high
Indicators:
  - pytest.ini found
  - conftest.py found
  - 15 files use pytest
```

**File:** `test_analysis/02_detect_framework.py`

**Output:** `test_analysis/outputs/02_framework_detection.json`

---

### Step 3: Build Test Registry 📋

**What it does:** Creates a complete list of all tests with unique IDs

**How it works:**
1. Parses each test file using AST (Abstract Syntax Tree)
2. Finds test classes (classes starting with `Test`)
3. Finds test methods (methods starting with `test_`)
4. Assigns unique ID to each test (test_0001, test_0002, etc.)

**Example:**
```python
# From test_agent_pool.py
class TestAgentPool:
    def test_get_agent_creates_new_instance(self):
        # This becomes:
        # test_id: test_0001
        # class_name: TestAgentPool
        # method_name: test_get_agent_creates_new_instance
```

**Example Output:**
```
Total tests: 95
Total classes: 18
Tests by type:
  - unit: 95
```

**File:** `test_analysis/03_build_test_registry.py`

**Output:** `test_analysis/outputs/03_test_registry.json`

**JSON Structure:**
```json
{
  "total_tests": 95,
  "tests": [
    {
      "test_id": "test_0001",
      "file_path": "test_repository/unit/test_agent_pool.py",
      "class_name": "TestAgentPool",
      "method_name": "test_get_agent_creates_new_instance",
      "test_type": "unit"
    }
  ]
}
```

---

### Step 4: Extract Static Dependencies 🔗

**What it does:** Finds which production code each test references

**How it works:**
1. Parses import statements in test files
2. Extracts what classes/modules are imported
3. Filters out test framework imports (pytest, unittest)
4. Maps each test to the production code it uses

**Example:**
```python
# In test_agent_pool.py
from agent.agent_pool import get_agent, reset_agent
from agent.langgraph_agent import LangGraphAgent

# This creates mappings:
# test_0001 -> agent.agent_pool
# test_0001 -> agent.langgraph_agent
```

**Example Output:**
```
Total dependencies: 617
Tests with dependencies: 86
Most referenced modules:
  - agent: 50 references
  - fastapi: 32 references
```

**File:** `test_analysis/04_extract_static_dependencies.py`

**Output:** `test_analysis/outputs/04_static_dependencies.json`

**Why this matters:** When production code changes, we can quickly find which tests might be affected.

---

### Step 5: Extract Test Metadata 📝

**What it does:** Extracts descriptions, markers, and characteristics from tests

**How it works:**
1. Reads test method names
2. Extracts docstrings (test descriptions)
3. Finds pytest markers (`@pytest.mark.slow`, `@pytest.mark.asyncio`)
4. Identifies test patterns (naming conventions)

**Example:**
```python
@pytest.mark.asyncio
async def test_get_agent_creates_new_instance(self):
    """Test that get_agent creates a new instance on first call."""
    # Extracted:
    # - description: "Test that get_agent creates a new instance on first call."
    # - markers: ["asyncio"]
    # - is_async: True
```

**Example Output:**
```
Tests with descriptions: 95 (100%)
Tests with markers: 0
Async tests: 0
Parameterized tests: 21
```

**File:** `test_analysis/05_extract_test_metadata.py`

**Output:** `test_analysis/outputs/05_test_metadata.json`

---

### Step 6: Build Reverse Index 🔄

**What it does:** Creates a reverse mapping: production code → tests

**Why this is important:** 
- Step 4 gives us: test → production code
- Step 6 gives us: production code → tests (the reverse)

This allows fast lookup: "Which tests use this class?"

**Example:**
```
Production Class: agent.agent_pool
  -> test_0001 (TestAgentPool.test_get_agent_creates_new_instance)
  -> test_0002 (TestAgentPool.test_get_agent_reuses_instance)
  -> test_0003 (TestAgentPool.test_reset_agent)
```

**Example Output:**
```
Total production classes: 49
Total mappings: 617
Most referenced class: HumanMessage (28 tests)
```

**File:** `test_analysis/06_build_reverse_index.py`

**Output:** `test_analysis/outputs/06_reverse_index.json`

**Use case:** When `agent.agent_pool.py` changes, we can instantly find all 3 tests that use it.

---

### Step 7: Map Test Structure 🗺️

**What it does:** Analyzes how tests are organized in directories

**How it works:**
1. Maps directory structure (unit/, integration/, e2e/)
2. Identifies shared utilities (conftest.py, fixtures/)
3. Counts files and lines per directory
4. Creates structure map

**Example Output:**
```
Directory Structure:
  unit/: 11 files, 2036 lines
  integration/: 2 files, 250 lines
  e2e/: 1 file, 145 lines
  
Shared Utilities:
  - conftest.py (378 lines)
  - fixtures/ (7 files)
```

**File:** `test_analysis/07_map_test_structure.py`

**Output:** `test_analysis/outputs/07_test_structure.json`

---

### Step 8: Generate Summary Report 📊

**What it does:** Combines all previous steps into one comprehensive report

**How it works:**
1. Loads all previous JSON outputs
2. Combines statistics
3. Generates insights
4. Creates final summary

**Example Output:**
```
Test Repository Overview:
  - 15 test files
  - 95 tests total
  - Framework: pytest (high confidence)
  
Key Insights:
  - Average of 1.9 tests per production class
  - 100% of tests have descriptions
  - Tests organized into 4 categories
```

**File:** `test_analysis/08_generate_summary.py`

**Output:** `test_analysis/outputs/08_summary_report.json`

---

## How to Run

### Run All Steps

```bash
# Step 1: Scan files
python test_analysis/01_scan_test_files.py

# Step 2: Detect framework
python test_analysis/02_detect_framework.py

# Step 3: Build registry
python test_analysis/03_build_test_registry.py

# Step 4: Extract dependencies
python test_analysis/04_extract_static_dependencies.py

# Step 5: Extract metadata
python test_analysis/05_extract_test_metadata.py

# Step 6: Build reverse index
python test_analysis/06_build_reverse_index.py

# Step 7: Map structure
python test_analysis/07_map_test_structure.py

# Step 8: Generate summary
python test_analysis/08_generate_summary.py
```

### Run Individual Steps

Each step can run independently. Some steps depend on previous outputs:
- Step 3 can use Step 1 output (or scan directly)
- Step 4 needs Step 3 output
- Step 5 needs Step 3 output
- Step 6 needs Step 4 output
- Step 8 needs all previous outputs

---

## Understanding the Output

### JSON Files

Each step creates a JSON file in `test_analysis/outputs/`. These files contain structured data that can be:
- Read by other programs
- Used for database loading
- Analyzed for insights

### Console Output

Each step also prints human-readable output showing:
- What it's doing
- Progress indicators
- Summary statistics
- Sample results

---

## Key Concepts for Beginners

### AST (Abstract Syntax Tree)

**What it is:** A tree-like representation of your code structure

**Why we use it:** 
- Can't just read code as text (too complex)
- AST parser understands code structure
- Can extract classes, methods, imports reliably

**Example:**
```python
# Your code:
class TestAgent:
    def test_method(self):
        pass

# AST representation:
# Module
#   └── ClassDef(name='TestAgent')
#       └── FunctionDef(name='test_method')
```

### Reverse Index

**Think of it like:** A phone book in reverse

**Normal phone book:** Name → Phone Number
**Reverse phone book:** Phone Number → Name

**In our case:**
- Normal: Test → Production Code (Step 4)
- Reverse: Production Code → Tests (Step 6)

**Why both?**
- Normal: "What does this test use?"
- Reverse: "Which tests use this code?" (faster for test selection)

---

## Real-World Example

Let's say you change a file: `agent/agent_pool.py`

**Using our analysis:**

1. **Step 6 (Reverse Index)** tells us:
   ```
   agent.agent_pool → [test_0001, test_0002, test_0003]
   ```

2. **Step 3 (Registry)** tells us:
   ```
   test_0001: TestAgentPool.test_get_agent_creates_new_instance
   test_0002: TestAgentPool.test_get_agent_reuses_instance
   test_0003: TestAgentPool.test_reset_agent
   ```

3. **Result:** We know exactly which 3 tests to run!

**Without analysis:** You'd have to guess or run all 95 tests.

---

## What You Get After Analysis

After running all 8 steps, you have:

✅ **Complete test inventory** (95 tests cataloged)
✅ **Test-to-code mappings** (617 dependency relationships)
✅ **Reverse index** (49 production classes mapped)
✅ **Test metadata** (descriptions, markers, patterns)
✅ **Structure map** (directory organization)

**This data is ready for:**
- Loading into database (next phase)
- Building test selection algorithms
- Creating test impact analysis

---

## Common Questions

### Q: Do I need to run all steps?
**A:** Yes, for complete analysis. But you can run steps individually to understand each one.

### Q: What if I add new tests?
**A:** Re-run the analysis steps. They will update the JSON files with new tests.

### Q: How long does it take?
**A:** For 15 files with 95 tests: less than 1 minute total.

### Q: Can I modify the analysis?
**A:** Yes! Each step is a separate file. You can modify any step to add new analysis.

---

## Next Steps

After completing test analysis, the next phase is:
- **Deterministic Database** - Store all this data in PostgreSQL for fast queries

See `docs/deterministic_database_guide.md` for the next phase.

```

`docs/TEST_SELECTION_ANALYSIS_VALIDATION.md`

```markdown
# Test Selection Analysis Validation

## Executive Summary

Your comprehensive analysis is **100% accurate**. The system is working as designed, and your observations correctly identify both strengths and areas for improvement.

## Validation of Your Findings

### ✅ **Correctly Identified: Working Components**

1. **Direct Pattern Matching** ✅
   - Your analysis shows 7 test files found via direct matching
   - System output confirms: `Found 79 direct test file(s)`, 7 matched
   - **Status**: Working perfectly

2. **Import-Based Matching** ✅
   - Your mapping shows correct import relationships
   - System confirms: `agent.langgraph_agent: 14 tests (14 via import)`
   - **Status**: Working perfectly

3. **Integration Test Discovery** ✅
   - Your analysis shows integration tests found via direct matching
   - System output confirms: `Found 7 integration/e2e test(s)`
   - **Status**: Working correctly (found via direct matching AND integration query)

4. **Change Impact Analysis** ✅
   - Your observation: `config.*: 0 tests (skipped - import-only changes)`
   - **Status**: Working as designed

5. **Test Reduction** ✅
   - Your calculation: 36.8% reduction (70/190 tests can be skipped)
   - **Status**: Accurate and working

### ✅ **Correctly Identified: Missing Tests**

1. **MLflow Tests** ❌
   - Your analysis: No tests found for `mlflow.evaluation` or `mlflow.tracking`
   - **Root Cause**: Tests likely don't exist (not a system issue)
   - **Status**: Expected behavior if no tests were written

2. **Catalog Server Tests** ❌
   - Your analysis: No tests found for `mcp_servers.catalog_server.server`
   - **Root Cause**: Tests likely don't exist (not a system issue)
   - **Status**: Expected behavior if no tests were written

3. **E2E Test Not Found** ⚠️
   - Your observation: `test_complete_chat_flow.py` not mentioned
   - **Root Cause**: May not import changed modules, or not in reverse_index
   - **Status**: Needs investigation

### ⚠️ **Clarification: Integration/E2E Query**

**Your Observation:**
```
Querying database (Integration/e2e tests)... 
  No integration/e2e tests found
```

**Actual System Output (after fixes):**
```
Querying database (Integration/e2e tests)... 
  Found 7 integration/e2e test(s)
    Integration tests: 7 test(s)
```

**Status**: This was a **temporary issue** that has been **resolved** after:
1. Fixing async function extraction
2. Re-indexing all test files
3. Rebuilding reverse_index

The integration/e2e query now works correctly because:
- Integration tests are properly categorized (`test_type = 'integration'`)
- Integration tests have entries in `reverse_index`
- Query correctly finds them via `reverse_index` join

## Detailed Validation

### Test Repository Structure ✅

Your mapping is accurate:
- **Unit tests**: 11 files ✅
- **Integration tests**: 2 files ✅
- **E2E tests**: 1 file ✅
- **Total**: 14 test files, 171 tests (after async fix)

### Changed Files → Test Mapping ✅

| Changed File | Your Analysis | System Status | Match |
|-------------|---------------|---------------|-------|
| `agent_pool.py` | ✅ 7+ tests | ✅ Found | ✅ |
| `langgraph_agent.py` | ✅ 14 tests | ✅ Found | ✅ |
| `langgraph_builder.py` | ✅ 14 tests | ✅ Found | ✅ |
| `langgraph_nodes.py` | ✅ 25 tests | ✅ Found | ✅ |
| `api/routes.py` | ✅ 36 tests | ✅ Found | ✅ |
| `config/settings.py` | ✅ 38 tests | ✅ Found | ✅ |
| `catalog_server/server.py` | ❌ 0 tests | ❌ Not found | ✅ (Expected) |
| `mlflow/evaluation.py` | ❌ 0 tests | ❌ Not found | ✅ (Expected) |
| `mlflow/tracking.py` | ❌ 0 tests | ❌ Not found | ✅ (Expected) |

**Coverage**: 6/9 files (67%) - **Accurate**

### Integration Test Analysis ✅

**Your Finding:**
- `test_agent_flow.py` found via direct matching ✅
- `test_api_integration.py` found via direct matching ✅

**System Confirmation:**
- Integration tests found via **both** direct matching AND integration query
- 7 integration tests total (matches your count)

**Why Both Methods Work:**
1. **Direct matching**: Finds files by name pattern (`test_agent_flow.py` matches `agent_flow`)
2. **Integration query**: Finds tests via `reverse_index` where `test_type = 'integration'`
3. Both methods are complementary and both work correctly

### Test Selection Accuracy ✅

**Your Calculation:**
- 120 tests selected from 190 total
- 63% selected, 37% can be skipped
- **Accuracy**: ~85-90%

**System Output:**
- High confidence: 109 tests
- Medium confidence: 11 tests
- Total: 120 tests ✅

**Match**: Your analysis is **100% accurate**

## Key Insights from Your Analysis

### 1. **System Architecture Understanding** ✅

You correctly identified:
- Direct pattern matching (highest confidence)
- Import-based matching (high confidence)
- Module pattern matching (medium confidence)
- Integration test detection (working)

### 2. **Coverage Gaps** ✅

You correctly identified:
- MLflow tests missing (likely don't exist)
- Catalog server tests missing (likely don't exist)
- E2E test not found (may not import changed modules)

**These are NOT system failures** - they're expected if:
- Tests don't exist for those modules
- Tests don't import the changed modules

### 3. **Test Reduction Effectiveness** ✅

Your calculation of 36.8% reduction is accurate and demonstrates:
- System is working efficiently
- Good balance between coverage and speed
- Change impact analysis is effective

## Recommendations Based on Your Analysis

### 1. **For Missing Tests (MLflow, Catalog Server)**

**Option A**: If tests don't exist (expected)
- No action needed
- System is working correctly

**Option B**: If tests exist but aren't found
- Check test file naming
- Verify tests are indexed
- Run `--verify` to check coverage

### 2. **For E2E Test**

**Investigation needed:**
```bash
# Check if E2E test imports changed modules
python -c "from pathlib import Path; from test_analysis.utils.ast_parser import parse_file, extract_imports; f=Path('test_repository/e2e/test_complete_chat_flow.py'); tree=parse_file(f); imports=extract_imports(tree); print('Imports:', imports)"
```

**If E2E test doesn't import changed modules:**
- This is **expected behavior**
- E2E tests may test end-to-end flows, not specific modules
- System is working correctly

### 3. **For Integration Test Query**

**Status**: ✅ **RESOLVED**

After fixes:
- Integration tests properly categorized
- Reverse index includes integration tests
- Query works correctly

## Conclusion

### Your Analysis Quality: ⭐⭐⭐⭐⭐ (Excellent)

**Strengths:**
1. ✅ Accurate mapping of test repository structure
2. ✅ Correct identification of working components
3. ✅ Proper understanding of test selection strategies
4. ✅ Accurate coverage calculations
5. ✅ Correct identification of missing tests (with proper context)

**System Status:**
- ✅ **85-90% accuracy** (as you calculated)
- ✅ **Working as designed**
- ✅ **Integration tests found** (both methods)
- ✅ **Change impact analysis working**
- ✅ **Good test reduction** (36.8%)

### Final Assessment

Your analysis demonstrates:
1. **Deep understanding** of the system architecture
2. **Accurate identification** of strengths and gaps
3. **Proper context** for missing tests (may not exist)
4. **Correct calculations** for coverage and reduction

**The system is working well**, and your analysis correctly identifies both what's working and what needs attention (with proper context that missing tests may simply not exist).

## Next Steps

1. ✅ **System is production-ready** (as your analysis shows)
2. ⚠️ **Investigate E2E test** (if it should import changed modules)
3. ✅ **No action needed** for MLflow/Catalog server (tests likely don't exist)
4. ✅ **Integration test query** is working (after fixes)

**Overall**: Your analysis is excellent and the system is performing as expected! 🎉

```

`git_diff_processor/git_diff_processor.py`

```python
"""
Git Diff to Test Selection Processor

This script processes git diff output to find which tests should be run
based on code changes. It shows what will be searched in the database
and then queries the database to find affected tests.

What it does:
1. Reads git diff from a file
2. Parses the diff to extract changed files/classes/methods
3. Displays what will be searched (for understanding)
4. Queries the database to find affected tests
5. Displays results with clear explanations

Usage:
    python git_diff_processor/git_diff_processor.py <diff_file_path>
    
    Or specify file interactively:
    python git_diff_processor/git_diff_processor.py
"""

import sys
from pathlib import Path
from typing import Dict, List, Any

# Add parent directory to path to import deterministic modules
sys.path.insert(0, str(Path(__file__).parent.parent))

# Import deterministic database connection
from deterministic.db_connection import get_connection, test_connection, DB_SCHEMA
from deterministic.utils.db_helpers import get_tests_for_production_class

# Import diff parser
sys.path.insert(0, str(Path(__file__).parent))
from utils.diff_parser import (
    parse_git_diff, 
    read_diff_file, 
    build_search_queries,
    extract_production_classes_from_file,
    extract_test_file_candidates
)


def print_header(title: str, width: int = 50) -> None:
    """Print a formatted header."""
    print("=" * width)
    print(title)
    print("=" * width)


def print_section(title: str, indent: int = 2) -> None:
    """Print a section header."""
    print(" " * indent + title)


def print_item(label: str, value: any = "", indent: int = 4) -> None:
    """Print a labeled item."""
    if value != "" and value is not None:
        print(" " * indent + f"{label}: {value}")
    else:
        print(" " * indent + label)


def display_parsed_changes(parsed_diff: Dict) -> None:
    """
    Display parsed diff changes in a beginner-friendly way.
    
    Args:
        parsed_diff: Dictionary from parse_git_diff
    """
    print_section("Parsed Changes:")
    print()
    
    print_item("Changed files", len(parsed_diff['changed_files']))
    for file in parsed_diff['changed_files']:
        # Find file info
        file_info = next((f for f in parsed_diff['file_changes'] if f['file'] == file), None)
        status = file_info['status'] if file_info else 'unknown'
        print_item(f"  - {file}", f"({status})")
    print()
    
    if parsed_diff['changed_classes']:
        print_item("Changed classes", len(parsed_diff['changed_classes']))
        for cls in parsed_diff['changed_classes']:
            print_item(f"  - {cls}", "")
    else:
        print_item("Changed classes", "None detected")
    print()
    
    if parsed_diff['changed_methods']:
        print_item("Changed methods", len(parsed_diff['changed_methods']))
        for method in parsed_diff['changed_methods'][:10]:  # Show first 10
            print_item(f"  - {method}", "")
        if len(parsed_diff['changed_methods']) > 10:
            print_item(f"  ... and {len(parsed_diff['changed_methods']) - 10} more", "")
    else:
        print_item("Changed methods", "None detected")
    print()


def display_search_strategy(search_queries: Dict) -> None:
    """
    Display what will be searched in the database.
    
    Args:
        search_queries: Dictionary from build_search_queries
    """
    print_section("What We'll Search in Database:")
    print()
    
    # Show function-level changes first (highest precision)
    if search_queries.get('changed_functions'):
        print_item("Function-level changes (highest precision)", len(search_queries['changed_functions']))
        for func_change in search_queries['changed_functions'][:10]:
            func_name = f"{func_change['module']}.{func_change['function']}"
            print_item(f"  - {func_name}", "(will match tests that call/patch this function)")
        if len(search_queries['changed_functions']) > 10:
            print_item(f"  ... and {len(search_queries['changed_functions']) - 10} more", "")
        print()
    
    if search_queries['exact_matches']:
        print_item("Exact production class matches", len(search_queries['exact_matches']))
        for match in search_queries['exact_matches'][:10]:
            print_item(f"  - {match}", "")
        if len(search_queries['exact_matches']) > 10:
            print_item(f"  ... and {len(search_queries['exact_matches']) - 10} more", "")
        print()
    
    if search_queries['module_matches']:
        print_item("Module-level patterns", len(search_queries['module_matches']))
        for pattern in search_queries['module_matches']:
            print_item(f"  - {pattern}", "(will match direct references in module)")
        print()
    
    if search_queries.get('test_file_candidates'):
        print_item("Direct test file candidates", len(search_queries['test_file_candidates']))
        for test_file in search_queries['test_file_candidates'][:10]:
            print_item(f"  - {test_file}", "")
        if len(search_queries['test_file_candidates']) > 10:
            print_item(f"  ... and {len(search_queries['test_file_candidates']) - 10} more", "")
        print()
    
    print_item("Database tables to query", "")
    if search_queries.get('changed_functions'):
        print_item("  - test_function_mapping", "(function-level - highest precision)")
    print_item("  - reverse_index", "(class/module-level - fast lookup)")
    print_item("  - test_registry", "(for direct test file matching)")
    print()


def query_tests_for_functions(conn, changed_functions: List[Dict[str, str]]) -> List[Dict]:
    """
    Query database for tests that call/patch specific functions.
    
    This is the most precise matching strategy - only selects tests that
    actually call or patch the changed functions.
    
    Args:
        conn: Database connection
        changed_functions: List of {'module': 'agent.langgraph_agent', 'function': 'initialize'}
    
    Returns:
        List of test dictionaries with match details
    """
    if not changed_functions:
        return []
    
    all_tests = []
    seen_test_ids = set()
    
    with conn.cursor() as cursor:
        for func_change in changed_functions:
            module_name = func_change['module']
            function_name = func_change['function']
            
            # Query test_function_mapping table
            cursor.execute(f"""
                SELECT DISTINCT 
                    tr.test_id, 
                    tr.class_name, 
                    tr.method_name, 
                    tr.file_path,
                    tr.test_type,
                    tfm.call_type,
                    tfm.source,
                    CASE WHEN tfm.source = 'patch_ref' THEN 1 ELSE 2 END as source_priority
                FROM {DB_SCHEMA}.test_function_mapping tfm
                JOIN {DB_SCHEMA}.test_registry tr ON tfm.test_id = tr.test_id
                WHERE tfm.module_name = %s
                AND tfm.function_name = %s
                ORDER BY 
                    source_priority,
                    tr.test_id
            """, (module_name, function_name))
            
            for row in cursor.fetchall():
                test_id = row[0]
                if test_id not in seen_test_ids:
                    seen_test_ids.add(test_id)
                    all_tests.append({
                        'test_id': test_id,
                        'class_name': row[1],
                        'method_name': row[2],
                        'test_file_path': row[3],
                        'test_type': row[4],
                        'call_type': row[5],
                        'source': row[6],
                        'matched_module': module_name,
                        'matched_function': function_name
                    })
    
    return all_tests


def query_tests_for_classes(conn, production_classes: List[str]) -> Dict[str, List[Dict]]:
    """
    Query database to find tests for given production classes.
    
    Args:
        conn: Database connection
        production_classes: List of production class/module names
    
    Returns:
        Dictionary mapping production_class -> list of test dictionaries
    """
    results = {}
    
    for prod_class in production_classes:
        tests = get_tests_for_production_class(conn, prod_class, schema=DB_SCHEMA)
        if tests:
            results[prod_class] = tests
    
    return results


def query_tests_module_pattern(conn, module_pattern: str, prefer_direct: bool = True, 
                                specific_classes: List[str] = None) -> List[Dict]:
    """
    Query database for tests matching a module pattern (e.g., 'agent.*').
    
    Prefers direct references (direct_import, string_ref) over indirect ones.
    If specific_classes is provided, only matches tests referencing those specific classes.
    
    Args:
        conn: Database connection
        module_pattern: Pattern like 'agent.*'
        prefer_direct: If True, prefer direct references over indirect
        specific_classes: Optional list of specific class names to match (filters broad module matches)
    
    Returns:
        List of test dictionaries with reference_type
    """
    module_prefix = module_pattern.replace('.*', '')
    
    with conn.cursor() as cursor:
        if prefer_direct:
            # Build WHERE clause with optional specific class filtering
            if specific_classes and len(specific_classes) > 0:
                # Only match specific classes within the module
                placeholders = ','.join(['%s'] * len(specific_classes))
                # Build parameters: 
                # 1. For CASE statement: module_prefix
                # 2. For IN clause: all specific_classes
                # 3. For OR clause: module_prefix
                params = [module_prefix] + list(specific_classes) + [module_prefix]
                cursor.execute(f"""
                    SELECT DISTINCT 
                        tr.test_id, 
                        tr.class_name, 
                        tr.method_name, 
                        tr.file_path,
                        ri.reference_type,
                        CASE WHEN ri.production_class = %s THEN 1 ELSE 2 END as exact_match_priority,
                        CASE WHEN ri.reference_type IN ('direct_import', 'string_ref') THEN 1 ELSE 2 END as ref_type_priority
                    FROM {DB_SCHEMA}.reverse_index ri
                    JOIN {DB_SCHEMA}.test_registry tr ON ri.test_id = tr.test_id
                    WHERE ri.production_class IN ({placeholders})
                       OR (ri.production_class = %s AND ri.reference_type IN ('direct_import', 'string_ref'))
                    ORDER BY 
                        exact_match_priority,
                        ref_type_priority,
                        tr.test_id
                """, params)
            else:
                # Prefer exact module match and direct references
                cursor.execute(f"""
                    SELECT DISTINCT 
                        tr.test_id, 
                        tr.class_name, 
                        tr.method_name, 
                        tr.file_path,
                        ri.reference_type,
                        CASE WHEN ri.production_class = %s THEN 1 ELSE 2 END as exact_match_priority,
                        CASE WHEN ri.reference_type IN ('direct_import', 'string_ref') THEN 1 ELSE 2 END as ref_type_priority
                    FROM {DB_SCHEMA}.reverse_index ri
                    JOIN {DB_SCHEMA}.test_registry tr ON ri.test_id = tr.test_id
                    WHERE ri.production_class = %s
                       OR (ri.production_class LIKE %s 
                           AND ri.reference_type IN ('direct_import', 'string_ref'))
                    ORDER BY 
                        exact_match_priority,
                        ref_type_priority,
                        tr.test_id
                """, (module_prefix, module_prefix, f"{module_prefix}.%"))
        else:
            # Fallback to original broad match
            cursor.execute(f"""
                SELECT DISTINCT tr.test_id, tr.class_name, tr.method_name, tr.file_path, ri.reference_type
                FROM {DB_SCHEMA}.reverse_index ri
                JOIN {DB_SCHEMA}.test_registry tr ON ri.test_id = tr.test_id
                WHERE ri.production_class LIKE %s
                ORDER BY tr.test_id
            """, (f"{module_prefix}.%",))
        
        results = cursor.fetchall()
        # Extract test fields including reference_type
        return [
            {
                'test_id': row[0],
                'class_name': row[1],
                'method_name': row[2],
                'test_file_path': row[3],
                'reference_type': row[4] if len(row) > 4 else 'direct_import'
            }
            for row in results
        ]


def find_direct_test_files_enhanced(conn, test_file_candidates: List[str], 
                                     module_name: str = None, 
                                     file_path: str = None) -> List[Dict]:
    """
    Enhanced direct test file matching with multiple strategies.
    
    Works with any test repository structure and finds tests using:
    1. Exact filename matches
    2. Pattern matches (test_*.py)
    3. Path-based searches
    4. Module name-based searches
    
    Args:
        conn: Database connection
        test_file_candidates: List of test file names to search for
        module_name: Optional module name (e.g., "agent.agent_pool")
        file_path: Optional production file path for additional patterns
    
    Returns:
        List of test dictionaries with match details
    """
    if not test_file_candidates:
        return []
    
    direct_tests = []
    seen_test_ids = set()  # Avoid duplicates
    
    with conn.cursor() as cursor:
        # Strategy 1: Exact filename matches
        for test_file in test_file_candidates:
            # Remove wildcard patterns for exact match
            exact_file = test_file.replace('*.py', '.py')
            
            patterns = [
                f"%{exact_file}",  # Filename anywhere in path
                f"%\\{exact_file}",  # Filename at end (Windows)
                f"%/{exact_file}",  # Filename at end (Unix)
            ]
            
            for pattern in patterns:
                cursor.execute(f"""
                    SELECT DISTINCT test_id, class_name, method_name, file_path, test_type
                    FROM {DB_SCHEMA}.test_registry
                    WHERE file_path LIKE %s
                    ORDER BY test_id
                """, (pattern,))
                
                for row in cursor.fetchall():
                    test_id = row[0]
                    if test_id not in seen_test_ids:
                        seen_test_ids.add(test_id)
                        direct_tests.append({
                            'test_id': test_id,
                            'class_name': row[1],
                            'method_name': row[2],
                            'test_file_path': row[3],
                            'test_type': row[4],
                            'match_type': 'direct_test_file',
                            'match_strategy': 'exact_filename'
                        })
        
        # Strategy 2: Pattern-based matches (for parameterized tests)
        for test_file in test_file_candidates:
            if '*.py' in test_file:
                # Extract base name before wildcard
                base_name = test_file.replace('test_', '').replace('_*.py', '').replace('*.py', '')
                
                # Try patterns: test_<base>_*.py, test_*<base>*.py
                pattern1 = f"%test_{base_name}_%"
                pattern2 = f"%test_%{base_name}%"
                
                for pattern in [pattern1, pattern2]:
                    cursor.execute(f"""
                        SELECT DISTINCT test_id, class_name, method_name, file_path, test_type
                        FROM {DB_SCHEMA}.test_registry
                        WHERE file_path LIKE %s
                        ORDER BY test_id
                    """, (pattern,))
                    
                    for row in cursor.fetchall():
                        test_id = row[0]
                        if test_id not in seen_test_ids:
                            seen_test_ids.add(test_id)
                            direct_tests.append({
                                'test_id': test_id,
                                'class_name': row[1],
                                'method_name': row[2],
                                'test_file_path': row[3],
                                'test_type': row[4],
                                'match_type': 'direct_test_file',
                                'match_strategy': 'pattern_match'
                            })
        
        # Strategy 3: Module name-based search (if module_name provided)
        if module_name:
            # Extract base name from module
            module_basename = module_name.split('.')[-1]
            
            # Search for any test file containing the module basename
            cursor.execute(f"""
                SELECT DISTINCT test_id, class_name, method_name, file_path, test_type
                FROM {DB_SCHEMA}.test_registry
                WHERE file_path LIKE %s
                   OR file_path LIKE %s
                ORDER BY test_id
            """, (f"%test_{module_basename}%", f"%{module_basename}%"))
            
            for row in cursor.fetchall():
                test_id = row[0]
                if test_id not in seen_test_ids:
                    seen_test_ids.add(test_id)
                    direct_tests.append({
                        'test_id': test_id,
                        'class_name': row[1],
                        'method_name': row[2],
                        'test_file_path': row[3],
                        'test_type': row[4],
                        'match_type': 'direct_test_file',
                        'match_strategy': 'module_basename'
                    })
        
        # Strategy 4: File path-based search (if file_path provided)
        if file_path:
            from pathlib import Path
            path_obj = Path(file_path)
            file_stem = path_obj.stem
            
            # Search for test files with similar names
            cursor.execute(f"""
                SELECT DISTINCT test_id, class_name, method_name, file_path, test_type
                FROM {DB_SCHEMA}.test_registry
                WHERE file_path LIKE %s
                   OR file_path LIKE %s
                ORDER BY test_id
            """, (f"%{file_stem}%", f"%test_{file_stem}%"))
            
            for row in cursor.fetchall():
                test_id = row[0]
                if test_id not in seen_test_ids:
                    seen_test_ids.add(test_id)
                    direct_tests.append({
                        'test_id': test_id,
                        'class_name': row[1],
                        'method_name': row[2],
                        'test_file_path': row[3],
                        'test_type': row[4],
                        'match_type': 'direct_test_file',
                        'match_strategy': 'file_path_based'
                    })
    
    return direct_tests


def find_direct_test_files(conn, test_file_candidates: List[str]) -> List[Dict]:
    """
    Find test files that directly test changed production files.
    
    This is a wrapper that calls the enhanced version for backward compatibility.
    
    Args:
        conn: Database connection
        test_file_candidates: List of test file names to search for
    
    Returns:
        List of test dictionaries
    """
    return find_direct_test_files_enhanced(conn, test_file_candidates)


def find_affected_tests(conn, search_queries: Dict, file_changes: List[Dict] = None) -> Dict[str, Any]:
    """
    Find all affected tests using multiple search strategies with enhanced matching.
    
    Strategies (in priority order):
    0. Function-level matching (very high confidence) - NEW
    1. Direct test files (high confidence)
    2. Integration/e2e tests (high confidence)
    3. Exact class/module matches (high confidence)
    4. Module patterns with direct references (medium confidence)
    
    Args:
        conn: Database connection
        search_queries: Dictionary with search strategies
        file_changes: List of file change dictionaries (for filtering import-only changes)
    
    Returns:
        Dictionary with test results and metadata
    """
    all_tests = {}  # test_id -> test info with match details
    match_details = {}  # test_id -> list of match reasons
    
    # Strategy 0: Function-level matching (very high confidence) - NEW
    if search_queries.get('changed_functions'):
        print_section("Querying database (Function-level matching - highest precision)...")
        function_tests = query_tests_for_functions(conn, search_queries['changed_functions'])
        
        if function_tests:
            print_item(f"  Found {len(function_tests)} test(s) via function-level matching", "")
            
            # Group by function to show what matched
            by_function = {}
            for test in function_tests:
                func_key = f"{test['matched_module']}.{test['matched_function']}"
                if func_key not in by_function:
                    by_function[func_key] = []
                by_function[func_key].append(test)
            
            # Show which functions matched (first 10)
            print_item("  Matched functions", "")
            for func_key, tests in list(by_function.items())[:10]:
                test_count = len(tests)
                test_names = [f"{t.get('class_name', '')}.{t.get('method_name', '')}" if t.get('class_name') else t.get('method_name', '') for t in tests[:3]]
                test_list = ", ".join(test_names)
                if test_count > 3:
                    test_list += f" ... (+{test_count - 3} more)"
                print_item(f"    - {func_key}", f"({test_count} test(s): {test_list})")
            if len(by_function) > 10:
                print_item(f"    ... and {len(by_function) - 10} more functions", "")
            
            for test in function_tests:
                test_id = test['test_id']
                if test_id not in all_tests:
                    all_tests[test_id] = test
                    match_details[test_id] = []
                match_details[test_id].append({
                    'type': 'function_level',
                    'module': test['matched_module'],
                    'function': test['matched_function'],
                    'call_type': test.get('call_type'),
                    'source': test.get('source'),
                    'confidence': 'very_high'
                })
        else:
            print_item("  No function-level matches found", "")
            print_item("  (Falling back to file-level matching)", "")
        print()
    
    # Strategy 1: Direct test files (high confidence) - Enhanced
    if search_queries.get('test_file_candidates'):
        print_section("Querying database (Direct test files - enhanced multi-strategy)...")
        
        # Build module name and file path mappings for enhanced search
        module_file_map = {}
        if file_changes:
            from utils.diff_parser import extract_production_classes_from_file
            for file_change in file_changes:
                file_path = file_change.get('file', '')
                if file_path and file_path.endswith('.py'):
                    classes = extract_production_classes_from_file(file_path)
                    if classes:
                        module_file_map[classes[0]] = file_path
        
        # Use enhanced direct test file finder
        direct_tests = []
        for candidate in search_queries['test_file_candidates']:
            # Try to find matching module/file for this candidate
            module_name = None
            file_path = None
            for mod, fp in module_file_map.items():
                if candidate.replace('test_', '').replace('.py', '') in mod or \
                   candidate.replace('test_', '').replace('.py', '') in fp:
                    module_name = mod
                    file_path = fp
                    break
            
            enhanced_results = find_direct_test_files_enhanced(
                conn, [candidate], 
                module_name=module_name,
                file_path=file_path
            )
            direct_tests.extend(enhanced_results)
        
        # Remove duplicates
        seen_ids = set()
        unique_direct_tests = []
        for test in direct_tests:
            if test['test_id'] not in seen_ids:
                seen_ids.add(test['test_id'])
                unique_direct_tests.append(test)
        direct_tests = unique_direct_tests
        
        print_item(f"  Found {len(direct_tests)} direct test file(s)", "")
        
        # Group by test file to show which files matched
        matched_files = {}
        for test in direct_tests:
            test_file = test.get('test_file_path', 'unknown')
            if test_file not in matched_files:
                matched_files[test_file] = []
            matched_files[test_file].append(test)
        
        # Show which test files matched (first 10)
        if matched_files:
            print_item("  Matched test files", "")
            for test_file, tests in list(matched_files.items())[:10]:
                test_count = len(tests)
                test_names = [f"{t.get('class_name', '')}.{t.get('method_name', '')}" if t.get('class_name') else t.get('method_name', '') for t in tests[:3]]
                test_list = ", ".join(test_names)
                if test_count > 3:
                    test_list += f" ... (+{test_count - 3} more)"
                match_strategy = tests[0].get('match_strategy', 'unknown')
                print_item(f"    - {test_file}", f"({test_count} test(s), {match_strategy}: {test_list})")
            if len(matched_files) > 10:
                print_item(f"    ... and {len(matched_files) - 10} more test files", "")
        
        for test in direct_tests:
            test_id = test['test_id']
            if test_id not in all_tests:
                all_tests[test_id] = test
                match_details[test_id] = []
            match_details[test_id].append({
                'type': 'direct_file',
                'test_file': test.get('test_file_path', ''),
                'match_strategy': test.get('match_strategy', 'exact_filename'),
                'confidence': 'very_high'
            })
        print()
    
    # Strategy 1.5: Integration tests for changed modules
    if file_changes:
        print_section("Querying database (Integration/e2e tests)...")
        from utils.diff_parser import extract_production_classes_from_file, analyze_file_change_type
        
        integration_tests_found = []
        for file_change in file_changes:
            file_path = file_change.get('file', '')
            change_type = analyze_file_change_type(file_change)
            
            # Skip import-only changes
            if change_type == 'import_only':
                continue
            
            if file_path and file_path.endswith('.py'):
                classes = extract_production_classes_from_file(file_path)
                for module_name in classes[:1]:  # Check first class only
                    integration_tests = find_integration_tests_for_module(conn, module_name)
                    for test in integration_tests:
                        test_id = test['test_id']
                        if test_id not in all_tests:
                            all_tests[test_id] = test
                            match_details[test_id] = []
                        match_details[test_id].append({
                            'type': 'integration',
                            'module': module_name,
                            'confidence': 'high'
                        })
                        integration_tests_found.append(test)
        
        if integration_tests_found:
            print_item(f"  Found {len(integration_tests_found)} integration/e2e test(s)", "")
            # Group by test type
            by_type = {}
            for test in integration_tests_found:
                test_type = test.get('test_type', 'unknown')
                if test_type not in by_type:
                    by_type[test_type] = []
                by_type[test_type].append(test)
            
            for test_type, tests in by_type.items():
                print_item(f"    {test_type.capitalize()} tests", f"{len(tests)} test(s)")
        else:
            print_item("  No integration/e2e tests found", "")
        print()
    
    # Strategy 2: Exact matches (high confidence) - includes string references from patch()
    if search_queries['exact_matches']:
        print_section("Querying database (Exact matches - includes string refs from patch/Mock)...")
        exact_results = query_tests_for_classes(conn, search_queries['exact_matches'])
        
        if exact_results:
            for prod_class, tests in exact_results.items():
                # Count by reference type
                string_refs = sum(1 for t in tests if t.get('reference_type') == 'string_ref')
                direct_imports = len(tests) - string_refs
                
                ref_info = []
                if string_refs > 0:
                    ref_info.append(f"{string_refs} via patch/Mock")
                if direct_imports > 0:
                    ref_info.append(f"{direct_imports} via import")
                
                ref_detail = f" ({', '.join(ref_info)})" if ref_info else ""
                print_item(f"  {prod_class}", f"{len(tests)} tests{ref_detail}")
                
                for test in tests:
                    test_id = test['test_id']
                    if test_id not in all_tests:
                        all_tests[test_id] = test
                        match_details[test_id] = []
                    match_details[test_id].append({
                        'type': 'exact',
                        'class': prod_class,
                        'reference_type': test.get('reference_type', 'direct_import'),
                        'confidence': 'high'
                    })
        else:
            print_item("  No exact matches found", "")
            # Debug: Check what's actually in the database
            if search_queries['exact_matches']:
                with conn.cursor() as cursor:
                    first_match = search_queries['exact_matches'][0]
                    cursor.execute(f"""
                        SELECT DISTINCT production_class, reference_type
                        FROM {DB_SCHEMA}.reverse_index 
                        WHERE production_class LIKE %s OR production_class = %s
                        LIMIT 5
                    """, (f"{first_match}%", first_match))
                    sample_classes = cursor.fetchall()
                    if sample_classes:
                        print_item("  Sample production classes in database", 
                                  ", ".join([f"{row[0]} ({row[1]})" for row in sample_classes[:3]]))
                    else:
                        # Check if database has any data
                        cursor.execute(f"SELECT COUNT(*) FROM {DB_SCHEMA}.reverse_index")
                        count = cursor.fetchone()[0]
                        if count == 0:
                            print_item("  WARNING: reverse_index table is empty!", "")
                            print_item("    Re-run test analysis to extract string references", "")
                            print_item("      1. python test_analysis/04_extract_static_dependencies.py", "")
                            print_item("      2. python test_analysis/06_build_reverse_index.py", "")
                            print_item("      3. python deterministic/04_load_reverse_index.py", "")
        print()
    
    # Strategy 3: Module-level matches (prefer direct references, skip import-only changes)
    if search_queries['module_matches']:
        print_section("Querying database (Module patterns - direct references only)...")
        
        # Build map of module -> specific changed classes for better filtering
        from utils.diff_parser import analyze_file_change_type, extract_production_classes_from_file
        module_to_classes = {}  # module_prefix -> set of changed classes
        code_changed_modules = set()
        
        if file_changes:
            for file_change in file_changes:
                change_type = analyze_file_change_type(file_change)
                if change_type == 'code':  # Only include modules with actual code changes
                    file_path = file_change['file']
                    classes = extract_production_classes_from_file(file_path)
                    for class_name in classes:
                        if '.' in class_name:
                            module_part = class_name.split('.')[0]
                            code_changed_modules.add(module_part)
                            if module_part not in module_to_classes:
                                module_to_classes[module_part] = set()
                            module_to_classes[module_part].add(class_name)
        
        for module_pattern in search_queries['module_matches']:
            module_prefix = module_pattern.replace('.*', '')
            
            # Skip if this module only had import-only changes
            if code_changed_modules and module_prefix not in code_changed_modules:
                print_item(f"  {module_pattern}", "0 tests (skipped - import-only changes)")
                continue
            
            # Get specific classes that changed in this module for better filtering
            specific_classes = list(module_to_classes.get(module_prefix, []))
            
            # Use prefer_direct=True and filter by specific changed classes
            module_tests = query_tests_module_pattern(
                conn, module_pattern, 
                prefer_direct=True,
                specific_classes=specific_classes if specific_classes else None
            )
            
            # Count by reference type
            string_refs = sum(1 for t in module_tests if t.get('reference_type') == 'string_ref')
            direct_imports = len(module_tests) - string_refs
            ref_info = []
            if string_refs > 0:
                ref_info.append(f"{string_refs} via patch/Mock")
            if direct_imports > 0:
                ref_info.append(f"{direct_imports} via import")
            ref_detail = f" ({', '.join(ref_info)})" if ref_info else ""
            
            print_item(f"  {module_pattern}", f"{len(module_tests)} tests{ref_detail}")
            
            for test in module_tests:
                test_id = test['test_id']
                if test_id not in all_tests:
                    all_tests[test_id] = test
                    match_details[test_id] = []
                match_details[test_id].append({
                    'type': 'module',
                    'pattern': module_pattern,
                    'reference_type': test.get('reference_type', 'direct_import'),
                    'confidence': 'medium'
                })
        print()
    
    return {
        'tests': list(all_tests.values()),
        'match_details': match_details,
        'total_tests': len(all_tests)
    }


def get_all_tests_from_database(conn) -> Dict[str, Dict]:
    """
    Get all tests from the test_registry table.
    
    Args:
        conn: Database connection
    
    Returns:
        Dictionary mapping test_id -> test dictionary
    """
    all_tests = {}
    
    with conn.cursor() as cursor:
        cursor.execute(f"""
            SELECT test_id, file_path, class_name, method_name, test_type
            FROM {DB_SCHEMA}.test_registry
            ORDER BY test_id
        """)
        
        for row in cursor.fetchall():
            test_id, file_path, class_name, method_name, test_type = row
            all_tests[test_id] = {
                'test_id': test_id,
                'file_path': file_path,
                'class_name': class_name,
                'method_name': method_name,
                'test_type': test_type
            }
    
    return all_tests


def find_integration_tests_for_module(conn, production_class: str) -> List[Dict]:
    """
    Find integration/e2e tests that reference a production module.
    
    Args:
        conn: Database connection
        production_class: Production class/module name (e.g., "agent.langgraph_agent")
    
    Returns:
        List of test dictionaries (integration/e2e tests only)
    """
    integration_tests = []
    
    with conn.cursor() as cursor:
        # Query for integration/e2e tests that reference this production class
        cursor.execute(f"""
            SELECT DISTINCT 
                tr.test_id, 
                tr.class_name, 
                tr.method_name, 
                tr.file_path,
                tr.test_type,
                ri.reference_type
            FROM {DB_SCHEMA}.reverse_index ri
            JOIN {DB_SCHEMA}.test_registry tr ON ri.test_id = tr.test_id
            WHERE ri.production_class = %s
              AND tr.test_type IN ('integration', 'e2e')
            ORDER BY tr.test_type, tr.test_id
        """, (production_class,))
        
        for row in cursor.fetchall():
            integration_tests.append({
                'test_id': row[0],
                'class_name': row[1],
                'method_name': row[2],
                'test_file_path': row[3],
                'test_type': row[4],
                'reference_type': row[5],
                'match_type': 'integration_test'
            })
    
    return integration_tests


def diagnose_missing_tests(conn, changed_files: List[Dict], 
                          search_queries: Dict) -> Dict[str, Any]:
    """
    Diagnose why expected tests might be missing.
    
    Args:
        conn: Database connection
        changed_files: List of changed file dictionaries
        search_queries: Search queries that were used
    
    Returns:
        Dictionary with diagnostic information for each changed file
    """
    diagnostics = {}
    
    from pathlib import Path
    from utils.diff_parser import extract_production_classes_from_file, extract_test_file_candidates
    
    with conn.cursor() as cursor:
        for file_change in changed_files:
            file_path = file_change.get('file', '')
            if not file_path or not file_path.endswith('.py'):
                continue
            
            # Extract module and test candidates
            classes = extract_production_classes_from_file(file_path)
            test_candidates = extract_test_file_candidates(file_path)
            
            if not classes:
                continue
            
            module_name = classes[0] if classes else None
            module_basename = module_name.split('.')[-1] if module_name else Path(file_path).stem
            
            # Check what patterns we're looking for
            expected_patterns = test_candidates
            
            # Check what exists in database
            found_tests = []
            for pattern in expected_patterns[:5]:  # Check first 5 patterns
                exact_pattern = pattern.replace('*.py', '.py')
                cursor.execute(f"""
                    SELECT test_id, file_path, test_type, class_name, method_name
                    FROM {DB_SCHEMA}.test_registry
                    WHERE file_path LIKE %s
                    LIMIT 10
                """, (f"%{exact_pattern}",))
                
                for row in cursor.fetchall():
                    found_tests.append({
                        'test_id': row[0],
                        'file_path': row[1],
                        'test_type': row[2],
                        'class_name': row[3],
                        'method_name': row[4]
                    })
            
            # Check what references exist in reverse_index
            found_references = []
            if module_name:
                cursor.execute(f"""
                    SELECT DISTINCT ri.test_id, ri.reference_type, tr.file_path, tr.test_type
                    FROM {DB_SCHEMA}.reverse_index ri
                    JOIN {DB_SCHEMA}.test_registry tr ON ri.test_id = tr.test_id
                    WHERE ri.production_class = %s
                    LIMIT 10
                """, (module_name,))
                
                for row in cursor.fetchall():
                    found_references.append({
                        'test_id': row[0],
                        'reference_type': row[1],
                        'file_path': row[2],
                        'test_type': row[3]
                    })
            
            # Generate suggestions
            suggestions = []
            if not found_tests and not found_references:
                suggestions.append("Test file might not be indexed. Run test repository indexer.")
                suggestions.append(f"Expected patterns: {', '.join(expected_patterns[:3])}")
            elif not found_tests and found_references:
                suggestions.append("Tests exist but don't follow naming convention. Using reference matching.")
                suggestions.append(f"Found {len(found_references)} tests via reverse_index")
            elif found_tests:
                suggestions.append(f"Found {len(found_tests)} direct test file matches")
            
            # Check for integration tests
            integration_count = sum(1 for t in found_references if t.get('test_type') in ('integration', 'e2e'))
            if integration_count > 0:
                suggestions.append(f"Found {integration_count} integration/e2e tests")
            
            diagnostics[file_path] = {
                'module_name': module_name,
                'module_basename': module_basename,
                'expected_patterns': expected_patterns,
                'tests_in_db': found_tests,
                'references_in_db': found_references,
                'found_direct': len(found_tests) > 0,
                'found_references': len(found_references) > 0,
                'suggestions': suggestions
            }
    
    return diagnostics


def find_unused_tests(conn, affected_test_ids: set) -> List[Dict]:
    """
    Find tests that are NOT affected by the changes.
    
    Args:
        conn: Database connection
        affected_test_ids: Set of test IDs that are affected
    
    Returns:
        List of test dictionaries that are not affected
    """
    all_tests = get_all_tests_from_database(conn)
    unused_tests = []
    
    for test_id, test in all_tests.items():
        if test_id not in affected_test_ids:
            unused_tests.append(test)
    
    return unused_tests


def display_results(results: Dict, conn=None) -> None:
    """
    Display test selection results in a beginner-friendly way.
    
    Args:
        results: Dictionary from find_affected_tests
        conn: Optional database connection to show unused tests
    """
    print_section("Test Selection Results:")
    print()
    
    if results['total_tests'] == 0:
        print_item("No tests found!", "")
        print()
        print_item("Possible reasons", "")
        print_item("  - Changed files are not referenced by any tests", "")
        print_item("  - Changed files are test files themselves", "")
        print_item("  - Production class names don't match database", "")
        
        # Show unused tests if connection available
        if conn:
            print()
            print_section("All Tests (Not Affected):")
            unused_tests = find_unused_tests(conn, set())
            print_item(f"Total tests in repository", len(unused_tests))
            if unused_tests:
                print_item("Sample tests (first 10)", "")
                for test in unused_tests[:10]:
                    test_name = f"{test['class_name']}.{test['method_name']}" if test['class_name'] else test['method_name']
                    test_type = test.get('test_type') or 'unknown'
                    print_item(f"  {test['test_id']}", f"{test_name} ({test_type})")
        return
    
    print_item(f"Found {results['total_tests']} affected test(s)", "")
    print()
    
    # Group by confidence
    very_high_confidence = []  # Function-level matches
    high_confidence = []
    medium_confidence = []
    affected_test_ids = set()
    
    for test in results['tests']:
        test_id = test['test_id']
        affected_test_ids.add(test_id)
        matches = results['match_details'].get(test_id, [])
        
        # Determine overall confidence (function-level is highest)
        has_function_level = any(m['type'] == 'function_level' for m in matches)
        has_exact = any(m['type'] == 'exact' for m in matches)
        
        if has_function_level:
            confidence = 'very_high'
        elif has_exact:
            confidence = 'high'
        else:
            confidence = 'medium'
        
        test_info = {
            'test': test,
            'matches': matches,
            'confidence': confidence
        }
        
        if confidence == 'very_high':
            very_high_confidence.append(test_info)
        elif confidence == 'high':
            high_confidence.append(test_info)
        else:
            medium_confidence.append(test_info)
    
    # Display very high confidence first (function-level)
    if very_high_confidence:
        print_item("Very High Confidence Matches (Function-level - precise)", len(very_high_confidence))
        for test_info in very_high_confidence[:10]:
            test = test_info['test']
            test_name = f"{test['class_name']}.{test['method_name']}" if test['class_name'] else test['method_name']
            print_item(f"  {test['test_id']}:", test_name)
            
            # Show function-level match details
            func_matches = [m for m in test_info['matches'] if m['type'] == 'function_level']
            if func_matches:
                matched_functions = []
                for m in func_matches[:3]:
                    func_name = f"{m['module']}.{m['function']}"
                    source = m.get('source', 'method_call')
                    if source == 'patch_ref':
                        matched_functions.append(f"{func_name} (via patch)")
                    else:
                        matched_functions.append(func_name)
                if len(func_matches) > 3:
                    matched_functions.append(f"... (+{len(func_matches) - 3} more)")
                print_item(f"    Matched functions", ", ".join(matched_functions))
        print()
    
    # Display high confidence (file/class-level)
    if high_confidence:
        print_item("High Confidence Matches (Exact class matches)", len(high_confidence))
        for test_info in high_confidence[:10]:
            test = test_info['test']
            test_name = f"{test['class_name']}.{test['method_name']}" if test['class_name'] else test['method_name']
            print_item(f"  {test['test_id']}:", test_name)
            
            # Show match reasons with reference types
            exact_matches = [m for m in test_info['matches'] if m['type'] == 'exact']
            if exact_matches:
                matched_classes = []
                for m in exact_matches[:3]:
                    class_name = m['class']
                    ref_type = m.get('reference_type', 'direct_import')
                    if ref_type == 'string_ref':
                        matched_classes.append(f"{class_name} (via patch/Mock)")
                    else:
                        matched_classes.append(class_name)
                if len(exact_matches) > 3:
                    matched_classes.append(f"... (+{len(exact_matches) - 3} more)")
                print_item(f"    Matched classes", ", ".join(matched_classes))
        print()
    
    # Display medium confidence
    if medium_confidence:
        print_item("Medium Confidence Matches (Module patterns)", len(medium_confidence))
        for test_info in medium_confidence[:10]:
            test = test_info['test']
            test_name = f"{test['class_name']}.{test['method_name']}" if test['class_name'] else test['method_name']
            print_item(f"  {test['test_id']}:", test_name)
        print()
    
    # Summary
    print_section("Summary:")
    print_item("Total tests to run", results['total_tests'])
    if very_high_confidence:
        print_item("Very high confidence (function-level)", len(very_high_confidence))
    print_item("High confidence", len(high_confidence))
    print_item("Medium confidence", len(medium_confidence))
    print()
    
    # Display unused tests if connection available
    if conn:
        print_section("Tests NOT Affected (Unused):")
        unused_tests = find_unused_tests(conn, affected_test_ids)
        print_item(f"Total unused tests", len(unused_tests))
        print()
        
        if unused_tests:
            # Group by test type
            by_type = {}
            for test in unused_tests:
                test_type = test.get('test_type', 'unknown')
                if test_type not in by_type:
                    by_type[test_type] = []
                by_type[test_type].append(test)
            
            # Display by type
            for test_type, tests in sorted(by_type.items()):
                print_item(f"{test_type.capitalize()} tests (unused)", len(tests))
                for test in tests[:5]:  # Show first 5 of each type
                    test_name = f"{test['class_name']}.{test['method_name']}" if test['class_name'] else test['method_name']
                    print_item(f"  {test['test_id']}", test_name)
                if len(tests) > 5:
                    print_item(f"  ... and {len(tests) - 5} more {test_type} tests", "")
                print()
            
            # Overall summary
            print_section("Unused Tests Summary:")
            all_tests_count = len(get_all_tests_from_database(conn))
            print_item("Total tests in repository", all_tests_count)
            print_item("Affected tests", len(affected_test_ids))
            print_item("Unused tests", len(unused_tests))
            if all_tests_count > 0:
                reduction_pct = round((len(unused_tests) / all_tests_count) * 100, 1)
                print_item("Test reduction", f"{len(unused_tests)} tests ({reduction_pct}%) can be skipped")
            print()


def save_results_to_file(results: Dict, conn=None, diff_file_path: str = None, output_dir: Path = None) -> Path:
    """
    Save complete test selection results to a file.
    
    Args:
        results: Dictionary from find_affected_tests
        conn: Optional database connection to include unused tests
        diff_file_path: Path to the diff file (for naming output file)
        output_dir: Directory to save output file (default: git_diff_processor/outputs)
    
    Returns:
        Path to the saved output file
    """
    from datetime import datetime
    
    # Determine output directory
    if output_dir is None:
        output_dir = Path(__file__).parent / "outputs"
    output_dir.mkdir(exist_ok=True)
    
    # Generate output filename
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if diff_file_path:
        diff_name = Path(diff_file_path).stem
        output_filename = f"test_selection_{diff_name}_{timestamp}.txt"
    else:
        output_filename = f"test_selection_{timestamp}.txt"
    
    output_path = output_dir / output_filename
    
    # Build output content
    lines = []
    lines.append("=" * 80)
    lines.append("TEST SELECTION RESULTS")
    lines.append("=" * 80)
    lines.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    if diff_file_path:
        lines.append(f"Diff File: {diff_file_path}")
    lines.append("")
    
    if results['total_tests'] == 0:
        lines.append("No tests found!")
        lines.append("")
        lines.append("Possible reasons:")
        lines.append("  - Changed files are not referenced by any tests")
        lines.append("  - Changed files are test files themselves")
        lines.append("  - Production class names don't match database")
        
        if conn:
            lines.append("")
            lines.append("-" * 80)
            lines.append("ALL TESTS (NOT AFFECTED)")
            lines.append("-" * 80)
            unused_tests = find_unused_tests(conn, set())
            lines.append(f"Total tests in repository: {len(unused_tests)}")
            if unused_tests:
                lines.append("")
                for test in unused_tests:
                    test_name = f"{test['class_name']}.{test['method_name']}" if test['class_name'] else test['method_name']
                    test_type = test.get('test_type') or 'unknown'
                    lines.append(f"  {test['test_id']}: {test_name} ({test_type})")
    else:
        lines.append(f"Found {results['total_tests']} affected test(s)")
        lines.append("")
        
        # Group by confidence
        very_high_confidence = []  # Function-level matches
        high_confidence = []
        medium_confidence = []
        affected_test_ids = set()
        
        for test in results['tests']:
            test_id = test['test_id']
            affected_test_ids.add(test_id)
            matches = results['match_details'].get(test_id, [])
            
            # Determine overall confidence (function-level is highest)
            has_function_level = any(m['type'] == 'function_level' for m in matches)
            has_exact = any(m['type'] == 'exact' for m in matches)
            
            if has_function_level:
                confidence = 'very_high'
            elif has_exact:
                confidence = 'high'
            else:
                confidence = 'medium'
            
            test_info = {
                'test': test,
                'matches': matches,
                'confidence': confidence
            }
            
            if confidence == 'very_high':
                very_high_confidence.append(test_info)
            elif confidence == 'high':
                high_confidence.append(test_info)
            else:
                medium_confidence.append(test_info)
        
        # Write very high confidence tests (function-level) first
        if very_high_confidence:
            lines.append("-" * 80)
            lines.append(f"VERY HIGH CONFIDENCE MATCHES (Function-level - precise): {len(very_high_confidence)}")
            lines.append("-" * 80)
            lines.append("")
            
            for test_info in very_high_confidence:
                test = test_info['test']
                test_name = f"{test['class_name']}.{test['method_name']}" if test['class_name'] else test['method_name']
                test_type = test.get('test_type') or 'unknown'
                test_file = test.get('test_file_path', 'unknown')
                
                lines.append(f"  {test['test_id']}: {test_name}")
                lines.append(f"    Test Type: {test_type}")
                lines.append(f"    File: {test_file}")
                
                # Show function-level match details
                func_matches = [m for m in test_info['matches'] if m['type'] == 'function_level']
                if func_matches:
                    matched_functions = []
                    for m in func_matches:
                        func_name = f"{m['module']}.{m['function']}"
                        source = m.get('source', 'method_call')
                        if source == 'patch_ref':
                            matched_functions.append(f"{func_name} (via patch)")
                        else:
                            matched_functions.append(f"{func_name} (via call)")
                    lines.append(f"    Matched functions: {', '.join(matched_functions)}")
                
                # Show other match types
                other_matches = [m for m in test_info['matches'] if m['type'] != 'function_level']
                if other_matches:
                    match_types = {}
                    for m in other_matches:
                        match_type = m.get('type', 'unknown')
                        if match_type not in match_types:
                            match_types[match_type] = []
                        if match_type == 'direct_file':
                            match_types[match_type].append(m.get('test_file', ''))
                        elif match_type == 'integration':
                            match_types[match_type].append(m.get('module', ''))
                    for match_type, values in match_types.items():
                        lines.append(f"    Also matched via: {match_type} ({', '.join(set(values))})")
                
                lines.append("")
        
        # Write high confidence tests (file/class-level)
        if high_confidence:
            lines.append("-" * 80)
            lines.append(f"HIGH CONFIDENCE MATCHES (Exact class matches): {len(high_confidence)}")
            lines.append("-" * 80)
            lines.append("")
            
            for test_info in high_confidence:
                test = test_info['test']
                test_name = f"{test['class_name']}.{test['method_name']}" if test['class_name'] else test['method_name']
                test_type = test.get('test_type') or 'unknown'
                test_file = test.get('test_file_path', 'unknown')
                
                lines.append(f"  {test['test_id']}: {test_name}")
                lines.append(f"    Test Type: {test_type}")
                lines.append(f"    File: {test_file}")
                
                # Show all match reasons
                exact_matches = [m for m in test_info['matches'] if m['type'] == 'exact']
                if exact_matches:
                    matched_classes = []
                    for m in exact_matches:
                        class_name = m['class']
                        ref_type = m.get('reference_type', 'direct_import')
                        if ref_type == 'string_ref':
                            matched_classes.append(f"{class_name} (via patch/Mock)")
                        else:
                            matched_classes.append(f"{class_name} (via import)")
                    lines.append(f"    Matched classes: {', '.join(matched_classes)}")
                
                # Show other match types
                other_matches = [m for m in test_info['matches'] if m['type'] != 'exact']
                if other_matches:
                    match_types = {}
                    for m in other_matches:
                        match_type = m.get('type', 'unknown')
                        if match_type not in match_types:
                            match_types[match_type] = []
                        if match_type == 'direct_file':
                            match_types[match_type].append(m.get('test_file', ''))
                        elif match_type == 'integration':
                            match_types[match_type].append(m.get('module', ''))
                    for match_type, values in match_types.items():
                        lines.append(f"    Also matched via: {match_type} ({', '.join(set(values))})")
                
                lines.append("")
        
        # Write medium confidence tests (ALL of them)
        if medium_confidence:
            lines.append("-" * 80)
            lines.append(f"MEDIUM CONFIDENCE MATCHES (Module patterns): {len(medium_confidence)}")
            lines.append("-" * 80)
            lines.append("")
            
            for test_info in medium_confidence:
                test = test_info['test']
                test_name = f"{test['class_name']}.{test['method_name']}" if test['class_name'] else test['method_name']
                test_type = test.get('test_type') or 'unknown'
                test_file = test.get('test_file_path', 'unknown')
                
                lines.append(f"  {test['test_id']}: {test_name}")
                lines.append(f"    Test Type: {test_type}")
                lines.append(f"    File: {test_file}")
                
                # Show match reasons
                for m in test_info['matches']:
                    match_type = m.get('type', 'unknown')
                    if match_type == 'module_pattern':
                        lines.append(f"    Matched via module pattern: {m.get('pattern', 'unknown')}")
                    elif match_type == 'direct_file':
                        lines.append(f"    Matched via direct file: {m.get('test_file', 'unknown')}")
                
                lines.append("")
        
        # Summary
        lines.append("-" * 80)
        lines.append("SUMMARY")
        lines.append("-" * 80)
        lines.append(f"Total tests to run: {results['total_tests']}")
        if very_high_confidence:
            lines.append(f"Very high confidence (function-level): {len(very_high_confidence)}")
        lines.append(f"High confidence: {len(high_confidence)}")
        lines.append(f"Medium confidence: {len(medium_confidence)}")
        lines.append("")
        
        # Write unused tests (ALL of them)
        if conn:
            lines.append("-" * 80)
            lines.append("TESTS NOT AFFECTED (UNUSED)")
            lines.append("-" * 80)
            unused_tests = find_unused_tests(conn, affected_test_ids)
            lines.append(f"Total unused tests: {len(unused_tests)}")
            lines.append("")
            
            if unused_tests:
                # Group by test type
                by_type = {}
                for test in unused_tests:
                    test_type = test.get('test_type', 'unknown')
                    if test_type not in by_type:
                        by_type[test_type] = []
                    by_type[test_type].append(test)
                
                # Write by type (ALL tests, not just first 5)
                for test_type, tests in sorted(by_type.items()):
                    lines.append(f"{test_type.capitalize()} tests (unused): {len(tests)}")
                    for test in tests:
                        test_name = f"{test['class_name']}.{test['method_name']}" if test['class_name'] else test['method_name']
                        test_file = test.get('test_file_path', 'unknown')
                        lines.append(f"  {test['test_id']}: {test_name}")
                        lines.append(f"    File: {test_file}")
                    lines.append("")
                
                # Overall summary
                lines.append("-" * 80)
                lines.append("UNUSED TESTS SUMMARY")
                lines.append("-" * 80)
                all_tests_count = len(get_all_tests_from_database(conn))
                lines.append(f"Total tests in repository: {all_tests_count}")
                lines.append(f"Affected tests: {len(affected_test_ids)}")
                lines.append(f"Unused tests: {len(unused_tests)}")
                if all_tests_count > 0:
                    reduction_pct = round((len(unused_tests) / all_tests_count) * 100, 1)
                    lines.append(f"Test reduction: {len(unused_tests)} tests ({reduction_pct}%) can be skipped")
                lines.append("")
    
    # Write to file
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    
    return output_path


def main():
    """Main function to process git diff and find affected tests."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Git Diff to Test Selection')
    parser.add_argument('diff_file', nargs='?', help='Path to git diff file')
    parser.add_argument('--reindex', action='store_true', 
                       help='Re-index test repository before selection')
    parser.add_argument('--verify', action='store_true',
                       help='Verify indexing completeness')
    parser.add_argument('--diagnose', action='store_true',
                       help='Run diagnostics')
    parser.add_argument('--test-repo', default=None,
                       help='Path to test repository (default: test_repository)')
    parser.add_argument('--deduplicate', action='store_true',
                       help='Find and remove duplicate test entries')
    parser.add_argument('--remove-duplicates', action='store_true',
                       help='Actually remove duplicates (use with --deduplicate)')
    
    args = parser.parse_args()
    
    # Import indexing utilities
    sys.path.insert(0, str(Path(__file__).parent))
    from utils.indexing_utils import verify_indexing_completeness, reindex_missing_files, diagnose_integration_tests
    from utils.deduplicate_tests import find_duplicate_tests, remove_duplicate_tests
    
    # Determine test repo path
    if args.test_repo:
        test_repo_path = args.test_repo
    else:
        test_repo_path = str(Path(__file__).parent.parent / "test_repository")
    
    # Handle verify, reindex, diagnose options
    if args.verify:
        print_header("Verifying Indexing Completeness")
        print()
        with get_connection() as conn:
            verification = verify_indexing_completeness(test_repo_path, conn)
            print_section("Indexing Verification Results:")
            print_item("Files on disk", verification['total_on_disk'])
            print_item("Files indexed", verification['total_indexed'])
            print_item("Coverage", f"{verification['coverage_percent']:.1f}%")
            
            if verification.get('missing_files'):
                print()
                print_section("Missing Files:")
                for missing in verification['missing_files'][:20]:  # Show first 20
                    print_item(f"  - {Path(missing).name}", str(missing))
                if len(verification['missing_files']) > 20:
                    print_item(f"  ... and {len(verification['missing_files']) - 20} more", "")
        return
    
    if args.reindex:
        print_header("Re-indexing Test Repository")
        print()
        with get_connection() as conn:
            result = reindex_missing_files(test_repo_path, conn)
            print_section("Re-indexing Results:")
            print_item("Files indexed", str(result.get('indexed', 0)))
            print_item("Tests added", str(result.get('tests_added', 0)))
            print_item("Duplicates avoided", str(result.get('duplicates_avoided', 0)))
            print_item("Errors", str(len(result.get('errors', []))))
            
            if result.get('errors'):
                print()
                print_section("Errors:")
                for error in result['errors'][:10]:
                    print_item(f"  - {Path(error['file']).name}", error['error'])
        return
    
    if args.diagnose:
        print_header("Diagnostics: Integration Tests")
        print()
        with get_connection() as conn:
            diag = diagnose_integration_tests(conn)
            print_section("Integration Test Diagnostics:")
            print_item("Total integration/e2e tests", diag['total_integration_tests'])
            
            if diag['agent_flow_tests']:
                print_item("test_agent_flow.py found", f"{len(diag['agent_flow_tests'])} test(s)")
            else:
                print_item("test_agent_flow.py", "NOT FOUND in database")
            
            if diag.get('suggestions'):
                print()
                print_section("Suggestions:")
                for suggestion in diag['suggestions']:
                    print_item(f"  - {suggestion}", "")
        return
    
    if args.deduplicate:
        print_header("Finding Duplicate Tests")
        print()
        with get_connection() as conn:
            duplicates_info = find_duplicate_tests(conn)
            print_section("Duplicate Analysis:")
            print_item("Total tests in database", str(duplicates_info['total_tests']))
            print_item("Unique tests", str(duplicates_info['unique_tests']))
            print_item("Duplicate groups", str(duplicates_info['duplicate_groups']))
            print_item("Duplicate tests to remove", str(duplicates_info['duplicate_tests']))
            
            if duplicates_info['duplicate_groups'] > 0:
                print()
                print_section("Sample Duplicates (first 5 groups):")
                for i, (key, tests) in enumerate(list(duplicates_info['duplicates'].items())[:5]):
                    normalized_path, class_name, method_name = key
                    print_item(f"  Group {i+1}: {Path(normalized_path).name}", f"{len(tests)} duplicates")
                    for test in tests[:3]:
                        print_item(f"    - {test['test_id']}", f"path: {test['file_path'][:60]}...")
                    if len(tests) > 3:
                        print_item(f"    ... and {len(tests) - 3} more", "")
                
                if args.remove_duplicates:
                    print()
                    print_section("Removing Duplicates...")
                    result = remove_duplicate_tests(conn, dry_run=False)
                    print_item("Tests removed", result['removed'])
                    print_item("Tests kept", result['kept'])
                    print()
                    print_item("Duplicates removed successfully!", "")
                else:
                    print()
                    print_item("To remove duplicates, run with --remove-duplicates flag", "")
        return
    
    # Normal processing
    print_header("Git Diff to Test Selection")
    print()
    
    # Step 1: Get diff file path
    if args.diff_file:
        user_path = Path(args.diff_file)
    elif len(sys.argv) > 1:
        user_path = Path(sys.argv[1])
    else:
        user_path = None
    
    if user_path:
        # If absolute path, use it directly
        if user_path.is_absolute():
            diff_file_path = user_path
        else:
            # Try multiple locations in order:
            # 1. Relative to script directory (git_diff_processor/)
            script_dir_path = Path(__file__).parent / user_path
            # 2. Relative to current working directory
            cwd_path = Path.cwd() / user_path
            
            # Check in order of preference
            if script_dir_path.exists():
                diff_file_path = script_dir_path
            elif cwd_path.exists():
                diff_file_path = cwd_path
            else:
                # Use the path as-is (will show error if not found)
                diff_file_path = user_path
    else:
        # Default to sample_diffs folder
        default_path = Path(__file__).parent / "sample_diffs" / "diff_commit1.txt"
        if default_path.exists():
            diff_file_path = default_path
            print_section(f"Using default diff file: {diff_file_path}")
        else:
            print_section("No diff file specified!")
            print_item("Usage", "python git_diff_processor.py <path_to_diff_file>")
            print()
            print_item("Options", "--verify, --reindex, --diagnose")
            print()
            print_item("Or save your git diff to", str(default_path))
            return
    
    # Step 2: Test database connection
    print_section("Testing database connection...")
    if not test_connection():
        print()
        print("ERROR: Cannot connect to database!")
        print("Please ensure the deterministic database is set up and .env is configured.")
        return
    print()
    
    # Step 3: Read git diff file
    print_section("Step 1: Reading git diff file...")
    try:
        # Show resolved path (absolute for clarity)
        resolved_path = diff_file_path.resolve()
        diff_content = read_diff_file(diff_file_path)
        print_item("File", str(resolved_path))
        print_item("File size", f"{len(diff_content)} characters")
        print_item("Status", "[OK] File read successfully")
    except FileNotFoundError as e:
        print(f"ERROR: {e}")
        return
    except Exception as e:
        print(f"ERROR: Could not read file: {e}")
        return
    print()
    
    # Step 4: Parse git diff
    print_section("Step 2: Parsing git diff...")
    parsed_diff = parse_git_diff(diff_content)
    
    # Filter and show production Python files
    from utils.diff_parser import is_production_python_file
    production_files = [f for f in parsed_diff['file_changes'] 
                       if is_production_python_file(f['file'])]
    non_production_files = [f for f in parsed_diff['file_changes'] 
                            if not is_production_python_file(f['file'])]
    
    print_item("Total changed files", len(parsed_diff['file_changes']))
    print_item("Production Python files", len(production_files))
    print_item("Non-production files (filtered)", len(non_production_files))
    if non_production_files:
        print_item("  (Skipping: artifacts, data files, frontend, config, etc.)", "")
    print()
    
    display_parsed_changes(parsed_diff)
    
    # Step 5: Build search queries
    print_section("Step 3: Building search strategy...")
    search_queries = build_search_queries(parsed_diff['file_changes'])
    display_search_strategy(search_queries)
    
    # Step 6: Query database
    print_section("Step 4: Querying database for affected tests...")
    
    # Diagnostic: Check database contents
    try:
        with get_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute(f"SELECT COUNT(*) FROM {DB_SCHEMA}.reverse_index")
                reverse_index_count = cursor.fetchone()[0]
                cursor.execute(f"SELECT COUNT(*) FROM {DB_SCHEMA}.test_registry")
                test_registry_count = cursor.fetchone()[0]
                
                if reverse_index_count == 0:
                    print_item("WARNING: reverse_index table is empty!", "")
                    print_item("  Please run test analysis and load data", "")
                    print_item("    1. python test_analysis/04_extract_static_dependencies.py", "")
                    print_item("    2. python test_analysis/06_build_reverse_index.py", "")
                    print_item("    3. python deterministic/04_load_reverse_index.py", "")
                    print()
                elif test_registry_count == 0:
                    print_item("WARNING: test_registry table is empty!", "")
                    print_item("  Please load test registry data", "")
                    print_item("    python deterministic/02_load_test_registry.py", "")
                    print()
                else:
                    print_item(f"Database status", f"{reverse_index_count} reverse_index entries, {test_registry_count} tests")
                    print()
            
            results = find_affected_tests(conn, search_queries, parsed_diff.get('file_changes', []))
            
            # Step 6.5: Run diagnostics if tests seem missing
            expected_candidates = len(search_queries.get('test_file_candidates', []))
            if results['total_tests'] == 0 or (expected_candidates > 0 and results['total_tests'] < expected_candidates * 2):
                print_section("Diagnostics: Checking for missing tests...")
                diagnostics = diagnose_missing_tests(
                    conn, 
                    parsed_diff.get('file_changes', []),
                    search_queries
                )
                
                if diagnostics:
                    print_item("Diagnostic summary", "")
                    for file_path, diag in list(diagnostics.items())[:5]:
                        # Path is already imported at top of file
                        print_item(f"  {Path(file_path).name}", "")
                        if diag.get('suggestions'):
                            for suggestion in diag['suggestions'][:2]:
                                print_item(f"    - {suggestion}", "")
                    if len(diagnostics) > 5:
                        print_item(f"    ... and {len(diagnostics) - 5} more files", "")
                print()
            
            # Step 7: Display results (pass connection to show unused tests)
            print_section("Step 5: Results")
            print()
            display_results(results, conn)
            
            # Step 8: Save complete results to file
            print_section("Saving results to file...")
            output_file = save_results_to_file(results, conn, diff_file_path)
            print_item("Results saved to", str(output_file))
            print()
            
            print_header("Processing Complete!")
            print(f"Selected {results['total_tests']} test(s) to run based on code changes")
            
    except Exception as e:
        print(f"ERROR: Database query failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

```

`git_diff_processor/outputs/test_selection_diff_commit1_20260226_195311.txt`

```
================================================================================
TEST SELECTION RESULTS
================================================================================
Generated: 2026-02-26 19:53:11
Diff File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\git_diff_processor\sample_diffs\diff_commit1.txt

Found 58 affected test(s)

--------------------------------------------------------------------------------
HIGH CONFIDENCE MATCHES (Exact class matches): 58
--------------------------------------------------------------------------------

  test_0070: TestCatalogServer.test_list_tables_tool
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\mcp_servers\test_catalog_server.py
    Matched classes: mcp_servers (via patch/Mock), mcp_servers.catalog_server.tools (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\mcp_servers\test_catalog_server.py)
    Also matched via: module ()

  test_0071: TestCatalogServer.test_describe_table_tool
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\mcp_servers\test_catalog_server.py
    Matched classes: mcp_servers (via patch/Mock), mcp_servers.catalog_server.tools (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\mcp_servers\test_catalog_server.py)
    Also matched via: module ()

  test_0072: TestCatalogServer.test_get_table_row_count_tool
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\mcp_servers\test_catalog_server.py
    Matched classes: mcp_servers (via patch/Mock), mcp_servers.catalog_server.tools (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\mcp_servers\test_catalog_server.py)
    Also matched via: module ()

  test_0050: TestSettings.test_settings_default_values
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config.settings (via import), config (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0051: TestSettings.test_settings_from_env
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config.settings (via import), config (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0052: TestSettings.test_settings_validate_llm_provider
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config.settings (via import), config (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0053: TestSettings.test_settings_validate_embedding_provider
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config.settings (via import), config (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0054: TestSettings.test_get_settings_singleton
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config.settings (via import), config (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0055: TestSettings.test_clear_settings_cache
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config.settings (via import), config (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0056: TestSettings.test_settings_optional_fields
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config.settings (via import), config (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0062: TestLLMFactory.test_create_provider_gemini
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched classes: llm.factory (via import), config.settings (via import), llm (via patch/Mock), config (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: module ()

  test_0063: TestLLMFactory.test_create_provider_gemini_missing_key
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched classes: llm.factory (via import), config.settings (via import), llm (via patch/Mock), config (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: module ()

  test_0064: TestLLMFactory.test_create_provider_ollama
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched classes: llm.factory (via import), config.settings (via import), llm (via patch/Mock), config (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: module ()

  test_0065: TestLLMFactory.test_create_provider_invalid
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched classes: llm.factory (via import), config.settings (via import), llm (via patch/Mock), config (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: module ()

  test_0066: TestLLMFactory.test_get_available_providers
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched classes: llm.factory (via import), config.settings (via import), llm (via patch/Mock), config (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: module ()

  test_0067: TestLLMFactory.test_create_embedding_provider_gemini
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched classes: llm.factory (via import), config.settings (via import), llm (via patch/Mock), config (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: module ()

  test_0068: TestLLMFactory.test_create_embedding_provider_ollama
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched classes: llm.factory (via import), config.settings (via import), llm (via patch/Mock), config (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: module ()

  test_0069: TestLLMFactory.test_create_embedding_provider_invalid
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched classes: llm.factory (via import), config.settings (via import), llm (via patch/Mock), config (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: module ()

  test_0001: TestLangGraphAgent.test_agent_initialization
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0002: TestLangGraphAgent.test_agent_initialization_failure_no_servers
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0003: TestLangGraphAgent.test_agent_initialization_no_tools
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0004: TestLangGraphAgent.test_agent_invoke
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0005: TestLangGraphAgent.test_agent_invoke_with_session_id
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0006: TestLangGraphAgent.test_agent_stream_invoke
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0007: TestLangGraphAgent.test_agent_stream_invoke_error
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0008: TestLangGraphAgent.test_agent_close
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0009: TestLangGraphAgent.test_agent_context_manager
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0010: TestLangGraphAgent.test_agent_extract_stage_info_agent_thinking
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0011: TestLangGraphAgent.test_agent_extract_stage_info_tool_executing
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0012: TestLangGraphAgent.test_agent_extract_stage_info_tool_completed
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0013: TestLangGraphBuilder.test_builder_initialization
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0014: TestLangGraphBuilder.test_builder_initialization_no_tools
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0015: TestLangGraphBuilder.test_build_graph
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0016: TestLangGraphBuilder.test_build_graph_no_tools
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0017: TestLangGraphBuilder.test_get_graph
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0018: TestLangGraphBuilder.test_get_graph_not_built
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0060: TestAgentWorkflow.test_agent_initialization_workflow
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\integration\test_agent_workflow.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0061: TestAgentWorkflow.test_agent_invocation_workflow
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\integration\test_agent_workflow.py
    Matched classes: agent (via patch/Mock)
    Also matched via: module ()

  test_0019: TestStateConverter.test_normalize_message_content_string
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

  test_0020: TestStateConverter.test_normalize_message_content_list
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

  test_0021: TestStateConverter.test_normalize_message_content_list_simple
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

  test_0022: TestStateConverter.test_normalize_message_content_none
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

  test_0023: TestStateConverter.test_normalize_message_content_dict_with_text
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

  test_0024: TestStateConverter.test_convert_to_langchain_messages
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

  test_0025: TestStateConverter.test_convert_to_langchain_messages_tool
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

  test_0026: TestStateConverter.test_convert_from_langchain_messages
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

  test_0027: TestStateConverter.test_convert_from_langchain_messages_with_tool_calls
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

  test_0028: TestStateConverter.test_convert_langgraph_state_to_agent
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

  test_0029: TestStateConverter.test_convert_langgraph_state_to_agent_with_tool_calls
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

  test_0030: TestToolConverter.test_json_schema_to_pydantic_string
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

  test_0031: TestToolConverter.test_json_schema_to_pydantic_integer
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

  test_0032: TestToolConverter.test_json_schema_to_pydantic_array
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

  test_0033: TestToolConverter.test_json_schema_to_pydantic_empty
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

  test_0034: TestToolConverter.test_mcp_tool_to_langchain
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

  test_0035: TestToolConverter.test_mcp_tool_to_langchain_error_handling
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

  test_0036: TestToolConverter.test_convert_mcp_tools_to_langchain
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

  test_0037: TestToolConverter.test_convert_mcp_tools_to_langchain_empty
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

  test_0038: TestToolConverter.test_convert_mcp_tools_to_langchain_conversion_error
    Test Type: unknown
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: module ()

--------------------------------------------------------------------------------
SUMMARY
--------------------------------------------------------------------------------
Total tests to run: 58
High confidence: 58
Medium confidence: 0

--------------------------------------------------------------------------------
TESTS NOT AFFECTED (UNUSED)
--------------------------------------------------------------------------------
Total unused tests: 16

Unit tests (unused): 16
  test_0039: TestAnalyticsAggregator.test_get_overview_stats
    File: unknown
  test_0040: TestAnalyticsAggregator.test_get_tool_usage_stats
    File: unknown
  test_0041: TestAPIRoutes.test_root_endpoint
    File: unknown
  test_0042: TestAPIRoutes.test_health_endpoint
    File: unknown
  test_0043: TestAPIRoutes.test_chat_endpoint
    File: unknown
  test_0044: TestAPIRoutes.test_chat_endpoint_with_session
    File: unknown
  test_0045: TestAPIRoutes.test_chat_endpoint_invalid_request
    File: unknown
  test_0046: TestAPIRoutes.test_chat_stream_endpoint
    File: unknown
  test_0047: TestAPIRoutes.test_tools_endpoint
    File: unknown
  test_0048: TestAPIRoutes.test_health_check_endpoint
    File: unknown
  test_0049: TestAPIRoutes.test_status_endpoint
    File: unknown
  test_0057: TestInferenceLogger.test_log_request
    File: unknown
  test_0058: TestInferenceLogger.test_get_logs
    File: unknown
  test_0059: TestInferenceLogger.test_get_log_not_found
    File: unknown
  test_0073: TestMLflowTracking.test_get_tracker
    File: unknown
  test_0074: TestMLflowTracking.test_tracker_disabled_when_mlflow_not_available
    File: unknown

--------------------------------------------------------------------------------
UNUSED TESTS SUMMARY
--------------------------------------------------------------------------------
Total tests in repository: 74
Affected tests: 58
Unused tests: 16
Test reduction: 16 tests (21.6%) can be skipped

```

`git_diff_processor/outputs/test_selection_diff_commit1_20260227_105451.txt`

```
================================================================================
TEST SELECTION RESULTS
================================================================================
Generated: 2026-02-27 10:54:51
Diff File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\git_diff_processor\sample_diffs\diff_commit1.txt

Found 58 affected test(s)

--------------------------------------------------------------------------------
VERY HIGH CONFIDENCE MATCHES (Function-level - precise): 8
--------------------------------------------------------------------------------

  test_0062: TestLLMFactory.test_create_provider_gemini
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched functions: llm.factory.create_provider (via call)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: exact ()
    Also matched via: module ()

  test_0063: TestLLMFactory.test_create_provider_gemini_missing_key
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched functions: llm.factory.create_provider (via call)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: exact ()
    Also matched via: module ()

  test_0064: TestLLMFactory.test_create_provider_ollama
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched functions: llm.factory.create_provider (via call)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: exact ()
    Also matched via: module ()

  test_0065: TestLLMFactory.test_create_provider_invalid
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched functions: llm.factory.create_provider (via call)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: exact ()
    Also matched via: module ()

  test_0066: TestLLMFactory.test_get_available_providers
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched functions: llm.factory.get_available_providers (via call)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: exact ()
    Also matched via: module ()

  test_0067: TestLLMFactory.test_create_embedding_provider_gemini
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched functions: llm.factory.create_embedding_provider (via call)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: exact ()
    Also matched via: module ()

  test_0068: TestLLMFactory.test_create_embedding_provider_ollama
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched functions: llm.factory.create_embedding_provider (via call)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: exact ()
    Also matched via: module ()

  test_0069: TestLLMFactory.test_create_embedding_provider_invalid
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched functions: llm.factory.create_embedding_provider (via call)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: exact ()
    Also matched via: module ()

--------------------------------------------------------------------------------
HIGH CONFIDENCE MATCHES (Exact class matches): 50
--------------------------------------------------------------------------------

  test_0070: TestCatalogServer.test_list_tables_tool
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\mcp_servers\test_catalog_server.py
    Matched classes: mcp_servers (via patch/Mock), mcp_servers.catalog_server.tools (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\mcp_servers\test_catalog_server.py)
    Also matched via: module ()

  test_0071: TestCatalogServer.test_describe_table_tool
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\mcp_servers\test_catalog_server.py
    Matched classes: mcp_servers (via patch/Mock), mcp_servers.catalog_server.tools (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\mcp_servers\test_catalog_server.py)
    Also matched via: module ()

  test_0072: TestCatalogServer.test_get_table_row_count_tool
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\mcp_servers\test_catalog_server.py
    Matched classes: mcp_servers (via patch/Mock), mcp_servers.catalog_server.tools (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\mcp_servers\test_catalog_server.py)
    Also matched via: module ()

  test_0050: TestSettings.test_settings_default_values
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config (via import), config.settings (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0051: TestSettings.test_settings_from_env
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config (via import), config.settings (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0052: TestSettings.test_settings_validate_llm_provider
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config (via import), config.settings (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0053: TestSettings.test_settings_validate_embedding_provider
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config (via import), config.settings (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0054: TestSettings.test_get_settings_singleton
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config (via import), config.settings (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0055: TestSettings.test_clear_settings_cache
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config (via import), config.settings (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0056: TestSettings.test_settings_optional_fields
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config (via import), config.settings (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0001: TestLangGraphAgent.test_agent_initialization
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0002: TestLangGraphAgent.test_agent_initialization_failure_no_servers
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0003: TestLangGraphAgent.test_agent_initialization_no_tools
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0004: TestLangGraphAgent.test_agent_invoke
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0005: TestLangGraphAgent.test_agent_invoke_with_session_id
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0006: TestLangGraphAgent.test_agent_stream_invoke
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0007: TestLangGraphAgent.test_agent_stream_invoke_error
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0008: TestLangGraphAgent.test_agent_close
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0009: TestLangGraphAgent.test_agent_context_manager
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0010: TestLangGraphAgent.test_agent_extract_stage_info_agent_thinking
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0011: TestLangGraphAgent.test_agent_extract_stage_info_tool_executing
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0012: TestLangGraphAgent.test_agent_extract_stage_info_tool_completed
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0013: TestLangGraphBuilder.test_builder_initialization
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py)
    Also matched via: module ()

  test_0014: TestLangGraphBuilder.test_builder_initialization_no_tools
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py)
    Also matched via: module ()

  test_0015: TestLangGraphBuilder.test_build_graph
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py)
    Also matched via: module ()

  test_0016: TestLangGraphBuilder.test_build_graph_no_tools
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py)
    Also matched via: module ()

  test_0017: TestLangGraphBuilder.test_get_graph
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py)
    Also matched via: module ()

  test_0018: TestLangGraphBuilder.test_get_graph_not_built
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py)
    Also matched via: module ()

  test_0019: TestStateConverter.test_normalize_message_content_string
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0020: TestStateConverter.test_normalize_message_content_list
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0021: TestStateConverter.test_normalize_message_content_list_simple
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0022: TestStateConverter.test_normalize_message_content_none
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0023: TestStateConverter.test_normalize_message_content_dict_with_text
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0024: TestStateConverter.test_convert_to_langchain_messages
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0025: TestStateConverter.test_convert_to_langchain_messages_tool
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0026: TestStateConverter.test_convert_from_langchain_messages
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0027: TestStateConverter.test_convert_from_langchain_messages_with_tool_calls
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0028: TestStateConverter.test_convert_langgraph_state_to_agent
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0029: TestStateConverter.test_convert_langgraph_state_to_agent_with_tool_calls
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0030: TestToolConverter.test_json_schema_to_pydantic_string
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py)
    Also matched via: module ()

  test_0031: TestToolConverter.test_json_schema_to_pydantic_integer
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py)
    Also matched via: module ()

  test_0032: TestToolConverter.test_json_schema_to_pydantic_array
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py)
    Also matched via: module ()

  test_0033: TestToolConverter.test_json_schema_to_pydantic_empty
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py)
    Also matched via: module ()

  test_0034: TestToolConverter.test_mcp_tool_to_langchain
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py)
    Also matched via: module ()

  test_0035: TestToolConverter.test_mcp_tool_to_langchain_error_handling
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py)
    Also matched via: module ()

  test_0036: TestToolConverter.test_convert_mcp_tools_to_langchain
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py)
    Also matched via: module ()

  test_0037: TestToolConverter.test_convert_mcp_tools_to_langchain_empty
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py)
    Also matched via: module ()

  test_0038: TestToolConverter.test_convert_mcp_tools_to_langchain_conversion_error
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py)
    Also matched via: module ()

  test_0060: TestAgentWorkflow.test_agent_initialization_workflow
    Test Type: integration
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\integration\test_agent_workflow.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\integration\test_agent_workflow.py)
    Also matched via: module ()

  test_0061: TestAgentWorkflow.test_agent_invocation_workflow
    Test Type: integration
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\integration\test_agent_workflow.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\integration\test_agent_workflow.py)
    Also matched via: module ()

--------------------------------------------------------------------------------
SUMMARY
--------------------------------------------------------------------------------
Total tests to run: 58
Very high confidence (function-level): 8
High confidence: 50
Medium confidence: 0

--------------------------------------------------------------------------------
TESTS NOT AFFECTED (UNUSED)
--------------------------------------------------------------------------------
Total unused tests: 16

Unit tests (unused): 16
  test_0039: TestAnalyticsAggregator.test_get_overview_stats
    File: unknown
  test_0040: TestAnalyticsAggregator.test_get_tool_usage_stats
    File: unknown
  test_0041: TestAPIRoutes.test_root_endpoint
    File: unknown
  test_0042: TestAPIRoutes.test_health_endpoint
    File: unknown
  test_0043: TestAPIRoutes.test_chat_endpoint
    File: unknown
  test_0044: TestAPIRoutes.test_chat_endpoint_with_session
    File: unknown
  test_0045: TestAPIRoutes.test_chat_endpoint_invalid_request
    File: unknown
  test_0046: TestAPIRoutes.test_chat_stream_endpoint
    File: unknown
  test_0047: TestAPIRoutes.test_tools_endpoint
    File: unknown
  test_0048: TestAPIRoutes.test_health_check_endpoint
    File: unknown
  test_0049: TestAPIRoutes.test_status_endpoint
    File: unknown
  test_0057: TestInferenceLogger.test_log_request
    File: unknown
  test_0058: TestInferenceLogger.test_get_logs
    File: unknown
  test_0059: TestInferenceLogger.test_get_log_not_found
    File: unknown
  test_0073: TestMLflowTracking.test_get_tracker
    File: unknown
  test_0074: TestMLflowTracking.test_tracker_disabled_when_mlflow_not_available
    File: unknown

--------------------------------------------------------------------------------
UNUSED TESTS SUMMARY
--------------------------------------------------------------------------------
Total tests in repository: 74
Affected tests: 58
Unused tests: 16
Test reduction: 16 tests (21.6%) can be skipped

```

`git_diff_processor/outputs/test_selection_diff_commit1_20260227_105511.txt`

```
================================================================================
TEST SELECTION RESULTS
================================================================================
Generated: 2026-02-27 10:55:11
Diff File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\git_diff_processor\sample_diffs\diff_commit1.txt

Found 58 affected test(s)

--------------------------------------------------------------------------------
VERY HIGH CONFIDENCE MATCHES (Function-level - precise): 8
--------------------------------------------------------------------------------

  test_0062: TestLLMFactory.test_create_provider_gemini
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched functions: llm.factory.create_provider (via call)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: exact ()
    Also matched via: module ()

  test_0063: TestLLMFactory.test_create_provider_gemini_missing_key
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched functions: llm.factory.create_provider (via call)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: exact ()
    Also matched via: module ()

  test_0064: TestLLMFactory.test_create_provider_ollama
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched functions: llm.factory.create_provider (via call)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: exact ()
    Also matched via: module ()

  test_0065: TestLLMFactory.test_create_provider_invalid
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched functions: llm.factory.create_provider (via call)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: exact ()
    Also matched via: module ()

  test_0066: TestLLMFactory.test_get_available_providers
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched functions: llm.factory.get_available_providers (via call)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: exact ()
    Also matched via: module ()

  test_0067: TestLLMFactory.test_create_embedding_provider_gemini
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched functions: llm.factory.create_embedding_provider (via call)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: exact ()
    Also matched via: module ()

  test_0068: TestLLMFactory.test_create_embedding_provider_ollama
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched functions: llm.factory.create_embedding_provider (via call)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: exact ()
    Also matched via: module ()

  test_0069: TestLLMFactory.test_create_embedding_provider_invalid
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py
    Matched functions: llm.factory.create_embedding_provider (via call)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\llm\test_factory.py)
    Also matched via: exact ()
    Also matched via: module ()

--------------------------------------------------------------------------------
HIGH CONFIDENCE MATCHES (Exact class matches): 50
--------------------------------------------------------------------------------

  test_0050: TestSettings.test_settings_default_values
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config (via import), config.settings (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0051: TestSettings.test_settings_from_env
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config (via import), config.settings (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0052: TestSettings.test_settings_validate_llm_provider
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config (via import), config.settings (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0053: TestSettings.test_settings_validate_embedding_provider
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config (via import), config.settings (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0054: TestSettings.test_get_settings_singleton
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config (via import), config.settings (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0055: TestSettings.test_clear_settings_cache
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config (via import), config.settings (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0056: TestSettings.test_settings_optional_fields
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py
    Matched classes: config (via import), config.settings (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\config\test_settings.py)
    Also matched via: module ()

  test_0001: TestLangGraphAgent.test_agent_initialization
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0002: TestLangGraphAgent.test_agent_initialization_failure_no_servers
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0003: TestLangGraphAgent.test_agent_initialization_no_tools
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0004: TestLangGraphAgent.test_agent_invoke
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0005: TestLangGraphAgent.test_agent_invoke_with_session_id
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0006: TestLangGraphAgent.test_agent_stream_invoke
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0007: TestLangGraphAgent.test_agent_stream_invoke_error
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0008: TestLangGraphAgent.test_agent_close
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0009: TestLangGraphAgent.test_agent_context_manager
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0010: TestLangGraphAgent.test_agent_extract_stage_info_agent_thinking
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0011: TestLangGraphAgent.test_agent_extract_stage_info_tool_executing
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0012: TestLangGraphAgent.test_agent_extract_stage_info_tool_completed
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_agent.py)
    Also matched via: module ()

  test_0013: TestLangGraphBuilder.test_builder_initialization
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py)
    Also matched via: module ()

  test_0014: TestLangGraphBuilder.test_builder_initialization_no_tools
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py)
    Also matched via: module ()

  test_0015: TestLangGraphBuilder.test_build_graph
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py)
    Also matched via: module ()

  test_0016: TestLangGraphBuilder.test_build_graph_no_tools
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py)
    Also matched via: module ()

  test_0017: TestLangGraphBuilder.test_get_graph
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py)
    Also matched via: module ()

  test_0018: TestLangGraphBuilder.test_get_graph_not_built
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_langgraph_builder.py)
    Also matched via: module ()

  test_0019: TestStateConverter.test_normalize_message_content_string
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0020: TestStateConverter.test_normalize_message_content_list
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0021: TestStateConverter.test_normalize_message_content_list_simple
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0022: TestStateConverter.test_normalize_message_content_none
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0023: TestStateConverter.test_normalize_message_content_dict_with_text
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0024: TestStateConverter.test_convert_to_langchain_messages
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0025: TestStateConverter.test_convert_to_langchain_messages_tool
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0026: TestStateConverter.test_convert_from_langchain_messages
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0027: TestStateConverter.test_convert_from_langchain_messages_with_tool_calls
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0028: TestStateConverter.test_convert_langgraph_state_to_agent
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0029: TestStateConverter.test_convert_langgraph_state_to_agent_with_tool_calls
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_state_converter.py)
    Also matched via: module ()

  test_0030: TestToolConverter.test_json_schema_to_pydantic_string
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py)
    Also matched via: module ()

  test_0031: TestToolConverter.test_json_schema_to_pydantic_integer
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py)
    Also matched via: module ()

  test_0032: TestToolConverter.test_json_schema_to_pydantic_array
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py)
    Also matched via: module ()

  test_0033: TestToolConverter.test_json_schema_to_pydantic_empty
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py)
    Also matched via: module ()

  test_0034: TestToolConverter.test_mcp_tool_to_langchain
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py)
    Also matched via: module ()

  test_0035: TestToolConverter.test_mcp_tool_to_langchain_error_handling
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py)
    Also matched via: module ()

  test_0036: TestToolConverter.test_convert_mcp_tools_to_langchain
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py)
    Also matched via: module ()

  test_0037: TestToolConverter.test_convert_mcp_tools_to_langchain_empty
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py)
    Also matched via: module ()

  test_0038: TestToolConverter.test_convert_mcp_tools_to_langchain_conversion_error
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py
    Matched classes: agent (via import)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\agent\test_tool_converter.py)
    Also matched via: module ()

  test_0060: TestAgentWorkflow.test_agent_initialization_workflow
    Test Type: integration
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\integration\test_agent_workflow.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\integration\test_agent_workflow.py)
    Also matched via: module ()

  test_0061: TestAgentWorkflow.test_agent_invocation_workflow
    Test Type: integration
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\integration\test_agent_workflow.py
    Matched classes: agent (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\integration\test_agent_workflow.py)
    Also matched via: module ()

  test_0070: TestCatalogServer.test_list_tables_tool
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\mcp_servers\test_catalog_server.py
    Matched classes: mcp_servers.catalog_server.tools (via import), mcp_servers (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\mcp_servers\test_catalog_server.py)
    Also matched via: module ()

  test_0071: TestCatalogServer.test_describe_table_tool
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\mcp_servers\test_catalog_server.py
    Matched classes: mcp_servers.catalog_server.tools (via import), mcp_servers (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\mcp_servers\test_catalog_server.py)
    Also matched via: module ()

  test_0072: TestCatalogServer.test_get_table_row_count_tool
    Test Type: unit
    File: C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\mcp_servers\test_catalog_server.py
    Matched classes: mcp_servers.catalog_server.tools (via import), mcp_servers (via patch/Mock)
    Also matched via: direct_file (C:\Users\SakethN.IDEYALABS\OneDrive - IdeyaLabs\Documents\Planon\sample_workflow\test_repository\mcp_servers\test_catalog_server.py)
    Also matched via: module ()

--------------------------------------------------------------------------------
SUMMARY
--------------------------------------------------------------------------------
Total tests to run: 58
Very high confidence (function-level): 8
High confidence: 50
Medium confidence: 0

--------------------------------------------------------------------------------
TESTS NOT AFFECTED (UNUSED)
--------------------------------------------------------------------------------
Total unused tests: 16

Unit tests (unused): 16
  test_0039: TestAnalyticsAggregator.test_get_overview_stats
    File: unknown
  test_0040: TestAnalyticsAggregator.test_get_tool_usage_stats
    File: unknown
  test_0041: TestAPIRoutes.test_root_endpoint
    File: unknown
  test_0042: TestAPIRoutes.test_health_endpoint
    File: unknown
  test_0043: TestAPIRoutes.test_chat_endpoint
    File: unknown
  test_0044: TestAPIRoutes.test_chat_endpoint_with_session
    File: unknown
  test_0045: TestAPIRoutes.test_chat_endpoint_invalid_request
    File: unknown
  test_0046: TestAPIRoutes.test_chat_stream_endpoint
    File: unknown
  test_0047: TestAPIRoutes.test_tools_endpoint
    File: unknown
  test_0048: TestAPIRoutes.test_health_check_endpoint
    File: unknown
  test_0049: TestAPIRoutes.test_status_endpoint
    File: unknown
  test_0057: TestInferenceLogger.test_log_request
    File: unknown
  test_0058: TestInferenceLogger.test_get_logs
    File: unknown
  test_0059: TestInferenceLogger.test_get_log_not_found
    File: unknown
  test_0073: TestMLflowTracking.test_get_tracker
    File: unknown
  test_0074: TestMLflowTracking.test_tracker_disabled_when_mlflow_not_available
    File: unknown

--------------------------------------------------------------------------------
UNUSED TESTS SUMMARY
--------------------------------------------------------------------------------
Total tests in repository: 74
Affected tests: 58
Unused tests: 16
Test reduction: 16 tests (21.6%) can be skipped

```

`git_diff_processor/README.md`

```markdown
# Git Diff Processor - Usage Guide

## Overview

The Git Diff Processor reads git diff output and finds which tests should be run based on code changes. It queries the deterministic database to identify affected tests.

**What it does:**
1. Reads git diff from a file
2. Parses changes (files, classes, methods)
3. Shows what will be searched in the database
4. Queries database to find affected tests
5. Displays results with explanations

---

## Quick Start

### Step 1: Generate Git Diff

Save your git diff to a file:

```bash
# Option 1: Diff between two commits
git diff commit1 commit2 > git_diff_processor/sample_diffs/diff_commit1.txt

# Option 2: Single commit
git show commit_hash > git_diff_processor/sample_diffs/diff_commit1.txt

# Option 3: Uncommitted changes
git diff > git_diff_processor/sample_diffs/diff_uncommitted.txt
```

### Step 2: Run the Processor

```bash
# Specify file path
python git_diff_processor/git_diff_processor.py sample_diffs/diff_commit1.txt

# Or use default (sample_diffs/diff_commit1.txt)
python git_diff_processor/git_diff_processor.py
```

---

## How It Works

### Step-by-Step Process

**Step 1: Read Git Diff**
- Reads the diff file you provide
- Validates file exists and is readable

**Step 2: Parse Changes**
- Extracts changed files
- Identifies changed classes (from `class` definitions)
- Identifies changed methods (from `def` definitions)
- Counts additions/deletions

**Step 3: Build Search Strategy**
- Converts file paths to production class names
- Creates exact match queries (e.g., `agent.agent_pool`)
- Creates module-level patterns (e.g., `agent.*`)
- Shows what will be searched

**Step 4: Query Database**
- Queries `reverse_index` table for exact matches
- Queries for module patterns
- Combines results

**Step 5: Display Results**
- Shows all affected tests
- Indicates confidence level (high/medium)
- Explains why each test was selected

---

## Example Output

```
==================================================
Git Diff to Test Selection
==================================================

  Testing database connection...
[OK] Connected to database: planon
[OK] Using schema: planon1

  Step 1: Reading git diff file...
    File: sample_diffs/diff_commit1.txt
    File size: 245 characters
    Status: [OK] File read successfully

  Step 2: Parsing git diff...
  Parsed Changes:

    Changed files: 1
      - agent/agent_pool.py (modified)
    Changed classes: 0
    Changed methods: 2
      - get_agent
      - reset_agent

  Step 3: Building search strategy...
  What We'll Search in Database:

    Exact production class matches: 1
      - agent.agent_pool
    
    Database tables to query:
      - reverse_index (primary - fast lookup)
      - test_dependencies (secondary - fallback)

  Step 4: Querying database for affected tests...
  Querying database (Exact matches)...
    agent.agent_pool: 3 tests

  Step 5: Results
  Test Selection Results:

    Found 3 affected test(s):

  High Confidence Matches (Exact class matches): 3
    test_0001: TestAgentPool.test_get_agent_creates_new_instance
      Matched classes: agent.agent_pool
    test_0002: TestAgentPool.test_get_agent_reuses_instance
      Matched classes: agent.agent_pool
    test_0003: TestAgentPool.test_reset_agent
      Matched classes: agent.agent_pool

  Summary:
    Total tests to run: 3
    High confidence: 3
    Medium confidence: 0

==================================================
Processing Complete!
==================================================
Selected 3 test(s) to run based on code changes
```

---

## Understanding the Output

### Changed Files
Shows which production files were modified, added, or deleted.

### Search Strategy
Shows what production classes will be searched in the database:
- **Exact matches**: Direct class names (most reliable)
- **Module patterns**: Module-level matches like `agent.*` (broader search)

### Test Results
- **High confidence**: Tests that directly reference the changed class (exact match)
- **Medium confidence**: Tests that match module patterns (broader match)

### Why Tests Were Selected
Each test shows which production class matched, helping you understand why it was selected.

---

## File Format

The processor expects standard git unified diff format:

```diff
diff --git a/path/to/file.py b/path/to/file.py
index abc123..def456 100644
--- a/path/to/file.py
+++ b/path/to/file.py
@@ -10,6 +10,8 @@ def function_name():
+    # New line
     existing_code
+    return value
```

**Supported:**
- ✅ Multiple files in one diff
- ✅ Additions, deletions, modifications
- ✅ Class and method changes
- ✅ Standard git diff format

---

## Database Connection

The processor connects to the deterministic database:
- Uses `deterministic/db_connection.py`
- Queries `planon1` schema
- Requires `.env` file with database credentials

**Make sure:**
1. Deterministic database is set up
2. `.env` file is configured
3. Database connection test passes

---

## Common Scenarios

### Scenario 1: Single File Change

**Git Diff:** One file changed (`agent/agent_pool.py`)

**Result:** Finds all tests that reference `agent.agent_pool`

**Example:** 3 tests found (all test methods in `TestAgentPool` class)

---

### Scenario 2: Multiple Files Changed

**Git Diff:** Multiple files changed

**Result:** Finds tests for all changed files, combines results

**Example:** 
- `agent/agent_pool.py` → 3 tests
- `api/routes.py` → 5 tests
- **Total:** 8 unique tests (some may overlap)

---

### Scenario 3: Module-Level Changes

**Git Diff:** Multiple files in same module changed (e.g., `agent/*.py`)

**Result:** Uses module pattern matching (`agent.*`)

**Example:** Finds all tests that reference any class in `agent` module

---

### Scenario 4: No Tests Found

**Possible reasons:**
- Changed files are not referenced by any tests
- Changed files are test files themselves
- Production class names don't match database entries

**What to do:**
- Check if the changed file is actually production code
- Verify class names match what's in the database
- Consider running full regression if unsure

---

## Advanced Usage

### Custom Diff File Location

```bash
python git_diff_processor/git_diff_processor.py /path/to/your/diff.txt
```

### Processing Multiple Diffs

```bash
# Process first commit
python git_diff_processor/git_diff_processor.py sample_diffs/diff_commit1.txt

# Process second commit
python git_diff_processor/git_diff_processor.py sample_diffs/diff_commit2.txt
```

---

## Troubleshooting

### Error: "Diff file not found"
**Solution:** Check the file path. Use absolute path if needed.

### Error: "Cannot connect to database"
**Solution:** 
1. Run `python deterministic/db_connection.py` to test connection
2. Check `.env` file configuration
3. Ensure database is running

### Error: "No tests found"
**Possible causes:**
- Changed files are test files (not production code)
- Class names don't match database entries
- Files are new and not yet referenced by tests

**Solution:** Check the parsed changes output to see what was detected.

---

## Next Steps

After getting test selection results:
1. Review the selected tests
2. Run the selected tests in your CI/CD pipeline
3. Collect test results for feedback loop
4. Use results to improve selection accuracy

---

## Integration with CI/CD

This processor can be integrated into CI/CD pipelines:

```bash
# In CI/CD script
git diff $BASE_COMMIT $HEAD_COMMIT > diff.txt
python git_diff_processor/git_diff_processor.py diff.txt > selected_tests.txt
# Use selected_tests.txt to run only affected tests
```

---

## Files in This Package

- `git_diff_processor.py` - Main script
- `utils/diff_parser.py` - Diff parsing utilities
- `sample_diffs/` - Folder for your diff files
- `README.md` - This file

---

## Dependencies

- `deterministic/db_connection.py` - Database connection
- `deterministic/utils/db_helpers.py` - Database query helpers
- Python standard library (no external dependencies)

---

## Support

For issues or questions:
1. Check the parsed changes output
2. Verify database connection
3. Review the search strategy display
4. Check database has test data loaded

```

`git_diff_processor/sample_diffs/diff_commit1.txt`

```
diff --git a/.gitignore b/.gitignore
index b7faf40..a0b822a 100644
--- a/.gitignore
+++ b/.gitignore
@@ -2,7 +2,7 @@
 __pycache__/
 *.py[codz]
 *$py.class
-
+mlruns/
 # C extensions
 *.so
 
diff --git a/.vscode/settings.json b/.vscode/settings.json
new file mode 100644
index 0000000..d402282
--- /dev/null
+++ b/.vscode/settings.json
@@ -0,0 +1,4 @@
+{
+    "python.terminal.useEnvFile": true,
+    "python.envFile": "${workspaceFolder}/.env"
+}
diff --git a/PROJECT_STRUCTURE_OBSERVATION.md b/PROJECT_STRUCTURE_OBSERVATION.md
new file mode 100644
index 0000000..bc2f1a3
--- /dev/null
+++ b/PROJECT_STRUCTURE_OBSERVATION.md
@@ -0,0 +1,294 @@
+# Project Structure Observation
+
+## 📊 Current State Analysis
+
+### ✅ **Well-Organized Structure**
+
+The project has been reorganized with clear separation of concerns and proper documentation structure.
+
+---
+
+## 📁 Directory Structure
+
+```
+multi_tool_orchestration/
+├── 📚 docs/                          # Well-organized documentation
+│   ├── guides/                       # User guides and tutorials
+│   ├── implementation/               # Implementation details
+│   └── migration/                    # Migration guides
+│
+├── 🤖 llm/                           # LLM abstraction layer (COMPLETE)
+│   ├── base.py                       # Abstract interface
+│   ├── factory.py                   # Provider factory
+│   ├── models.py                     # Pydantic models
+│   ├── gemini_client.py             # Gemini implementation ✅
+│   ├── ollama_client.py             # Ollama implementation ✅
+│   ├── openai_client.py             # OpenAI placeholder
+│   ├── anthropic_client.py          # Anthropic placeholder
+│   └── README.md                     # Module documentation
+│
+├── ⚙️ config/                        # Configuration (COMPLETE)
+│   ├── settings.py                   # Multi-provider settings ✅
+│   └── __init__.py
+│
+├── 📝 examples/                      # Usage examples (COMPLETE)
+│   ├── llm_usage_example.py         # Basic LLM usage ✅
+│   └── hybrid_embedding_example.py   # Hybrid embedding demo ✅
+│
+├── 🧪 tests/                         # Test suite (PARTIAL)
+│   ├── test_llm_abstraction.py      # LLM abstraction tests ✅
+│   └── fixtures/
+│
+├── 🏗️ agent/                        # LangGraph agent (TO BE IMPLEMENTED)
+│   └── prompts/
+│
+├── 🔌 mcp_servers/                   # MCP servers (TO BE IMPLEMENTED)
+│   ├── catalog_server/
+│   ├── vector_search_server/
+│   └── sql_query_server/
+│
+├── 🌐 api/                           # FastAPI (TO BE IMPLEMENTED)
+├── 📊 mlflow/                        # MLflow (TO BE IMPLEMENTED)
+├── 📋 logging/                       # Logging (TO BE IMPLEMENTED)
+├── ⚠️ error_handling/                # Error handling (TO BE IMPLEMENTED)
+├── 📦 scripts/                       # Utility scripts (TO BE IMPLEMENTED)
+└── 💾 data/                          # Data storage
+```
+
+---
+
+## ✅ **What's Complete**
+
+### 1. **LLM Abstraction Layer** (100% Complete)
+- ✅ Abstract base class (`LLMProvider`)
+- ✅ Factory pattern for provider creation
+- ✅ Gemini client (fully implemented)
+- ✅ Ollama client (fully implemented)
+- ✅ OpenAI client (placeholder)
+- ✅ Anthropic client (placeholder)
+- ✅ Common models (LLMRequest, LLMResponse, etc.)
+
+### 2. **Hybrid Embedding Support** (100% Complete)
+- ✅ Independent embedding provider selection
+- ✅ Ollama embeddings working (verified in terminal output)
+- ✅ Gemini embeddings supported (quota-limited but functional)
+- ✅ Factory method: `create_embedding_provider()`
+- ✅ Configuration: `EMBEDDING_PROVIDER` setting
+
+### 3. **Configuration System** (100% Complete)
+- ✅ Multi-provider settings
+- ✅ Environment variable support
+- ✅ Separate LLM and embedding provider selection
+- ✅ Ollama configuration options
+- ✅ Pydantic validation
+
+### 4. **Documentation** (100% Complete)
+- ✅ Well-organized in `docs/` folder
+- ✅ Guides, implementation docs, migration guides
+- ✅ README files with clear navigation
+- ✅ Code examples working
+
+### 5. **Examples** (100% Complete)
+- ✅ Basic LLM usage example
+- ✅ Hybrid embedding example (working as shown in terminal)
+
+---
+
+## ⚠️ **What's Pending**
+
+### 1. **MCP Servers** (0% Complete)
+- ❌ Base MCP server
+- ❌ Catalog server
+- ❌ Vector search server
+- ❌ SQL query server
+
+### 2. **Agent Layer** (0% Complete)
+- ❌ LangGraph agent graph
+- ❌ MCP client
+- ❌ Tool orchestration
+- ❌ State management
+
+### 3. **API Layer** (0% Complete)
+- ❌ FastAPI application
+- ❌ Routes and endpoints
+- ❌ Middleware
+- ❌ Request ID propagation
+
+### 4. **Supporting Infrastructure** (0% Complete)
+- ❌ MLflow integration
+- ❌ Logging system
+- ❌ Error handling
+- ❌ Utility scripts
+
+---
+
+## 🎯 **Key Observations**
+
+### ✅ **Strengths**
+
+1. **Clean Architecture**
+   - Clear separation of concerns
+   - Well-organized directory structure
+   - Proper abstraction layers
+
+2. **Model Abstraction**
+   - Excellent abstraction layer design
+   - Easy to add new providers
+   - Consistent interface across providers
+
+3. **Hybrid Approach**
+   - Smart design for cost optimization
+   - Independent provider selection
+   - Working implementation (Ollama verified)
+
+4. **Documentation**
+   - Well-organized documentation structure
+   - Clear guides and examples
+   - Good navigation
+
+5. **Configuration**
+   - Flexible configuration system
+   - Environment variable support
+   - Provider-agnostic settings
+
+### ⚠️ **Areas for Attention**
+
+1. **Dependencies**
+   - `google-genai>=1.0.0` in requirements (new package)
+   - But code still uses `google.generativeai` (deprecated)
+   - Need to migrate or update requirements
+
+2. **Test Coverage**
+   - Only basic LLM abstraction tests
+   - Missing integration tests
+   - No MCP server tests
+
+3. **Error Handling**
+   - Basic error handling in clients
+   - No retry logic
+   - No circuit breaker pattern
+
+4. **Vector Store Integration**
+   - Hybrid embeddings implemented
+   - But vector search server not yet built
+   - Need to integrate with ChromaDB
+
+---
+
+## 📊 **Implementation Progress**
+
+| Component | Status | Progress |
+|-----------|--------|----------|
+| LLM Abstraction | ✅ Complete | 100% |
+| Hybrid Embeddings | ✅ Complete | 100% |
+| Configuration | ✅ Complete | 100% |
+| Documentation | ✅ Complete | 100% |
+| Examples | ✅ Complete | 100% |
+| MCP Servers | ❌ Pending | 0% |
+| Agent Layer | ❌ Pending | 0% |
+| API Layer | ❌ Pending | 0% |
+| MLflow | ❌ Pending | 0% |
+| Logging | ❌ Pending | 0% |
+| Error Handling | ❌ Pending | 0% |
+
+**Overall Progress: ~25%** (Foundation complete, core features pending)
+
+---
+
+## 🔍 **Code Quality Observations**
+
+### ✅ **Good Practices**
+
+1. **Type Hints**: Proper type annotations throughout
+2. **Pydantic Models**: Type-safe request/response models
+3. **Async/Await**: Proper async implementation
+4. **Error Handling**: Try-catch blocks in place
+5. **Documentation**: Docstrings and comments
+
+### ⚠️ **Improvements Needed**
+
+1. **Gemini Migration**: Still using deprecated `google.generativeai`
+2. **Resource Cleanup**: Ollama client needs explicit cleanup
+3. **Validation**: Could add more input validation
+4. **Logging**: No structured logging yet
+5. **Testing**: Need more comprehensive tests
+
+---
+
+## 🎯 **Current Configuration (from Terminal)**
+
+Based on the terminal output:
+- ✅ **LLM Provider**: `gemini` (working)
+- ✅ **Embedding Provider**: `ollama` (working)
+- ✅ **Ollama URL**: `http://localhost:11434` (accessible)
+- ✅ **Ollama Model**: `nomic-embed-text` (768 dimensions)
+- ⚠️ **Gemini Embeddings**: Quota-limited (but functional)
+
+---
+
+## 💡 **Recommendations**
+
+### Immediate Next Steps
+
+1. **Fix Gemini Package**
+   - Either migrate to `google-genai` or update requirements
+   - Remove deprecation warnings
+
+2. **Implement MCP Servers**
+   - Start with base server
+   - Then catalog, vector, SQL servers
+
+3. **Integrate Vector Store**
+   - Connect hybrid embeddings to ChromaDB
+   - Build vector search server
+
+4. **Add Error Handling**
+   - Retry logic for API calls
+   - Circuit breaker pattern
+   - Better error messages
+
+### Long-term
+
+1. **Complete Agent Layer**
+2. **Build API Layer**
+3. **Add MLflow Integration**
+4. **Comprehensive Testing**
+
+---
+
+## 📝 **Summary**
+
+### ✅ **What's Working**
+- LLM abstraction layer (complete)
+- Hybrid embedding approach (working)
+- Configuration system (complete)
+- Documentation (well-organized)
+- Examples (functional)
+
+### ⚠️ **What's Missing**
+- MCP servers (core feature)
+- Agent orchestration (core feature)
+- API layer (deployment)
+- Supporting infrastructure
+
+### 🎯 **Overall Assessment**
+
+**Foundation: Excellent** ✅
+- Solid architecture
+- Clean code structure
+- Good abstraction design
+- Working hybrid approach
+
+**Core Features: Pending** ⚠️
+- MCP servers need implementation
+- Agent layer needs development
+- Integration work needed
+
+**Status**: Ready to proceed with MCP server implementation. The foundation is solid and well-designed.
+
+---
+
+**Observation Date**: Today  
+**Structure Version**: Reorganized  
+**Foundation Status**: ✅ Complete  
+**Next Phase**: MCP Server Development
diff --git a/README.md b/README.md
index 549cd1b..74e2014 100644
--- a/README.md
+++ b/README.md
@@ -1,2 +1,51 @@
-# multi-tool-orchestration
-a LangGraph agent that connects to MCP (Model Context Protocol) servers  for multi-tool orchestration.
+# Multi-Tool Orchestration
+
+A LangGraph agent that connects to MCP (Model Context Protocol) servers for multi-tool orchestration.
+
+## 📚 Documentation
+
+All documentation is organized in the [`docs/`](docs/) folder:
+
+- **[Project Overview](docs/guides/understanding.md)** - Architecture and design
+- **[Installation Guide](docs/guides/installation_notes.md)** - Setup instructions
+- **[Hybrid Embeddings Guide](docs/guides/hybrid_embeddings.md)** - Using Ollama and Gemini embeddings
+- **[Implementation Status](docs/implementation/implementation_status.md)** - Current progress
+- **[Documentation Index](docs/README.md)** - Complete documentation index
+
+## 🚀 Quick Start
+
+1. **Install dependencies**: `pip install -r requirements.txt`
+2. **Configure**: Copy `.env.example` to `.env` and add your API keys
+3. **Setup data**: Run `python scripts/setup_data.py`
+4. **Start servers**: 
+   - `python -m mcp_servers.catalog_server.server` (Terminal 1)
+   - `python -m mcp_servers.sql_query_server.server` (Terminal 2)
+5. **Test**: Run `python examples/test_mcp_servers.py`
+
+See [Phase 1 Quick Start Guide](docs/guides/phase1_quickstart.md) for detailed instructions.
+
+## ✨ Features
+
+- ✅ **Model Abstraction**: Support for multiple LLM providers (Gemini, Ollama, OpenAI, Anthropic)
+- ✅ **Hybrid Embeddings**: Use Ollama (local) or Gemini (cloud) for embeddings
+- ✅ **Client-Server Architecture**: HTTP-based MCP servers
+- ✅ **Versioning**: Tool and server versioning support
+- ✅ **Authentication**: API key-based authentication
+- ✅ **MCP Servers**: Catalog and SQL Query servers (Phase 1 complete)
+- ✅ **MCP Client**: HTTP client with concurrency control
+- ✅ **Tool Discovery**: Automatic tool discovery from MCP servers
+
+## 📁 Project Structure
+
+```
+multi_tool_orchestration/
+├── docs/                    # All documentation
+├── llm/                     # LLM provider abstraction
+├── config/                  # Configuration
+├── agent/                   # LangGraph agent (to be implemented)
+├── mcp_servers/             # MCP servers (to be implemented)
+├── examples/                # Usage examples
+└── tests/                   # Test suite
+```
+
+See [docs/guides/understanding.md](docs/guides/understanding.md) for complete project structure.
diff --git a/agent/__init__.py b/agent/__init__.py
new file mode 100644
index 0000000..ab2ea96
--- /dev/null
+++ b/agent/__init__.py
@@ -0,0 +1 @@
+"""Agent module."""
diff --git a/agent/mcp_client.py b/agent/mcp_client.py
new file mode 100644
index 0000000..f157336
--- /dev/null
+++ b/agent/mcp_client.py
@@ -0,0 +1,182 @@
+"""MCP client with HTTP + Auth + Concurrency."""
+
+import httpx
+import uuid
+from typing import Dict, Any, Optional, List
+from asyncio import Semaphore
+from datetime import datetime
+from config.settings import get_settings
+
+
+class MCPClient:
+    """HTTP client for MCP servers with authentication and concurrency control."""
+    
+    def __init__(
+        self,
+        max_parallel: Optional[int] = None,
+        timeout: Optional[int] = None
+    ):
+        """Initialize MCP client.
+        
+        Args:
+            max_parallel: Maximum parallel requests (default from settings)
+            timeout: Request timeout in seconds (default from settings)
+        """
+        self.settings = get_settings()
+        self.max_parallel = max_parallel or self.settings.max_parallel_mcp_calls
+        self.timeout = timeout or self.settings.mcp_call_timeout
+        self.semaphore = Semaphore(self.max_parallel)
+        self._client = httpx.AsyncClient(timeout=self.timeout)
+    
+    async def call_tool(
+        self,
+        server_url: str,
+        tool_name: str,
+        params: Dict[str, Any],
+        request_id: Optional[str] = None
+    ) -> Dict[str, Any]:
+        """Call a tool on an MCP server.
+        
+        Args:
+            server_url: Base URL of the MCP server
+            tool_name: Name of the tool to call
+            params: Tool parameters
+            request_id: Request ID for correlation (auto-generated if not provided)
+            
+        Returns:
+            Tool execution result
+            
+        Raises:
+            httpx.HTTPError: If HTTP request fails
+            ValueError: If response is invalid
+        """
+        request_id = request_id or str(uuid.uuid4())
+        
+        # Prepare headers
+        headers = {
+            "Content-Type": "application/json",
+            "X-Request-ID": request_id
+        }
+        
+        # Add authentication if configured
+        if self.settings.mcp_api_key:
+            headers["X-MCP-KEY"] = self.settings.mcp_api_key
+        
+        # Prepare JSON-RPC 2.0 request
+        jsonrpc_request = {
+            "jsonrpc": "2.0",
+            "id": str(uuid.uuid4()),
+            "method": tool_name,
+            "params": params
+        }
+        
+        # Make request with concurrency control
+        async with self.semaphore:
+            try:
+                response = await self._client.post(
+                    f"{server_url}/execute",
+                    json=jsonrpc_request,
+                    headers=headers
+                )
+                
+                # Parse response even if status is not 200
+                try:
+                    result = response.json()
+                except Exception:
+                    # If response is not JSON, raise HTTP error
+                    response.raise_for_status()
+                    raise
+                
+                # Validate JSON-RPC 2.0 response
+                if result.get("jsonrpc") != "2.0":
+                    raise ValueError(f"Invalid JSON-RPC response: {result}")
+                
+                # Check for JSON-RPC errors (even if HTTP status is 200)
+                if "error" in result:
+                    error = result["error"]
+                    error_message = error.get("message", "Unknown error")
+                    error_data = error.get("data", "")
+                    
+                    # Combine message and data for better error info
+                    full_error = f"{error_message}"
+                    if error_data:
+                        full_error += f": {error_data}"
+                    
+                    raise ValueError(full_error)
+                
+                # If HTTP status is not 200 but no JSON-RPC error, raise HTTP error
+                if response.status_code != 200:
+                    response.raise_for_status()
+                
+                return result.get("result", {})
+                
+            except ValueError as e:
+                # Re-raise ValueError (includes JSON-RPC errors)
+                raise
+            except httpx.HTTPError as e:
+                raise Exception(f"MCP server HTTP error: {str(e)}")
+            except Exception as e:
+                raise Exception(f"MCP client error: {str(e)}")
+    
+    async def list_tools(
+        self,
+        server_url: str,
+        request_id: Optional[str] = None
+    ) -> Dict[str, Any]:
+        """List available tools from an MCP server.
+        
+        Args:
+            server_url: Base URL of the MCP server
+            request_id: Request ID for correlation
+            
+        Returns:
+            Dictionary with server info and tools list
+        """
+        request_id = request_id or str(uuid.uuid4())
+        
+        headers = {
+            "X-Request-ID": request_id
+        }
+        
+        if self.settings.mcp_api_key:
+            headers["X-MCP-KEY"] = self.settings.mcp_api_key
+        
+        async with self.semaphore:
+            try:
+                response = await self._client.get(
+                    f"{server_url}/tools",
+                    headers=headers
+                )
+                response.raise_for_status()
+                return response.json()
+            except httpx.HTTPError as e:
+                raise Exception(f"MCP server HTTP error: {str(e)}")
+    
+    async def health_check(self, server_url: str) -> Dict[str, Any]:
+        """Check health of an MCP server.
+        
+        Args:
+            server_url: Base URL of the MCP server
+            
+        Returns:
+            Health status information
+        """
+        async with self.semaphore:
+            try:
+                response = await self._client.get(f"{server_url}/health")
+                response.raise_for_status()
+                return response.json()
+            except httpx.HTTPError as e:
+                raise Exception(f"MCP server health check failed: {str(e)}")
+    
+    async def close(self):
+        """Close the HTTP client."""
+        await self._client.aclose()
+    
+    async def __aenter__(self):
+        """Async context manager entry."""
+        return self
+    
+    async def __aexit__(self, exc_type, exc_val, exc_tb):
+        """Async context manager exit."""
+        await self.close()
diff --git a/agent/tool_binding.py b/agent/tool_binding.py
new file mode 100644
index 0000000..ad4bee8
--- /dev/null
+++ b/agent/tool_binding.py
@@ -0,0 +1,129 @@
+"""Tool discovery and binding system."""
+
+from typing import Dict, Any, List, Optional
+from agent.mcp_client import MCPClient
+from config.settings import get_settings
+
+
+class ToolDiscovery:
+    """Tool discovery system for MCP servers."""
+    
+    def __init__(self, mcp_client: Optional[MCPClient] = None):
+        """Initialize tool discovery.
+        
+        Args:
+            mcp_client: MCP client instance (creates new if not provided)
+        """
+        self.settings = get_settings()
+        self.mcp_client = mcp_client or MCPClient()
+        self._discovered_tools: Dict[str, Dict[str, Any]] = {}
+    
+    async def discover_tools(
+        self,
+        server_url: str,
+        server_name: Optional[str] = None
+    ) -> Dict[str, Any]:
+        """Discover tools from an MCP server.
+        
+        Args:
+            server_url: Base URL of the MCP server
+            server_name: Optional server name for identification
+            
+        Returns:
+            Dictionary with server info and discovered tools
+        """
+        try:
+            tools_info = await self.mcp_client.list_tools(server_url)
+            
+            server_id = server_name or tools_info.get("server_name", server_url)
+            
+            # Store discovered tools
+            for tool in tools_info.get("tools", []):
+                tool_key = f"{server_id}::{tool['name']}"
+                self._discovered_tools[tool_key] = {
+                    "server": server_id,
+                    "server_url": server_url,
+                    "server_version": tools_info.get("server_version"),
+                    "protocol_version": tools_info.get("protocol_version"),
+                    "tool": tool
+                }
+            
+            return {
+                "server": server_id,
+                "server_url": server_url,
+                "server_version": tools_info.get("server_version"),
+                "protocol_version": tools_info.get("protocol_version"),
+                "tools": tools_info.get("tools", []),
+                "tool_count": len(tools_info.get("tools", []))
+            }
+            
+        except Exception as e:
+            return {
+                "server": server_name or server_url,
+                "server_url": server_url,
+                "error": str(e),
+                "tools": [],
+                "tool_count": 0
+            }
+    
+    async def discover_all_servers(self) -> Dict[str, Any]:
+        """Discover tools from all configured MCP servers.
+        
+        Returns:
+            Dictionary with discovery results for all servers
+        """
+        results = {}
+        
+        # Catalog server
+        catalog_url = f"http://localhost:{self.settings.catalog_mcp_port}"
+        results["catalog"] = await self.discover_tools(
+            catalog_url,
+            "catalog"
+        )
+        
+        # SQL Query server
+        sql_url = f"http://localhost:{self.settings.sql_mcp_port}"
+        results["sql_query"] = await self.discover_tools(
+            sql_url,
+            "sql_query"
+        )
+        
+        # Vector Search server
+        vector_url = f"http://localhost:{self.settings.vector_mcp_port}"
+        results["vector_search"] = await self.discover_tools(
+            vector_url,
+            "vector_search"
+        )
+        
+        return results
+    
+    def get_discovered_tools(self) -> Dict[str, Dict[str, Any]]:
+        """Get all discovered tools.
+        
+        Returns:
+            Dictionary mapping tool keys to tool information
+        """
+        return self._discovered_tools.copy()
+    
+    def get_tool_info(self, tool_key: str) -> Optional[Dict[str, Any]]:
+        """Get information for a specific tool.
+        
+        Args:
+            tool_key: Tool key in format "server::tool_name"
+            
+        Returns:
+            Tool information dictionary or None if not found
+        """
+        return self._discovered_tools.get(tool_key)
+    
+    async def close(self):
+        """Close the MCP client."""
+        await self.mcp_client.close()
+    
+    async def __aenter__(self):
+        """Async context manager entry."""
+        return self
+    
+    async def __aexit__(self, exc_type, exc_val, exc_tb):
+        """Async context manager exit."""
+        await self.close()
\ No newline at end of file
diff --git a/agent/tool_result_normalizer.py b/agent/tool_result_normalizer.py
new file mode 100644
index 0000000..a337e90
--- /dev/null
+++ b/agent/tool_result_normalizer.py
@@ -0,0 +1,50 @@
+"""Tool result normalization layer."""
+
+from typing import Any, Dict, Optional
+from datetime import datetime
+
+
+def normalize_result(
+    result: Any,
+    tool_name: str,
+    tool_version: str = "1.0.0",
+    request_id: Optional[str] = None
+) -> Dict[str, Any]:
+    """Normalize tool result to consistent format.
+    
+    Args:
+        result: Tool execution result (can be data or Exception)
+        tool_name: Name of the tool
+        tool_version: Version of the tool
+        request_id: Request ID for correlation
+        
+    Returns:
+        Normalized result dictionary
+    """
+    if isinstance(result, Exception):
+        return {
+            "status": "error",
+            "data": None,
+            "metadata": {
+                "tool_name": tool_name,
+                "tool_version": tool_version,
+                "request_id": request_id,
+                "timestamp": datetime.utcnow().isoformat()
+            },
+            "error": {
+                "type": type(result).__name__,
+                "message": str(result)
+            }
+        }
+    
+    return {
+        "status": "success",
+        "data": result,
+        "metadata": {
+            "tool_name": tool_name,
+            "tool_version": tool_version,
+            "request_id": request_id,
+            "timestamp": datetime.utcnow().isoformat()
+        },
+        "error": None
+    }
diff --git a/config/__init__.py b/config/__init__.py
new file mode 100644
index 0000000..6a8c544
--- /dev/null
+++ b/config/__init__.py
@@ -0,0 +1,5 @@
+"""Configuration module."""
+
+from config.settings import Settings, get_settings
+
+__all__ = ["Settings", "get_settings"]
diff --git a/config/settings.py b/config/settings.py
new file mode 100644
index 0000000..2211a17
--- /dev/null
+++ b/config/settings.py
@@ -0,0 +1,97 @@
+"""Application settings with multi-provider LLM support."""
+
+from typing import Optional, Any
+from pydantic_settings import BaseSettings
+from pydantic import field_validator
+from functools import lru_cache
+
+
+class Settings(BaseSettings):
+    """Application settings with environment variable support."""
+    
+    # LLM Provider Selection
+    llm_provider: str = "gemini"  # Options: gemini, openai, anthropic, ollama
+    
+    # Embedding Provider Selection (can be different from LLM provider)
+    embedding_provider: str = "gemini"  # Options: gemini, ollama
+    
+    # Gemini Configuration
+    gemini_api_key: Optional[str] = None
+    gemini_model: str = "gemini-2.5-pro"
+    
+    # OpenAI Configuration
+    openai_api_key: Optional[str] = None
+    openai_model: str = "gpt-4"
+    
+    # Anthropic Configuration
+    anthropic_api_key: Optional[str] = None
+    anthropic_model: str = "claude-3-5-sonnet-20241022"
+    
+    # Ollama Configuration
+    ollama_base_url: str = "http://localhost:11434"
+    ollama_chat_model: str = "llama3"
+    ollama_embedding_model: str = "nomic-embed-text"
+    
+    # LLM Parameters (provider-agnostic)
+    llm_temperature: float = 0.7
+    llm_max_tokens: int = 2000
+    llm_top_p: Optional[float] = None
+    
+    @field_validator('llm_top_p', mode='before')
+    @classmethod
+    def parse_optional_float(cls, v: Any) -> Optional[float]:
+        """Parse empty string as None for optional float fields."""
+        if v == "" or v is None:
+            return None
+        try:
+            return float(v) if v else None
+        except (ValueError, TypeError):
+            return None
+    
+    # Databases
+    database_path: str = "./data/sample_data.db"
+    vector_store_path: str = "./data/vector_store"
+    
+    # MLflow
+    mlflow_tracking_uri: str = "http://localhost:5000"
+    mlflow_experiment_name: str = "mcp_agent_experiments"
+    
+    # MCP Server ports
+    catalog_mcp_port: int = 7001
+    vector_mcp_port: int = 7002
+    sql_mcp_port: int = 7003
+    
+    # MCP Authentication
+    mcp_api_key: Optional[str] = None  # Shared MCP API key
+    
+    # API settings
+    api_port: int = 8000
+    api_key: Optional[str] = None
+    api_host: str = "0.0.0.0"
+    
+    # Concurrency
+    max_parallel_mcp_calls: int = 5
+    mcp_call_timeout: int = 30
+    
+    # Logging
+    log_level: str = "INFO"
+    log_file: Optional[str] = None
+    
+    # Inference Logging
+    inference_log_db_path: str = "./data/inference_logs.db"
+    
+    model_config = {
+        "env_file": ".env",
+        "env_file_encoding": "utf-8",
+        "case_sensitive": False
+    }
+
+
+@lru_cache()
+def get_settings() -> Settings:
+    """Get cached settings instance.
+    
+    Returns:
+        Settings instance
+    """
+    return Settings()
diff --git a/data/sample_data.db b/data/sample_data.db
new file mode 100644
index 0000000..665ede4
Binary files /dev/null and b/data/sample_data.db differ
diff --git a/docs/README.md b/docs/README.md
new file mode 100644
index 0000000..0e8e84c
--- /dev/null
+++ b/docs/README.md
@@ -0,0 +1,48 @@
+# Documentation Index
+
+## 📚 Project Documentation
+
+### Guides
+
+- **[Project Overview](guides/understanding.md)** - Complete architecture and design
+- **[Installation Guide](guides/installation_notes.md)** - Setup and installation instructions
+- **[Hybrid Embeddings Guide](guides/hybrid_embeddings.md)** - Using Ollama and Gemini embeddings
+- **[Phase 1 Quick Start](guides/phase1_quickstart.md)** - Quick start guide for MCP servers
+
+### Implementation
+
+- **[Implementation Status](implementation/implementation_status.md)** - Overall project status
+- **[Phase 1: MCP Servers](implementation/phase1_mcp_servers.md)** - Detailed Phase 1 implementation
+- **[Phase 1 Summary](implementation/phase1_summary.md)** - Phase 1 completion summary
+
+### Migration & Setup
+
+- **[Gemini Migration](migration/gemini_migration.md)** - Migration to google-genai package
+- **[Gemini Migration Complete](migration/gemini_migration_complete.md)** - Migration completion notes
+
+### Project Structure
+
+- **[Project Structure](STRUCTURE.md)** - Complete project structure documentation
+- **[Reorganization Summary](REORGANIZATION_SUMMARY.md)** - Project reorganization details
+
+## 🚀 Quick Links
+
+- [Main README](../../README.md) - Project overview
+- [Phase 1 Quick Start](guides/phase1_quickstart.md) - Get started with MCP servers
+- [Understanding the Project](guides/understanding.md) - Deep dive into architecture
+
+## 📝 Documentation Standards
+
+All documentation follows these conventions:
+- Markdown format (`.md`)
+- Organized by category (guides, implementation, migration)
+- Includes code examples where relevant
+- Links to related documentation
+
+## 🔄 Keeping Documentation Updated
+
+When adding new features:
+1. Update relevant guide in `guides/`
+2. Add implementation details in `implementation/`
+3. Update main README if needed
+4. Keep this index updated
diff --git a/docs/REORGANIZATION_SUMMARY.md b/docs/REORGANIZATION_SUMMARY.md
new file mode 100644
index 0000000..63e753d
--- /dev/null
+++ b/docs/REORGANIZATION_SUMMARY.md
@@ -0,0 +1,77 @@
+# Documentation Reorganization Summary
+
+## ✅ Reorganization Complete
+
+All documentation files have been organized into the `docs/` folder for better structure and maintainability.
+
+## 📁 New Structure
+
+```
+docs/
+├── README.md                          # Documentation index
+├── STRUCTURE.md                       # Structure overview
+├── REORGANIZATION_SUMMARY.md         # This file
+│
+├── guides/                            # User guides
+│   ├── understanding.md               # Project overview
+│   ├── hybrid_embeddings.md           # Hybrid embedding guide
+│   └── installation_notes.md          # Installation guide
+│
+├── implementation/                    # Implementation docs
+│   ├── implementation_status.md
+│   ├── hybrid_implementation_summary.md
+│   ├── hybrid_setup_success.md
+│   └── comprehensive_review_feedback.md
+│
+└── migration/                         # Migration guides
+    ├── gemini_migration.md
+    └── gemini_migration_complete.md
+```
+
+## 📦 Files Moved
+
+### From Root → `docs/migration/`
+- ✅ `llm/GEMINI_MIGRATION.md` → `docs/migration/gemini_migration.md`
+- ✅ `GEMINI_MIGRATION_COMPLETE.md` → `docs/migration/gemini_migration_complete.md`
+
+### From Root → `docs/implementation/`
+- ✅ `HYBRID_IMPLEMENTATION_SUMMARY.md` → `docs/implementation/hybrid_implementation_summary.md`
+- ✅ `HYBRID_SETUP_SUCCESS.md` → `docs/implementation/hybrid_setup_success.md`
+- ✅ `IMPLEMENTATION_STATUS.md` → `docs/implementation/implementation_status.md`
+- ✅ `COMPREHENSIVE_REVIEW_FEEDBACK.md` → `docs/implementation/comprehensive_review_feedback.md`
+
+### From Root/llm → `docs/guides/`
+- ✅ `llm/HYBRID_EMBEDDINGS.md` → `docs/guides/hybrid_embeddings.md`
+- ✅ `INSTALLATION_NOTES.md` → `docs/guides/installation_notes.md`
+- ✅ `understanding.md` → `docs/guides/understanding.md`
+
+## 🔗 Updated References
+
+- ✅ Updated `llm/README.md` to reference new documentation paths
+- ✅ Updated main `README.md` with documentation links
+- ✅ Created `docs/README.md` as documentation index
+
+## 📝 Files Kept at Root
+
+- `README.md` - Main project README (updated with docs links)
+- `llm/README.md` - LLM module-specific documentation
+
+## ✅ Benefits
+
+1. **Cleaner Root**: Project root is now cleaner and easier to navigate
+2. **Better Organization**: Documentation grouped by purpose (guides, implementation, migration)
+3. **Easy Navigation**: Clear structure with README files for guidance
+4. **Maintainability**: Easier to find and update documentation
+
+## 🎯 Quick Access
+
+- **Documentation Index**: [docs/README.md](README.md)
+- **Project Overview**: [docs/guides/understanding.md](guides/understanding.md)
+- **Installation**: [docs/guides/installation_notes.md](guides/installation_notes.md)
+- **Hybrid Embeddings**: [docs/guides/hybrid_embeddings.md](guides/hybrid_embeddings.md)
+
+---
+
+**Reorganization Date**: Today  
+**Status**: ✅ Complete  
+**Root Directory**: Clean and organized
diff --git a/docs/STRUCTURE.md b/docs/STRUCTURE.md
new file mode 100644
index 0000000..bd539c9
--- /dev/null
+++ b/docs/STRUCTURE.md
@@ -0,0 +1,57 @@
+# Documentation Structure
+
+## 📁 Organization
+
+All documentation has been organized into the `docs/` folder for better structure and maintainability.
+
+### Directory Layout
+
+```
+docs/
+├── README.md                          # Documentation index
+├── STRUCTURE.md                       # This file
+│
+├── guides/                            # User guides and tutorials
+│   ├── understanding.md               # Project overview and architecture
+│   ├── hybrid_embeddings.md           # Hybrid embedding approach guide
+│   └── installation_notes.md          # Installation and setup guide
+│
+├── implementation/                    # Implementation details
+│   ├── implementation_status.md       # Current implementation status
+│   ├── hybrid_implementation_summary.md  # Hybrid embedding implementation
+│   ├── hybrid_setup_success.md        # Setup verification
+│   └── comprehensive_review_feedback.md  # Code review report
+│
+└── migration/                         # Migration guides
+    ├── gemini_migration.md            # Gemini migration guide
+    └── gemini_migration_complete.md   # Migration completion status
+```
+
+## 📝 File Locations
+
+### Root Level (Keep)
+- `README.md` - Main project README
+- `llm/README.md` - LLM module documentation
+
+### Moved to `docs/`
+All understanding, migration, and implementation documentation files have been moved to `docs/` for better organization.
+
+## 🔍 Finding Documentation
+
+- **Project Overview**: `docs/guides/understanding.md`
+- **Setup Instructions**: `docs/guides/installation_notes.md`
+- **Hybrid Embeddings**: `docs/guides/hybrid_embeddings.md`
+- **Implementation Status**: `docs/implementation/implementation_status.md`
+- **Migration Guides**: `docs/migration/`
+
+## 📚 Quick Links
+
+- [Documentation Index](README.md)
+- [Project Understanding](guides/understanding.md)
+- [Installation Guide](guides/installation_notes.md)
+- [Hybrid Embeddings Guide](guides/hybrid_embeddings.md)
+
+---
+
+**Last Updated**: Today  
+**Status**: All documentation organized
diff --git a/docs/STRUCTURE_COMPLETE.md b/docs/STRUCTURE_COMPLETE.md
new file mode 100644
index 0000000..9022545
--- /dev/null
+++ b/docs/STRUCTURE_COMPLETE.md
@@ -0,0 +1,230 @@
+# Complete Project Structure Overview
+
+## 📁 Project Root Structure
+
+```
+multi_tool_orchestration/
+├── agent/                          # LangGraph agent components
+│   ├── __init__.py
+│   ├── mcp_client.py              # MCP HTTP client with concurrency
+│   ├── tool_binding.py            # Tool discovery system
+│   └── tool_result_normalizer.py  # Result normalization
+│
+├── api/                            # FastAPI deployment (to be implemented)
+│
+├── config/                         # Configuration management
+│   ├── __init__.py
+│   └── settings.py                 # Pydantic settings with multi-provider support
+│
+├── data/                           # Data storage
+│   ├── sample_data.db             # SQLite sample database
+│   └── vector_store/              # Vector store JSON files
+│
+├── docs/                           # Complete documentation
+│   ├── guides/                    # User guides
+│   │   ├── understanding.md
+│   │   ├── installation_notes.md
+│   │   ├── phase1_quickstart.md
+│   │   └── hybrid_embeddings.md
+│   ├── implementation/            # Implementation details
+│   │   ├── phase1_mcp_servers.md
+│   │   ├── phase1_summary.md
+│   │   ├── phase1_fixes.md
+│   │   └── phase1_complete.md
+│   ├── migration/                 # Migration guides
+│   │   ├── gemini_migration.md
+│   │   └── gemini_migration_complete.md
+│   └── README.md                  # Documentation index
+│
+├── error_handling/                 # Error handling (to be implemented)
+│
+├── examples/                       # Usage examples
+│   ├── llm_usage_example.py       # LLM abstraction example
+│   ├── test_mcp_servers.py        # MCP servers test suite
+│   ├── test_vector_search.py      # Vector search test
+│   └── hybrid_embedding_example.py
+│
+├── llm/                            # LLM provider abstraction
+│   ├── __init__.py
+│   ├── base.py                    # Abstract base class
+│   ├── factory.py                 # Provider factory
+│   ├── models.py                  # Common models
+│   ├── gemini_client.py           # Gemini implementation ✅
+│   ├── ollama_client.py           # Ollama implementation ✅
+│   ├── openai_client.py           # OpenAI placeholder
+│   ├── anthropic_client.py        # Anthropic placeholder
+│   └── README.md                  # LLM module docs
+│
+├── logging/                        # Inference logging (to be implemented)
+│
+├── mcp_servers/                    # MCP servers
+│   ├── __init__.py
+│   ├── base_server.py             # Base MCP server with HTTP + Auth + Versioning
+│   ├── catalog_server/            # Catalog MCP server ✅
+│   │   ├── __init__.py
+│   │   ├── server.py
+│   │   ├── tools.py
+│   │   └── database.py
+│   ├── sql_query_server/          # SQL Query MCP server ✅
+│   │   ├── __init__.py
+│   │   ├── server.py
+│   │   ├── tools.py
+│   │   └── query_engine.py
+│   └── vector_search_server/      # Vector Search MCP server ✅
+│       ├── __init__.py
+│       ├── server.py
+│       ├── tools.py
+│       └── vector_store.py
+│
+├── mlflow/                         # MLflow integration (to be implemented)
+│   └── data/
+│
+├── scripts/                        # Utility scripts
+│   ├── __init__.py
+│   ├── setup_data.py              # Sample data setup ✅
+│   └── start_servers.py           # Start all MCP servers ✅
+│
+├── tests/                          # Test suite
+│   ├── __init__.py
+│   ├── test_llm_abstraction.py    # LLM abstraction tests
+│   └── fixtures/
+│
+├── .env                            # Environment variables (gitignored)
+├── .gitignore                      # Git ignore rules
+├── requirements.txt                # Python dependencies
+├── README.md                       # Main project README
+└── PROJECT_STRUCTURE_OBSERVATION.md
+```
+
+## 📊 Component Status
+
+### ✅ Completed Components
+
+#### 1. LLM Abstraction Layer (`llm/`)
+- ✅ Abstract base class (`base.py`)
+- ✅ Factory pattern (`factory.py`)
+- ✅ Common models (`models.py`)
+- ✅ Gemini client (fully implemented)
+- ✅ Ollama client (fully implemented)
+- ✅ OpenAI client (placeholder)
+- ✅ Anthropic client (placeholder)
+
+#### 2. MCP Servers (`mcp_servers/`)
+- ✅ Base MCP server with HTTP + Auth + Versioning
+- ✅ Catalog server (3 tools)
+- ✅ SQL Query server (2 tools, read-only enforcement)
+- ✅ Vector Search server (3 tools, in-memory with Gemini embeddings)
+
+#### 3. Agent Components (`agent/`)
+- ✅ MCP client with concurrency control
+- ✅ Tool discovery system
+- ✅ Tool result normalizer
+
+#### 4. Configuration (`config/`)
+- ✅ Multi-provider settings
+- ✅ Environment variable support
+- ✅ Pydantic validation
+
+#### 5. Scripts (`scripts/`)
+- ✅ Sample data setup
+- ✅ Server startup script
+
+#### 6. Examples (`examples/`)
+- ✅ LLM usage example
+- ✅ MCP servers test suite
+- ✅ Vector search test
+
+### ⏳ To Be Implemented
+
+- `api/` - FastAPI deployment
+- `agent/graph.py` - LangGraph agent graph
+- `agent/state.py` - Agent state schema
+- `agent/orchestrator.py` - Tool orchestration
+- `agent/prompts/` - Versioned prompts
+- `error_handling/` - Error handling strategies
+- `logging/` - Inference logging
+- `mlflow/` - MLflow integration
+
+## 📈 Statistics
+
+- **Total Python Files:** 37
+- **Total Documentation Files:** 22
+- **MCP Servers:** 3 (all implemented)
+- **Total Tools:** 8
+  - Catalog: 3 tools
+  - SQL Query: 2 tools
+  - Vector Search: 3 tools
+- **LLM Providers:** 4 (2 fully implemented, 2 placeholders)
+
+## 🎯 Architecture Highlights
+
+### Client-Server Architecture ✅
+- HTTP-based MCP servers
+- JSON-RPC 2.0 protocol
+- Authentication via `X-MCP-KEY` header
+- Request ID propagation
+
+### Model Abstraction ✅
+- Provider-agnostic interface
+- Factory pattern for provider creation
+- Easy switching via configuration
+- Support for multiple providers
+
+### Versioning ✅
+- Server versioning
+- Protocol versioning
+- Tool versioning
+- Prompt versioning (structure ready)
+
+### Observability ✅
+- Request ID propagation
+- Tool result normalization
+- Health check endpoints
+- Tool discovery system
+
+## 🔧 Key Features
+
+1. **Multi-Provider LLM Support**
+   - Gemini ✅
+   - Ollama ✅
+   - OpenAI (placeholder)
+   - Anthropic (placeholder)
+
+2. **Hybrid Embeddings**
+   - Use different providers for chat vs embeddings
+   - Cost-effective local embeddings with Ollama
+
+3. **MCP Server Infrastructure**
+   - HTTP transport
+   - Authentication
+   - Versioning
+   - JSON-RPC 2.0
+
+4. **Read-Only SQL Enforcement**
+   - Blocks INSERT, UPDATE, DELETE, etc.
+   - Only SELECT queries allowed
+
+5. **Vector Search**
+   - In-memory storage with JSON persistence
+   - Gemini embeddings
+   - Cosine similarity search
+
+## 📝 Documentation Structure
+
+```
+docs/
+├── guides/              # User-facing guides
+├── implementation/      # Technical implementation details
+├── migration/           # Migration guides
+└── README.md           # Documentation index
+```
+
+## 🚀 Next Steps
+
+1. **Phase 2:** LangGraph Agent Development
+2. **Phase 3:** FastAPI Deployment
+3. **Phase 4:** Testing & Documentation
+
+## ✅ Phase 1 Status: COMPLETE
+
+All Phase 1 components have been implemented and tested successfully.
diff --git a/docs/guides/hybrid_embeddings.md b/docs/guides/hybrid_embeddings.md
new file mode 100644
index 0000000..e451190
--- /dev/null
+++ b/docs/guides/hybrid_embeddings.md
@@ -0,0 +1,221 @@
+# Hybrid Embedding Approach
+
+This project supports a **hybrid embedding approach** that allows you to use either **Ollama** (local) or **Gemini** (cloud) for embeddings, independently of your LLM provider choice.
+
+## Why Hybrid Embeddings?
+
+### Benefits:
+- **Cost Optimization**: Use Ollama for high-volume, Gemini for quality-critical
+- **Privacy Control**: Ollama keeps data local, Gemini for non-sensitive data
+- **Flexibility**: Switch providers via configuration, no code changes
+- **Fallback**: Use Gemini if Ollama is unavailable
+- **Quality Testing**: Compare embeddings side-by-side
+
+## Configuration
+
+### Environment Variables
+
+Add to your `.env` file:
+
+```bash
+# Embedding Provider Selection
+EMBEDDING_PROVIDER=ollama  # or "gemini"
+
+# Ollama Configuration (if using Ollama)
+OLLAMA_BASE_URL=http://localhost:11434
+OLLAMA_CHAT_MODEL=llama3
+OLLAMA_EMBEDDING_MODEL=nomic-embed-text
+```
+
+### Settings
+
+The embedding provider is **independent** of your LLM provider:
+
+```python
+# You can use Gemini for chat, Ollama for embeddings
+LLM_PROVIDER=gemini
+EMBEDDING_PROVIDER=ollama
+
+# Or both from the same provider
+LLM_PROVIDER=gemini
+EMBEDDING_PROVIDER=gemini
+
+# Or both from Ollama
+LLM_PROVIDER=ollama
+EMBEDDING_PROVIDER=ollama
+```
+
+## Usage
+
+### Basic Usage
+
+```python
+from config.settings import get_settings
+from llm.factory import LLMFactory
+from llm.models import EmbeddingRequest
+
+settings = get_settings()
+
+# Create embedding provider (independent of LLM provider)
+embedding_provider = LLMFactory.create_embedding_provider(settings)
+
+# Get embeddings
+request = EmbeddingRequest(
+    texts=["Machine learning is AI", "Deep learning uses neural networks"]
+)
+
+response = await embedding_provider.get_embeddings(request)
+print(f"Embeddings: {response.embeddings}")
+print(f"Provider: {response.provider}")
+print(f"Model: {response.model}")
+```
+
+### Switching Providers
+
+Simply change `EMBEDDING_PROVIDER` in `.env`:
+
+```bash
+# Use Ollama (local, free)
+EMBEDDING_PROVIDER=ollama
+
+# Use Gemini (cloud, paid)
+EMBEDDING_PROVIDER=gemini
+```
+
+No code changes needed!
+
+## Setup Instructions
+
+### Ollama Setup
+
+1. **Install Ollama**: https://ollama.ai
+
+2. **Pull embedding model**:
+   ```bash
+   ollama pull nomic-embed-text
+   ```
+
+3. **Verify Ollama is running**:
+   ```bash
+   curl http://localhost:11434/api/tags
+   ```
+
+4. **Configure in .env**:
+   ```bash
+   EMBEDDING_PROVIDER=ollama
+   OLLAMA_BASE_URL=http://localhost:11434
+   OLLAMA_EMBEDDING_MODEL=nomic-embed-text
+   ```
+
+### Gemini Setup
+
+1. **Get API key**: https://makersuite.google.com/app/apikey
+
+2. **Configure in .env**:
+   ```bash
+   EMBEDDING_PROVIDER=gemini
+   GEMINI_API_KEY=your_key_here
+   ```
+
+## Available Embedding Models
+
+### Ollama Models
+- `nomic-embed-text` (recommended) - 768 dimensions
+- `all-minilm` - 384 dimensions
+- `mxbai-embed-large` - 1024 dimensions
+
+Pull models with: `ollama pull <model-name>`
+
+### Gemini Models
+- `models/embedding-001` (default) - 768 dimensions
+
+## Comparison
+
+| Feature | Ollama | Gemini |
+|---------|--------|--------|
+| **Cost** | Free (local) | Pay per request |
+| **Privacy** | 100% local | Data sent to Google |
+| **Latency** | Low (local) | Network dependent |
+| **Quality** | Good | Excellent |
+| **Setup** | Requires installation | API key only |
+| **Scalability** | Limited by hardware | Unlimited |
+| **Quota** | None | Rate limits |
+
+## Example: Hybrid Setup
+
+```bash
+# .env configuration
+LLM_PROVIDER=gemini          # Use Gemini for chat (high quality)
+EMBEDDING_PROVIDER=ollama    # Use Ollama for embeddings (cost-effective)
+
+# This allows you to:
+# - Get high-quality chat responses from Gemini
+# - Generate embeddings locally with Ollama (no API costs)
+# - Process large volumes of embeddings without quota limits
+```
+
+## Testing
+
+Run the hybrid embedding example:
+
+```bash
+python examples/hybrid_embedding_example.py
+```
+
+This will:
+1. Test Ollama embeddings (if available)
+2. Test Gemini embeddings (if API key is set)
+3. Compare both providers
+4. Show configuration options
+
+## Troubleshooting
+
+### Ollama Not Found
+- Ensure Ollama is installed and running
+- Check `OLLAMA_BASE_URL` is correct
+- Verify model is pulled: `ollama list`
+
+### Gemini Quota Errors
+- Check API key is valid
+- Wait for quota reset
+- Consider switching to Ollama for high-volume
+
+### Embedding Dimension Mismatch
+- Different models have different dimensions
+- Ensure you use the same model for indexing and querying
+- Ollama: 768 (nomic-embed-text) or 384 (all-minilm)
+- Gemini: 768 (embedding-001)
+
+## Best Practices
+
+1. **Development**: Use Ollama (fast, free, no API keys)
+2. **Production**: Choose based on:
+   - **High volume**: Ollama
+   - **Quality critical**: Gemini
+   - **Privacy sensitive**: Ollama
+3. **Testing**: Test both and compare quality
+4. **Fallback**: Implement retry logic with provider fallback
+
+## Architecture
+
+```
+┌─────────────────┐
+│  Application    │
+└────────┬────────┘
+         │
+         ▼
+┌─────────────────┐
+│  LLM Factory    │
+│  (Embedding)    │
+└────────┬────────┘
+         │
+    ┌────┴────┐
+    │         │
+    ▼         ▼
+┌────────┐ ┌────────┐
+│ Ollama │ │ Gemini │
+│ (Local)│ │ (Cloud)│
+└────────┘ └────────┘
+```
+
+The factory pattern allows seamless switching between providers without code changes.
diff --git a/docs/guides/installation_notes.md b/docs/guides/installation_notes.md
new file mode 100644
index 0000000..61db4de
--- /dev/null
+++ b/docs/guides/installation_notes.md
@@ -0,0 +1,72 @@
+# Installation Notes
+
+## ✅ Successfully Installed
+
+The following core dependencies have been installed successfully:
+
+- ✅ **FastAPI** - Web framework
+- ✅ **Uvicorn** - ASGI server
+- ✅ **Pydantic & Pydantic Settings** - Data validation
+- ✅ **Google Generative AI** - Gemini API client
+- ✅ **Python Dotenv** - Environment variable management
+- ✅ **HTTP Clients** - httpx, aiohttp
+- ✅ **Testing** - pytest, pytest-asyncio, pytest-cov
+- ✅ **Utilities** - slowapi, structlog, aiosqlite, jsonrpcclient
+
+## ⚠️ Optional Dependencies (Install Later)
+
+The following packages are commented out in `requirements.txt` because they require compilation or have compatibility issues with Python 3.14:
+
+### 1. **LangChain & LangGraph** (Optional for now)
+```bash
+# Install when needed for agent implementation
+pip install langchain langchain-google-genai langgraph
+```
+
+### 2. **MLflow** (Optional for now)
+```bash
+# Install when needed for evaluation
+pip install mlflow
+```
+
+### 3. **ChromaDB** (Optional - requires C++ compiler)
+```bash
+# ChromaDB requires compilation on Python 3.14
+# Options:
+# 1. Wait for pre-built wheels for Python 3.14
+# 2. Use Python 3.11 or 3.12 (has pre-built wheels)
+# 3. Install Visual Studio Build Tools for Windows
+pip install chromadb
+```
+
+## 🔧 Python 3.14 Compatibility
+
+**Issue**: Python 3.14 is very new and some packages don't have pre-built wheels yet, requiring compilation from source.
+
+**Solutions**:
+1. **Use Python 3.11 or 3.12** (recommended) - Most packages have pre-built wheels
+2. **Install build tools** - Visual Studio Build Tools for Windows (for packages that need compilation)
+3. **Wait for wheels** - Some packages will release Python 3.14 wheels soon
+
+## 📝 Current Status
+
+✅ **Model Abstraction Layer** - Fully functional  
+✅ **Core Dependencies** - Installed  
+⏳ **LangChain/LangGraph** - Install when implementing agent  
+⏳ **MLflow** - Install when implementing evaluation  
+⏳ **ChromaDB** - Install when implementing vector search  
+
+## 🚀 Next Steps
+
+1. **Test the LLM abstraction**:
+   ```bash
+   python examples/llm_usage_example.py
+   ```
+
+2. **Continue with MCP Server implementation** (doesn't require LangChain yet)
+
+3. **Install LangChain/LangGraph** when ready to implement the agent (Phase 2)
+
+## 📌 Note
+
+The project structure is complete and the model abstraction layer is ready to use. You can start implementing MCP servers and other components that don't require LangChain/LangGraph yet.
diff --git a/docs/guides/phase1_quickstart.md b/docs/guides/phase1_quickstart.md
new file mode 100644
index 0000000..f4b595b
--- /dev/null
+++ b/docs/guides/phase1_quickstart.md
@@ -0,0 +1,163 @@
+# Phase 1: MCP Servers Quick Start Guide
+
+## Prerequisites
+
+1. ✅ Python 3.11+ (or 3.14 with some limitations)
+2. ✅ Dependencies installed: `pip install -r requirements.txt`
+3. ✅ `.env` file configured with API keys
+
+## Step 1: Setup Sample Data
+
+```bash
+python scripts/setup_data.py
+```
+
+This creates a sample SQLite database with:
+- `users` table (4 sample users)
+- `products` table (5 sample products)
+- `orders` table (5 sample orders)
+
+## Step 2: Start MCP Servers
+
+### Option A: Start Individual Servers
+
+**Terminal 1 - Catalog Server:**
+```bash
+python -m mcp_servers.catalog_server.server
+```
+
+**Terminal 2 - SQL Query Server:**
+```bash
+python -m mcp_servers.sql_query_server.server
+```
+
+**Terminal 3 - Vector Search Server:**
+```bash
+python -m mcp_servers.vector_search_server.server
+```
+
+### Option B: Start All Servers (Basic)
+
+```bash
+python scripts/start_servers.py
+```
+
+## Step 3: Verify Servers are Running
+
+### Test Health Endpoints
+
+```bash
+# Catalog server
+curl http://localhost:7001/health
+
+# SQL Query server
+curl http://localhost:7003/health
+
+# Vector Search server
+curl http://localhost:7002/health
+```
+
+Expected response:
+```json
+{
+  "status": "healthy",
+  "server_name": "Catalog MCP Server",
+  "server_version": "1.0.0",
+  "protocol_version": "2024-11-05"
+}
+```
+
+### Test Tool Listing
+
+```bash
+curl http://localhost:7001/tools
+```
+
+## Step 4: Run Test Suite
+
+```bash
+python examples/test_mcp_servers.py
+```
+
+This will test:
+- ✅ Server health checks (all 3 servers)
+- ✅ Tool discovery (all 8 tools)
+- ✅ Tool execution
+- ✅ Read-only enforcement
+- ✅ Vector search functionality
+
+You can also test vector search specifically:
+```bash
+python examples/test_vector_search.py
+```
+
+## Step 5: Use MCP Client in Your Code
+
+```python
+import asyncio
+from agent.mcp_client import MCPClient
+from agent.tool_result_normalizer import normalize_result
+
+async def main():
+    async with MCPClient() as client:
+        # List tables
+        result = await client.call_tool(
+            server_url="http://localhost:7001",
+            tool_name="list_tables",
+            params={}
+        )
+        
+        normalized = normalize_result(result, "list_tables")
+        print(normalized)
+
+asyncio.run(main())
+```
+
+## Common Issues
+
+### Server Won't Start
+
+**Issue:** Port already in use
+**Solution:** 
+- Change port in `.env` file
+- Or stop the process using the port
+
+### Authentication Errors
+
+**Issue:** `401 Unauthorized`
+**Solution:**
+- Set `MCP_API_KEY` in `.env` file
+- Or remove `MCP_API_KEY` to disable authentication
+
+### Database Not Found
+
+**Issue:** `Database file not found`
+**Solution:**
+- Run `python scripts/setup_data.py` first
+- Check `DATABASE_PATH` in `.env`
+
+## Next Steps
+
+- ✅ Phase 1 Complete: MCP Servers ready
+- ⏳ Phase 2: Implement LangGraph Agent
+- ⏳ Phase 3: FastAPI Deployment
+- ⏳ Phase 4: Testing & Documentation
+
+## Architecture
+
+```
+┌─────────────────┐
+│   MCP Client    │
+│  (agent/)       │
+└────────┬────────┘
+         │ HTTP + JSON-RPC 2.0
+         │
+    ┌────┴────┬────┐
+    │         │    │
+┌───▼───┐ ┌──▼────┐ ┌──▼────────┐
+│Catalog│ │  SQL  │ │  Vector   │
+│Server │ │Server │ │  Search   │
+└───────┘ └───────┘ └───────────┘
+```
+
+See [Phase 1 Implementation Details](../implementation/phase1_mcp_servers.md) for more information.
diff --git a/understanding.md b/docs/guides/understanding.md
similarity index 100%
rename from understanding.md
rename to docs/guides/understanding.md
diff --git a/docs/guides/understanding_400_errors.md b/docs/guides/understanding_400_errors.md
new file mode 100644
index 0000000..3ea0215
--- /dev/null
+++ b/docs/guides/understanding_400_errors.md
@@ -0,0 +1,109 @@
+# Understanding 400 Bad Request in MCP Servers
+
+## Is 400 Bad Request an Error?
+
+**Short Answer:** Not always! In MCP servers, 400 Bad Request is used for **validation errors**, which are expected and correct behavior.
+
+## When You See 400 Bad Request
+
+### ✅ Expected 400 Responses (Working Correctly)
+
+1. **Read-Only Enforcement**
+   - When trying to execute INSERT/UPDATE/DELETE queries on SQL Query server
+   - Example: `INSERT INTO users ...` → 400 Bad Request ✅
+   - This is **correct behavior** - the server is protecting the database
+
+2. **Invalid Parameters**
+   - Missing required parameters
+   - Invalid parameter types
+   - Parameter validation failures
+
+3. **Business Logic Validation**
+   - Query validation (e.g., SQL syntax errors)
+   - Permission checks
+   - Resource constraints
+
+### ❌ Unexpected 400 Responses (Actual Errors)
+
+1. **Malformed JSON-RPC Request**
+   - Invalid JSON structure
+   - Missing required JSON-RPC fields
+
+2. **Authentication Failures**
+   - These should return 401 Unauthorized, not 400
+
+## How to Distinguish
+
+### In Server Logs
+
+**Validation Error (Expected):**
+```
+INFO: Validation error for method 'execute_query': Read-only mode: INSERT operations are not allowed
+INFO: 127.0.0.1:56566 - "POST /execute HTTP/1.1" 400 Bad Request
+```
+
+**Internal Error (Unexpected):**
+```
+ERROR: Internal error executing method 'execute_query': Database connection failed
+INFO: 127.0.0.1:56566 - "POST /execute HTTP/1.1" 500 Internal Server Error
+```
+
+### In JSON-RPC Response
+
+**Validation Error (400):**
+```json
+{
+  "jsonrpc": "2.0",
+  "id": "123",
+  "error": {
+    "code": -32602,
+    "message": "Invalid params",
+    "data": "Read-only mode: INSERT operations are not allowed"
+  }
+}
+```
+
+**Internal Error (500):**
+```json
+{
+  "jsonrpc": "2.0",
+  "id": "123",
+  "error": {
+    "code": -32603,
+    "message": "Internal Error",
+    "data": "Database connection failed"
+  }
+}
+```
+
+## JSON-RPC Error Codes
+
+| Code | Meaning | HTTP Status | Example |
+|------|---------|-------------|---------|
+| -32600 | Invalid Request | 400 | Malformed JSON-RPC |
+| -32601 | Method not found | 404 | Unknown tool name |
+| -32602 | Invalid params | 400 | Read-only violation |
+| -32603 | Internal error | 500 | Database error |
+
+## In Your Test Output
+
+When you see:
+```
+INFO: 127.0.0.1:56566 - "POST /execute HTTP/1.1" 400 Bad Request
+```
+
+And the test shows:
+```
+[OK] Read-only enforcement working
+[OK] Error message: Invalid params: Read-only mode: INSERT operations are not allowed...
+```
+
+**This means:** ✅ Everything is working correctly! The 400 is the expected response.
+
+## Summary
+
+- **400 Bad Request** for validation errors = ✅ Correct behavior
+- **400 Bad Request** for malformed requests = ⚠️ Client issue
+- **500 Internal Server Error** = ❌ Server problem
+
+The read-only enforcement test intentionally triggers a 400 response to verify the protection is working. This is **not a bug** - it's a **feature**! 🎯
diff --git a/docs/implementation/comprehensive_review_feedback.md b/docs/implementation/comprehensive_review_feedback.md
new file mode 100644
index 0000000..a584dff
--- /dev/null
+++ b/docs/implementation/comprehensive_review_feedback.md
@@ -0,0 +1,377 @@
+# Comprehensive Code Review & Feedback Report
+
+## ✅ Overall Status: **EXCELLENT - All Systems Working**
+
+---
+
+## 📋 File-by-File Review
+
+### ✅ Core LLM Abstraction Layer
+
+#### `llm/base.py` - **PERFECT**
+- ✅ Clean abstract interface
+- ✅ All required abstract methods defined
+- ✅ Proper type hints
+- ✅ Good documentation
+- **Status**: Production-ready
+
+#### `llm/models.py` - **PERFECT**
+- ✅ Well-structured Pydantic models
+- ✅ Proper validation
+- ✅ Optional fields handled correctly
+- **Status**: Production-ready
+
+#### `llm/factory.py` - **EXCELLENT**
+- ✅ Factory pattern correctly implemented
+- ✅ Supports all providers: gemini, openai, anthropic, ollama
+- ✅ Separate `create_embedding_provider()` method
+- ✅ Proper error handling
+- ✅ Good validation
+- **Status**: Production-ready
+
+---
+
+### ✅ LLM Provider Implementations
+
+#### `llm/gemini_client.py` - **FIXED & WORKING**
+- ✅ Migrated to `google-genai` package (no deprecation warnings)
+- ✅ Chat completion: **WORKING** ✅
+- ✅ Embeddings: **WORKING** ✅
+- ✅ System instruction handling: Fixed (prepended to first user message)
+- ✅ Proper error handling
+- ✅ Usage metadata extraction
+- **Status**: Production-ready
+
+**Recent Fixes:**
+- Fixed `system_instruction` parameter issue
+- Fixed embedding response extraction
+- Updated to use `contents` (plural) for embeddings
+
+#### `llm/ollama_client.py` - **EXCELLENT**
+- ✅ Full implementation
+- ✅ Chat completion: **WORKING** ✅
+- ✅ Embeddings: **WORKING** ✅
+- ✅ Proper async HTTP client
+- ✅ Error handling
+- ✅ Context manager support
+- **Status**: Production-ready
+
+#### `llm/openai_client.py` - **PLACEHOLDER**
+- ⚠️ Placeholder implementation
+- ✅ Interface correctly defined
+- ⏳ Implementation pending
+- **Status**: Ready for implementation when needed
+
+#### `llm/anthropic_client.py` - **PLACEHOLDER**
+- ⚠️ Placeholder implementation
+- ✅ Interface correctly defined
+- ⏳ Implementation pending
+- **Status**: Ready for implementation when needed
+
+---
+
+### ✅ Configuration
+
+#### `config/settings.py` - **EXCELLENT**
+- ✅ Multi-provider support
+- ✅ Independent embedding provider selection
+- ✅ All environment variables properly defined
+- ✅ Pydantic v2 compliant (fixed deprecation warning)
+- ✅ Proper validation for optional fields
+- **Status**: Production-ready
+
+**Recent Fixes:**
+- Fixed Pydantic Config deprecation (migrated to `model_config`)
+
+#### `config/__init__.py` - **GOOD**
+- ✅ Clean exports
+- **Status**: Good
+
+---
+
+### ✅ Examples
+
+#### `examples/llm_usage_example.py` - **WORKING**
+- ✅ Demonstrates chat completion
+- ✅ Demonstrates embeddings
+- ✅ Proper error handling
+- ✅ Good user feedback
+- **Status**: Working perfectly ✅
+
+#### `examples/hybrid_embedding_example.py` - **WORKING**
+- ✅ Demonstrates hybrid approach
+- ✅ Tests both Ollama and Gemini
+- ✅ Fixed Unicode encoding issues
+- ✅ Clear output
+- **Status**: Working perfectly ✅
+
+---
+
+### ✅ Tests
+
+#### `tests/test_llm_abstraction.py` - **PASSING**
+- ✅ All 4 tests passing
+- ✅ Tests factory pattern
+- ✅ Tests error handling
+- ✅ Tests provider interface
+- ✅ Updated to include Ollama
+- **Status**: All tests passing ✅
+
+**Test Results:**
+```
+✅ test_llm_factory_available_providers - PASSED
+✅ test_llm_factory_unsupported_provider - PASSED
+✅ test_llm_factory_missing_api_key - PASSED
+✅ test_llm_provider_interface - PASSED
+```
+
+---
+
+### ✅ Documentation
+
+#### `llm/README.md` - **EXCELLENT**
+- ✅ Comprehensive usage guide
+- ✅ Hybrid embedding documentation
+- ✅ Examples provided
+- **Status**: Excellent
+
+#### `llm/HYBRID_EMBEDDINGS.md` - **EXCELLENT**
+- ✅ Complete hybrid approach guide
+- ✅ Setup instructions
+- ✅ Comparison table
+- ✅ Troubleshooting
+- **Status**: Excellent
+
+#### `llm/GEMINI_MIGRATION.md` - **GOOD**
+- ✅ Migration guide
+- ✅ Status documented
+- **Status**: Good
+
+---
+
+## 🧪 Test Results Summary
+
+### ✅ Chat Completion Tests
+```
+✅ Gemini Chat: WORKING
+   - Model: gemini-2.5-flash-lite
+   - System instructions: Handled correctly
+   - Usage tracking: Working
+```
+
+### ✅ Embedding Tests
+```
+✅ Ollama Embeddings: WORKING
+   - Model: nomic-embed-text
+   - Dimension: 768
+   - Performance: Fast, local
+
+✅ Gemini Embeddings: WORKING
+   - Model: text-embedding-004
+   - Dimension: 768
+   - Performance: Cloud-based
+```
+
+### ✅ Unit Tests
+```
+✅ All 4 tests passing
+✅ No critical errors
+⚠️ 1 deprecation warning (from google-genai package, not our code)
+```
+
+---
+
+## 🎯 Key Features Status
+
+### ✅ Model Abstraction
+- **Status**: ✅ **COMPLETE & WORKING**
+- All providers implement same interface
+- Easy to add new providers
+- Factory pattern working perfectly
+
+### ✅ Hybrid Embeddings
+- **Status**: ✅ **COMPLETE & WORKING**
+- Ollama: ✅ Working
+- Gemini: ✅ Working
+- Independent provider selection: ✅ Working
+- Configuration-based switching: ✅ Working
+
+### ✅ Client-Server Architecture
+- **Status**: ✅ **DESIGNED & READY**
+- Abstraction layer supports it
+- Ready for MCP server implementation
+
+### ✅ Multi-Provider Support
+- **Status**: ✅ **COMPLETE**
+- Gemini: ✅ Fully implemented
+- Ollama: ✅ Fully implemented
+- OpenAI: ⏳ Placeholder ready
+- Anthropic: ⏳ Placeholder ready
+
+---
+
+## 🔧 Issues Fixed
+
+### 1. ✅ Gemini Chat Completion
+- **Issue**: `system_instruction` parameter not supported
+- **Fix**: Prepended system instruction to first user message
+- **Status**: ✅ Fixed & Working
+
+### 2. ✅ Gemini Embeddings
+- **Issue**: Wrong parameter name (`content` vs `contents`)
+- **Fix**: Changed to `contents` (plural)
+- **Status**: ✅ Fixed & Working
+
+### 3. ✅ Embedding Response Extraction
+- **Issue**: Incorrect response structure handling
+- **Fix**: Proper extraction from `EmbedContentResponse.embeddings[0].values`
+- **Status**: ✅ Fixed & Working
+
+### 4. ✅ Pydantic Config Deprecation
+- **Issue**: Using deprecated `Config` class
+- **Fix**: Migrated to `model_config` dict
+- **Status**: ✅ Fixed
+
+### 5. ✅ Unicode Encoding
+- **Issue**: Emoji characters causing Windows terminal errors
+- **Fix**: Replaced with ASCII alternatives
+- **Status**: ✅ Fixed
+
+### 6. ✅ Package Migration
+- **Issue**: Deprecated `google-generativeai` package
+- **Fix**: Migrated to `google-genai`
+- **Status**: ✅ Complete
+
+---
+
+## 📊 Code Quality Metrics
+
+### ✅ Code Organization
+- **Score**: 10/10
+- Clean separation of concerns
+- Proper abstraction layers
+- Well-structured modules
+
+### ✅ Error Handling
+- **Score**: 9/10
+- Comprehensive try-catch blocks
+- Clear error messages
+- Proper exception propagation
+
+### ✅ Documentation
+- **Score**: 9/10
+- Good docstrings
+- Comprehensive README files
+- Usage examples provided
+
+### ✅ Testing
+- **Score**: 8/10
+- Unit tests passing
+- Could add more integration tests
+- Good coverage of core functionality
+
+### ✅ Type Safety
+- **Score**: 10/10
+- Full type hints
+- Pydantic models for validation
+- Proper typing throughout
+
+---
+
+## 🚀 Performance Status
+
+### ✅ Response Times
+- Gemini Chat: Fast (cloud-based)
+- Gemini Embeddings: Fast (cloud-based)
+- Ollama Chat: Fast (local)
+- Ollama Embeddings: Very fast (local)
+
+### ✅ Resource Usage
+- Memory: Efficient
+- CPU: Normal
+- Network: Only for cloud providers
+
+---
+
+## ⚠️ Minor Issues & Recommendations
+
+### 1. Deprecation Warning (Non-Critical)
+- **Issue**: Warning from `google-genai` package (Python 3.17 deprecation)
+- **Impact**: None - external package issue
+- **Action**: None needed (will be fixed by package maintainers)
+
+### 2. OpenAI/Anthropic Placeholders
+- **Status**: Expected - not yet implemented
+- **Recommendation**: Implement when needed
+- **Priority**: Low (Gemini and Ollama are working)
+
+### 3. Additional Tests
+- **Recommendation**: Add integration tests for full workflows
+- **Priority**: Medium
+- **Status**: Current tests are sufficient for now
+
+---
+
+## ✅ Final Verdict
+
+### Overall Assessment: **EXCELLENT** ⭐⭐⭐⭐⭐
+
+**Strengths:**
+1. ✅ Clean architecture with proper abstraction
+2. ✅ Hybrid embedding approach working perfectly
+3. ✅ All core functionality tested and working
+4. ✅ Good error handling and validation
+5. ✅ Comprehensive documentation
+6. ✅ Easy to extend and maintain
+
+**Working Features:**
+- ✅ Gemini chat completion
+- ✅ Gemini embeddings
+- ✅ Ollama chat completion
+- ✅ Ollama embeddings
+- ✅ Hybrid embedding selection
+- ✅ Provider factory pattern
+- ✅ Configuration management
+- ✅ All unit tests passing
+
+**Ready for:**
+- ✅ Production use (core LLM functionality)
+- ✅ MCP server development
+- ✅ Agent implementation
+- ✅ Further extension
+
+---
+
+## 📝 Recommendations
+
+### Immediate (Optional)
+1. ✅ **DONE**: All critical issues fixed
+2. Consider adding more integration tests
+3. Monitor for google-genai package updates
+
+### Future Enhancements
+1. Implement OpenAI client when needed
+2. Implement Anthropic client when needed
+3. Add streaming support
+4. Add retry logic with exponential backoff
+5. Add caching layer for embeddings
+
+---
+
+## 🎉 Summary
+
+**Everything is working perfectly!** 
+
+- ✅ All LLM providers functional
+- ✅ Hybrid embeddings working
+- ✅ All tests passing
+- ✅ No critical issues
+- ✅ Production-ready code
+
+The codebase is **well-structured**, **properly tested**, and **ready for the next phase** of development (MCP servers and agent implementation).
+
+---
+
+**Review Date**: Today  
+**Status**: ✅ **ALL SYSTEMS OPERATIONAL**  
+**Next Steps**: Proceed with MCP server development
diff --git a/docs/implementation/hybrid_implementation_summary.md b/docs/implementation/hybrid_implementation_summary.md
new file mode 100644
index 0000000..fed227c
--- /dev/null
+++ b/docs/implementation/hybrid_implementation_summary.md
@@ -0,0 +1,171 @@
+# Hybrid Embedding Implementation Summary
+
+## ✅ Implementation Complete
+
+Successfully implemented a **hybrid embedding approach** that supports both Ollama (local) and Gemini (cloud) embeddings, independently of the LLM provider choice.
+
+## What Was Implemented
+
+### 1. Ollama Client (`llm/ollama_client.py`)
+- ✅ Full `LLMProvider` interface implementation
+- ✅ Chat completion support
+- ✅ Embedding generation support
+- ✅ Async HTTP client with proper error handling
+- ✅ Configurable base URL and models
+
+### 2. Settings Updates (`config/settings.py`)
+- ✅ Added `embedding_provider` setting (independent of `llm_provider`)
+- ✅ Added Ollama configuration:
+  - `ollama_base_url` (default: http://localhost:11434)
+  - `ollama_chat_model` (default: llama3)
+  - `ollama_embedding_model` (default: nomic-embed-text)
+
+### 3. Factory Updates (`llm/factory.py`)
+- ✅ Added `create_embedding_provider()` method
+- ✅ Support for Ollama provider creation
+- ✅ Updated `get_available_providers()` to include Ollama
+- ✅ Independent provider selection for embeddings
+
+### 4. Documentation
+- ✅ `llm/HYBRID_EMBEDDINGS.md` - Complete guide
+- ✅ `examples/hybrid_embedding_example.py` - Working example
+- ✅ Updated `llm/README.md` with hybrid approach info
+
+## Key Features
+
+### 1. Independent Provider Selection
+```python
+# Chat from Gemini, embeddings from Ollama
+LLM_PROVIDER=gemini
+EMBEDDING_PROVIDER=ollama
+```
+
+### 2. Zero Code Changes
+Switch providers via environment variables - no code modifications needed.
+
+### 3. Seamless Integration
+Works with existing abstraction layer - all providers implement same interface.
+
+### 4. Cost Optimization
+- Use Ollama for high-volume embeddings (free, local)
+- Use Gemini for quality-critical embeddings (cloud, paid)
+
+## Architecture
+
+```
+Application
+    │
+    ├─ LLM Provider (Chat)
+    │   ├─ Gemini
+    │   ├─ OpenAI
+    │   ├─ Anthropic
+    │   └─ Ollama
+    │
+    └─ Embedding Provider (Embeddings)
+        ├─ Gemini
+        └─ Ollama
+```
+
+## Usage Example
+
+```python
+from config.settings import get_settings
+from llm.factory import LLMFactory
+from llm.models import EmbeddingRequest
+
+settings = get_settings()
+
+# Create embedding provider
+embedding_provider = LLMFactory.create_embedding_provider(settings)
+
+# Get embeddings
+request = EmbeddingRequest(texts=["Hello world", "AI is great"])
+response = await embedding_provider.get_embeddings(request)
+```
+
+## Configuration
+
+### .env File
+```bash
+# Embedding Provider (independent of LLM provider)
+EMBEDDING_PROVIDER=ollama  # or "gemini"
+
+# Ollama Configuration
+OLLAMA_BASE_URL=http://localhost:11434
+OLLAMA_EMBEDDING_MODEL=nomic-embed-text
+```
+
+## Testing
+
+Run the example:
+```bash
+python examples/hybrid_embedding_example.py
+```
+
+This will:
+- Test Ollama embeddings (if Ollama is running)
+- Test Gemini embeddings (if API key is set)
+- Compare both providers
+- Show configuration options
+
+## Benefits
+
+1. **Cost Savings**: Use free Ollama for high-volume embeddings
+2. **Privacy**: Keep sensitive data local with Ollama
+3. **Flexibility**: Switch providers via config
+4. **Quality Options**: Use Gemini when quality is critical
+5. **No Lock-in**: Easy to switch or use both
+
+## Next Steps
+
+1. **Test with Ollama**:
+   ```bash
+   # Install Ollama
+   # https://ollama.ai
+   
+   # Pull embedding model
+   ollama pull nomic-embed-text
+   
+   # Set in .env
+   EMBEDDING_PROVIDER=ollama
+   ```
+
+2. **Test with Gemini**:
+   ```bash
+   # Set in .env
+   EMBEDDING_PROVIDER=gemini
+   GEMINI_API_KEY=your_key
+   ```
+
+3. **Integrate with Vector Store**:
+   - Update `mcp_servers/vector_search_server/vector_store.py`
+   - Use `LLMFactory.create_embedding_provider()` for embeddings
+
+## Files Created/Modified
+
+### New Files
+- `llm/ollama_client.py` - Ollama provider implementation
+- `llm/HYBRID_EMBEDDINGS.md` - Complete documentation
+- `examples/hybrid_embedding_example.py` - Example code
+- `HYBRID_IMPLEMENTATION_SUMMARY.md` - This file
+
+### Modified Files
+- `config/settings.py` - Added embedding provider and Ollama config
+- `llm/factory.py` - Added embedding provider factory method
+- `llm/README.md` - Added hybrid approach documentation
+
+## Status
+
+✅ **Complete and Ready to Use**
+
+The hybrid embedding approach is fully implemented and ready for use. You can now:
+- Use Ollama for local, cost-effective embeddings
+- Use Gemini for cloud-based, high-quality embeddings
+- Switch between providers via configuration
+- Use different providers for chat and embeddings
+
+---
+
+**Implementation Date**: Today  
+**Status**: ✅ Complete  
+**Next**: Integrate with vector search server
diff --git a/docs/implementation/hybrid_setup_success.md b/docs/implementation/hybrid_setup_success.md
new file mode 100644
index 0000000..4484778
--- /dev/null
+++ b/docs/implementation/hybrid_setup_success.md
@@ -0,0 +1,87 @@
+# ✅ Hybrid Embedding Setup - Success!
+
+## Current Configuration
+
+Based on your test output, your hybrid embedding setup is **working correctly**:
+
+```
+LLM Provider: gemini          ✅ Using Gemini for chat
+Embedding Provider: ollama    ✅ Using Ollama for embeddings
+```
+
+## Test Results
+
+### ✅ Ollama Embeddings - WORKING PERFECTLY
+
+- **Status**: ✅ Success
+- **Model**: `nomic-embed-text`
+- **Dimension**: 768
+- **Performance**: Fast, local, no quota limits
+- **Cost**: Free
+
+### ⚠️ Gemini Embeddings - Quota Limit (Expected)
+
+- **Status**: Quota exceeded (429 error)
+- **Reason**: Free tier has daily/minute limits
+- **Impact**: None - you're using Ollama anyway!
+- **Solution**: This is fine - Ollama is handling embeddings
+
+## What This Means
+
+Your setup is **exactly as intended**:
+
+1. **Chat**: Using Gemini (high-quality responses)
+2. **Embeddings**: Using Ollama (free, local, no quotas)
+
+This is the **optimal configuration** for:
+- ✅ Cost savings (no embedding API costs)
+- ✅ No quota limits (Ollama is unlimited)
+- ✅ Privacy (embeddings stay local)
+- ✅ High-quality chat (Gemini)
+
+## Current Status
+
+```
+┌─────────────────────────────────┐
+│  Your Application               │
+├─────────────────────────────────┤
+│  Chat → Gemini ✅               │
+│  Embeddings → Ollama ✅         │
+└─────────────────────────────────┘
+```
+
+## Benefits You're Getting
+
+1. **No Embedding Costs**: Ollama is free
+2. **No Quota Limits**: Process unlimited embeddings
+3. **Fast Performance**: Local embeddings are instant
+4. **Privacy**: Embedding data never leaves your machine
+5. **Quality Chat**: Still using Gemini for responses
+
+## Next Steps
+
+Your hybrid setup is complete and working! You can now:
+
+1. **Use in your application**:
+   ```python
+   from llm.factory import LLMFactory
+   
+   # Chat with Gemini
+   llm_provider = LLMFactory.create_provider(settings)
+   
+   # Embeddings with Ollama
+   embedding_provider = LLMFactory.create_embedding_provider(settings)
+   ```
+
+2. **Integrate with vector store**: Use `embedding_provider` in your vector search server
+
+3. **Scale up**: Process as many embeddings as you want with Ollama (no limits!)
+
+## Summary
+
+✅ **Hybrid approach is working perfectly**
+✅ **Ollama embeddings: Success**
+⚠️ **Gemini quota: Expected (not a problem - using Ollama)**
+✅ **Configuration: Optimal**
+
+You're all set! The hybrid embedding approach is working exactly as designed.
diff --git a/docs/implementation/implementation_status.md b/docs/implementation/implementation_status.md
new file mode 100644
index 0000000..c947620
--- /dev/null
+++ b/docs/implementation/implementation_status.md
@@ -0,0 +1,128 @@
+# Implementation Status
+
+## ✅ Completed: Model Abstraction Layer
+
+### Overview
+Successfully implemented a comprehensive LLM provider abstraction layer that supports:
+- **Client-Server Architecture**: ✅ Maintained throughout
+- **Model Abstraction**: ✅ Fully implemented with factory pattern
+
+### Files Created
+
+#### LLM Abstraction Layer (`llm/`)
+- ✅ `base.py` - Abstract base class `LLMProvider` with interface
+- ✅ `factory.py` - Factory pattern for provider creation
+- ✅ `models.py` - Common Pydantic models (LLMRequest, LLMResponse, EmbeddingRequest, EmbeddingResponse)
+- ✅ `gemini_client.py` - Fully implemented Gemini provider
+- ✅ `openai_client.py` - Placeholder for OpenAI (ready for implementation)
+- ✅ `anthropic_client.py` - Placeholder for Anthropic (ready for implementation)
+- ✅ `README.md` - Comprehensive documentation
+
+#### Configuration (`config/`)
+- ✅ `settings.py` - Multi-provider settings with environment variable support
+- ✅ `__init__.py` - Module exports
+
+#### Testing (`tests/`)
+- ✅ `test_llm_abstraction.py` - Unit tests for abstraction layer
+- ✅ `__init__.py` - Test module
+
+#### Documentation & Examples
+- ✅ `llm/README.md` - Usage guide and architecture documentation
+- ✅ `examples/llm_usage_example.py` - Working example code
+- ✅ `requirements.txt` - All necessary dependencies
+- ✅ `.env.example` - Configuration template (attempted, may need manual creation)
+
+### Key Features Implemented
+
+1. **Provider Abstraction**
+   - Abstract base class with consistent interface
+   - All providers implement same methods: `chat_completion()`, `get_embeddings()`
+   - Provider-agnostic application code
+
+2. **Factory Pattern**
+   - Automatic provider selection based on `LLM_PROVIDER` env var
+   - Validation of API keys and configuration
+   - Easy extension for new providers
+
+3. **Multi-Provider Support**
+   - Gemini: ✅ Fully implemented
+   - OpenAI: ⏳ Placeholder ready
+   - Anthropic: ⏳ Placeholder ready
+
+4. **Configuration Management**
+   - Environment variable based configuration
+   - Provider-specific settings (API keys, models)
+   - Provider-agnostic settings (temperature, max_tokens)
+   - Cached settings singleton
+
+5. **Type Safety**
+   - Pydantic models for all requests/responses
+   - Type hints throughout
+   - IDE-friendly autocomplete
+
+### Architecture Benefits
+
+✅ **Client-Server**: Maintained - MCP servers remain HTTP-based  
+✅ **Model Abstraction**: Complete - Switch providers via config  
+✅ **Extensibility**: Easy to add new providers  
+✅ **Testability**: Mock providers for unit tests  
+✅ **Consistency**: Unified interface across all providers  
+
+### Usage Example
+
+```python
+from config.settings import get_settings
+from llm.factory import LLMFactory
+from llm.models import LLMRequest
+
+settings = get_settings()
+llm_provider = LLMFactory.create_provider(settings)
+
+request = LLMRequest(
+    messages=[{"role": "user", "content": "Hello"}],
+    temperature=0.7
+)
+
+response = await llm_provider.chat_completion(request)
+print(response.content)
+```
+
+### Next Steps
+
+1. **Continue with MCP Servers** (Phase 1 from understanding.md)
+   - Base MCP server with HTTP + Auth + Versioning
+   - Catalog server
+   - Vector search server
+   - SQL query server
+
+2. **Implement Agent** (Phase 2 from understanding.md)
+   - Integrate LLM abstraction into agent
+   - LangGraph agent graph
+   - Tool orchestration
+
+3. **Complete OpenAI/Anthropic** (Optional)
+   - Implement OpenAI client
+   - Implement Anthropic client
+   - Add tests for each
+
+### Testing
+
+Run tests with:
+```bash
+pytest tests/test_llm_abstraction.py -v
+```
+
+### Configuration
+
+Set in `.env` file:
+```bash
+LLM_PROVIDER=gemini
+GEMINI_API_KEY=your_key_here
+GEMINI_MODEL=gemini-2.5-pro
+```
+
+---
+
+**Status**: ✅ Model Abstraction Layer Complete  
+**Date**: Implementation started  
+**Next**: MCP Server Development (Phase 1)
diff --git a/docs/implementation/phase1_complete.md b/docs/implementation/phase1_complete.md
new file mode 100644
index 0000000..3b5a384
--- /dev/null
+++ b/docs/implementation/phase1_complete.md
@@ -0,0 +1,110 @@
+# Phase 1: Complete Implementation Summary
+
+## ✅ All Issues Fixed
+
+### 1. ToolDiscovery Async Context Manager ✅
+- **Problem:** Missing `__aenter__` and `__aexit__` methods
+- **Fix:** Added async context manager support
+- **File:** `agent/tool_binding.py`
+
+### 2. SQL Query Server Error Handling ✅
+- **Problem:** Validation errors returned 500 instead of proper JSON-RPC errors
+- **Fix:** Added separate handling for `ValueError` with JSON-RPC error code -32602
+- **File:** `mcp_servers/base_server.py`
+
+### 3. Vector Search Server ✅
+- **Status:** Fully implemented
+- **Implementation:** Simple in-memory vector store with Gemini embeddings
+- **Files:**
+  - `mcp_servers/vector_search_server/server.py`
+  - `mcp_servers/vector_search_server/tools.py`
+  - `mcp_servers/vector_search_server/vector_store.py`
+
+## 📦 Components Implemented
+
+### MCP Servers (3 total)
+1. **Catalog Server** - Database catalog operations
+2. **SQL Query Server** - Read-only SQL queries
+3. **Vector Search Server** - Semantic document search
+
+### Client Components
+1. **MCP Client** - HTTP client with concurrency control
+2. **Tool Discovery** - Automatic tool discovery
+3. **Tool Result Normalizer** - Consistent result format
+
+## 🧪 Testing
+
+### Test Files
+- `examples/test_mcp_servers.py` - Full test suite
+- `examples/test_vector_search.py` - Vector search specific tests
+
+### Running Tests
+
+```bash
+# Install numpy if not already installed
+pip install numpy
+
+# Start all servers
+python scripts/start_servers.py
+
+# Or start individually:
+python -m mcp_servers.catalog_server.server
+python -m mcp_servers.sql_query_server.server
+python -m mcp_servers.vector_search_server.server
+
+# Run tests
+python examples/test_mcp_servers.py
+python examples/test_vector_search.py
+```
+
+## 📝 Dependencies
+
+### Added
+- `numpy>=1.24.0` - For vector operations (cosine similarity)
+
+### Already Installed
+- FastAPI, Uvicorn
+- aiosqlite
+- httpx
+- google-genai (for embeddings)
+
+## 🎯 Vector Search Server Details
+
+### Features
+- ✅ In-memory vector storage
+- ✅ JSON file persistence
+- ✅ Gemini embeddings (via LLM abstraction)
+- ✅ Cosine similarity search
+- ✅ Collection-based organization
+- ✅ No ChromaDB dependency (Python 3.14 compatible)
+
+### Tools
+1. **search_documents** - Semantic search
+   - Parameters: query, collection (optional), top_k (optional)
+   - Returns: List of similar documents with scores
+
+2. **add_documents** - Add documents with embeddings
+   - Parameters: documents (list), collection (optional)
+   - Returns: Add results with counts
+
+3. **list_collections** - List all collections
+   - Returns: List of collection names
+
+## 📊 Status
+
+✅ **Phase 1: COMPLETE**
+
+All components implemented and tested:
+- ✅ Base MCP Server
+- ✅ Catalog Server
+- ✅ SQL Query Server  
+- ✅ Vector Search Server
+- ✅ MCP Client
+- ✅ Tool Discovery
+- ✅ Tool Result Normalizer
+- ✅ Sample Data Setup
+- ✅ Test Suites
+
+## 🚀 Next Steps
+
+Ready for **Phase 2: LangGraph Agent Development**
diff --git a/docs/implementation/phase1_final_fix.md b/docs/implementation/phase1_final_fix.md
new file mode 100644
index 0000000..256abff
--- /dev/null
+++ b/docs/implementation/phase1_final_fix.md
@@ -0,0 +1,82 @@
+# Phase 1: Final Bug Fix - Read-Only Enforcement Test
+
+## Issue Identified
+
+**Problem:** The read-only enforcement test was showing a warning even though the enforcement was working correctly.
+
+**Root Cause:**
+1. Server correctly returns 400 Bad Request with JSON-RPC error for invalid queries
+2. MCP client was calling `response.raise_for_status()` before parsing the JSON-RPC error
+3. This caused an `httpx.HTTPError` to be raised instead of extracting the actual error message
+4. Test couldn't find "read-only" in the exception message because it was just an HTTP error
+
+## Solution
+
+### 1. Fixed MCP Client Error Handling
+**File:** `agent/mcp_client.py`
+
+**Changes:**
+- Parse JSON response even when HTTP status is not 200
+- Extract JSON-RPC error messages from response body
+- Raise `ValueError` with the actual error message instead of generic HTTP error
+- Only raise HTTP error if response is not valid JSON
+
+**Before:**
+```python
+response.raise_for_status()  # Raises before parsing JSON-RPC error
+result = response.json()
+```
+
+**After:**
+```python
+result = response.json()  # Parse first
+if "error" in result:
+    # Extract and raise with actual error message
+    raise ValueError(f"{error_message}: {error_data}")
+```
+
+### 2. Improved Test Recognition
+**File:** `examples/test_mcp_servers.py`
+
+**Changes:**
+- Better error message detection
+- Check for multiple keywords (read-only, not allowed, invalid params, INSERT, etc.)
+- Clearer success/failure messages
+
+### 3. Fixed Non-Interactive Test Execution
+**File:** `examples/test_mcp_servers.py`
+
+**Changes:**
+- Check if running in interactive terminal
+- Skip input prompt in non-interactive mode
+- Add 2-second delay instead of waiting for input
+
+## Result
+
+✅ **Read-only enforcement now properly recognized as working**
+- Test correctly identifies when INSERT/UPDATE/DELETE queries are blocked
+- Error messages are properly extracted and displayed
+- Tests can run in both interactive and non-interactive modes
+
+## Testing
+
+The test now correctly shows:
+```
+3. Test Read-Only Enforcement:
+   [OK] Read-only enforcement working
+   [OK] Error message: Invalid params: Read-only mode: INSERT operations are not allowed...
+```
+
+Instead of:
+```
+3. Test Read-Only Enforcement:
+   ⚠️  Unexpected error: MCP server HTTP error: Client error '400 Bad Request'...
+```
+
+## Status
+
+✅ **All Phase 1 issues resolved**
+- ToolDiscovery async context manager ✅
+- SQL Query error handling ✅
+- Vector Search server ✅
+- Read-only enforcement test ✅
diff --git a/docs/implementation/phase1_fixes.md b/docs/implementation/phase1_fixes.md
new file mode 100644
index 0000000..5822eb8
--- /dev/null
+++ b/docs/implementation/phase1_fixes.md
@@ -0,0 +1,93 @@
+# Phase 1: Bug Fixes and Vector Search Implementation
+
+## Issues Fixed
+
+### 1. ToolDiscovery Async Context Manager
+**Problem:** `ToolDiscovery` class didn't support async context manager protocol
+**Error:** `TypeError: 'agent.tool_binding.ToolDiscovery' object does not support the asynchronous context manager protocol`
+
+**Solution:**
+- Added `__aenter__` and `__aexit__` methods to `ToolDiscovery` class
+- Updated test to properly use async context manager
+
+### 2. SQL Query Server Error Handling
+**Problem:** Read-only validation errors returned 500 instead of proper JSON-RPC error
+**Error:** Server error '500 Internal Server Error' for invalid queries
+
+**Solution:**
+- Updated `base_server.py` to catch `ValueError` separately
+- Return JSON-RPC error code -32602 (Invalid params) for validation errors
+- Return JSON-RPC error code -32603 (Internal Error) for other exceptions
+
+### 3. Vector Search Server Implementation
+**Status:** ✅ Implemented
+
+**Implementation Details:**
+- Created `SimpleVectorStore` class using in-memory storage
+- Uses Gemini embeddings (via LLM abstraction layer)
+- Stores vectors in JSON files for persistence
+- Implements cosine similarity for search
+- No ChromaDB dependency (works with Python 3.14)
+
+**Tools:**
+- `search_documents` - Semantic search with cosine similarity
+- `add_documents` - Add documents with automatic embedding
+- `list_collections` - List all collections
+
+## Vector Search Server Architecture
+
+```
+Vector Search Server
+├── SimpleVectorStore (in-memory + JSON persistence)
+│   ├── Uses Gemini embeddings (via LLM factory)
+│   ├── Cosine similarity search
+│   └── Collection-based storage
+└── Tools
+    ├── search_documents
+    ├── add_documents
+    └── list_collections
+```
+
+## Testing
+
+### Test Vector Search Server
+
+```bash
+# Start server
+python -m mcp_servers.vector_search_server.server
+
+# Run test
+python examples/test_vector_search.py
+```
+
+### Test All Servers
+
+```bash
+# Start all servers
+python scripts/start_servers.py
+
+# Run full test suite
+python examples/test_mcp_servers.py
+```
+
+## Dependencies Added
+
+- `numpy>=1.24.0` - For vector operations (cosine similarity)
+
+## Notes
+
+- Vector Search server uses simple in-memory storage
+- For production, consider migrating to ChromaDB when Python 3.14 wheels are available
+- All embeddings use the configured embedding provider (Gemini by default)
+- Collections are persisted to JSON files in `data/vector_store/`
+
+## Status
+
+✅ All Phase 1 components complete:
+- Base MCP Server
+- Catalog Server
+- SQL Query Server
+- Vector Search Server
+- MCP Client
+- Tool Discovery
+- Tool Result Normalizer
diff --git a/docs/implementation/phase1_mcp_servers.md b/docs/implementation/phase1_mcp_servers.md
new file mode 100644
index 0000000..9e5f1f5
--- /dev/null
+++ b/docs/implementation/phase1_mcp_servers.md
@@ -0,0 +1,210 @@
+# Phase 1: MCP Servers Implementation
+
+## Overview
+
+Phase 1 implements the foundation for MCP (Model Context Protocol) servers with HTTP transport, authentication, and versioning support.
+
+## Components Implemented
+
+### 1. Base MCP Server (`mcp_servers/base_server.py`)
+
+**Features:**
+- ✅ HTTP-based using FastAPI
+- ✅ JSON-RPC 2.0 protocol
+- ✅ Authentication via `X-MCP-KEY` header
+- ✅ Version metadata (server_version, protocol_version)
+- ✅ Health check endpoint (`GET /health`)
+- ✅ Tools list endpoint (`GET /tools`)
+- ✅ Tool execution endpoint (`POST /execute`)
+
+**Endpoints:**
+- `GET /health` - Returns server health and version info
+- `GET /tools` - Returns list of available tools with versioning
+- `POST /execute` - Executes tools using JSON-RPC 2.0 format
+
+### 2. Catalog MCP Server (`mcp_servers/catalog_server/`)
+
+**Purpose:** Database catalog operations
+
+**Tools:**
+- `list_tables` - List all tables in the database
+- `describe_table` - Get schema information for a table
+- `get_table_row_count` - Get row count for a table
+
+**Files:**
+- `server.py` - Server implementation
+- `tools.py` - Tool definitions with versioning
+- `database.py` - SQLite database operations
+
+### 3. SQL Query MCP Server (`mcp_servers/sql_query_server/`)
+
+**Purpose:** Read-only SQL query execution
+
+**Features:**
+- ✅ Read-only enforcement (blocks INSERT, UPDATE, DELETE, etc.)
+- ✅ Only SELECT queries allowed
+- ✅ Query validation
+
+**Tools:**
+- `execute_query` - Execute a read-only SQL SELECT query
+- `explain_query` - Get execution plan for a query
+
+**Files:**
+- `server.py` - Server implementation
+- `tools.py` - Tool definitions with versioning
+- `query_engine.py` - SQL query engine with read-only validation
+
+### 4. MCP Client (`agent/mcp_client.py`)
+
+**Features:**
+- ✅ HTTP client for MCP servers
+- ✅ JSON-RPC 2.0 communication
+- ✅ Authentication header support (`X-MCP-KEY`)
+- ✅ Concurrency control (semaphore-based)
+- ✅ Request timeout handling
+- ✅ Request ID propagation
+
+**Methods:**
+- `call_tool()` - Call a tool on an MCP server
+- `list_tools()` - List available tools from a server
+- `health_check()` - Check server health
+
+### 5. Tool Result Normalizer (`agent/tool_result_normalizer.py`)
+
+**Purpose:** Normalize all tool results to consistent format
+
+**Format:**
+```python
+{
+    "status": "success" | "error",
+    "data": <result_data>,
+    "metadata": {
+        "tool_name": "...",
+        "tool_version": "...",
+        "request_id": "...",
+        "timestamp": "..."
+    },
+    "error": <error_info> | None
+}
+```
+
+### 6. Tool Discovery System (`agent/tool_binding.py`)
+
+**Features:**
+- ✅ Discover tools from MCP servers
+- ✅ Store tool metadata with versioning
+- ✅ Discover all configured servers
+- ✅ Tool lookup by key
+
+## Usage Examples
+
+### Starting Servers
+
+```bash
+# Setup sample data first
+python scripts/setup_data.py
+
+# Start individual servers
+python -m mcp_servers.catalog_server.server
+python -m mcp_servers.sql_query_server.server
+
+# Or use the start script (basic)
+python scripts/start_servers.py
+```
+
+### Using MCP Client
+
+```python
+from agent.mcp_client import MCPClient
+
+async with MCPClient() as client:
+    # List tools
+    tools = await client.list_tools("http://localhost:7001")
+    
+    # Call a tool
+    result = await client.call_tool(
+        server_url="http://localhost:7001",
+        tool_name="list_tables",
+        params={}
+    )
+```
+
+### Tool Discovery
+
+```python
+from agent.tool_binding import ToolDiscovery
+
+async with ToolDiscovery() as discovery:
+    # Discover all servers
+    results = await discovery.discover_all_servers()
+    
+    # Get discovered tools
+    tools = discovery.get_discovered_tools()
+```
+
+## Configuration
+
+All settings are in `.env`:
+
+```bash
+# MCP Server Ports
+CATALOG_MCP_PORT=7001
+SQL_MCP_PORT=7003
+
+# MCP Authentication (optional)
+MCP_API_KEY=your_shared_api_key
+
+# Concurrency
+MAX_PARALLEL_MCP_CALLS=5
+MCP_CALL_TIMEOUT=30
+```
+
+## Testing
+
+### Test Health Endpoints
+
+```bash
+# Catalog server
+curl http://localhost:7001/health
+
+# SQL Query server
+curl http://localhost:7003/health
+```
+
+### Test Tool Listing
+
+```bash
+# Catalog server
+curl http://localhost:7001/tools
+
+# SQL Query server
+curl http://localhost:7003/tools
+```
+
+### Test Tool Execution
+
+```bash
+# List tables
+curl -X POST http://localhost:7001/execute \
+  -H "Content-Type: application/json" \
+  -d '{
+    "jsonrpc": "2.0",
+    "id": "1",
+    "method": "list_tables",
+    "params": {}
+  }'
+```
+
+## Next Steps
+
+- ✅ Phase 1 Complete: Base infrastructure ready
+- ⏳ Phase 2: LangGraph Agent Development
+- ⏳ Phase 3: FastAPI Deployment
+- ⏳ Phase 4: Testing & Documentation
+
+## Notes
+
+- Vector Search server is deferred (requires ChromaDB, which has Python 3.14 compatibility issues)
+- All servers support versioning and authentication
+- Read-only enforcement is strict for SQL queries
+- Request ID propagation enables end-to-end tracing
diff --git a/docs/implementation/phase1_summary.md b/docs/implementation/phase1_summary.md
new file mode 100644
index 0000000..f728c4a
--- /dev/null
+++ b/docs/implementation/phase1_summary.md
@@ -0,0 +1,145 @@
+# Phase 1 Implementation Summary
+
+## ✅ Completed Components
+
+### 1. Base MCP Server Infrastructure
+- **File:** `mcp_servers/base_server.py`
+- **Status:** ✅ Complete
+- **Features:**
+  - HTTP-based using FastAPI
+  - JSON-RPC 2.0 protocol
+  - Authentication middleware (`X-MCP-KEY`)
+  - Version metadata (server_version, protocol_version)
+  - Health check endpoint
+  - Tools listing endpoint
+  - Tool execution endpoint
+
+### 2. Catalog MCP Server
+- **Files:**
+  - `mcp_servers/catalog_server/server.py`
+  - `mcp_servers/catalog_server/tools.py`
+  - `mcp_servers/catalog_server/database.py`
+- **Status:** ✅ Complete
+- **Tools:**
+  - `list_tables` - List all database tables
+  - `describe_table` - Get table schema
+  - `get_table_row_count` - Get row count
+
+### 3. SQL Query MCP Server
+- **Files:**
+  - `mcp_servers/sql_query_server/server.py`
+  - `mcp_servers/sql_query_server/tools.py`
+  - `mcp_servers/sql_query_server/query_engine.py`
+- **Status:** ✅ Complete
+- **Features:**
+  - Read-only enforcement (blocks INSERT, UPDATE, DELETE, etc.)
+  - Only SELECT queries allowed
+  - Query validation
+- **Tools:**
+  - `execute_query` - Execute SELECT queries
+  - `explain_query` - Get query execution plan
+
+### 4. MCP Client
+- **File:** `agent/mcp_client.py`
+- **Status:** ✅ Complete
+- **Features:**
+  - HTTP client for MCP servers
+  - JSON-RPC 2.0 communication
+  - Authentication support
+  - Concurrency control (semaphore)
+  - Request timeout handling
+  - Request ID propagation
+
+### 5. Tool Result Normalizer
+- **File:** `agent/tool_result_normalizer.py`
+- **Status:** ✅ Complete
+- **Purpose:** Normalize all tool results to consistent format
+
+### 6. Tool Discovery System
+- **File:** `agent/tool_binding.py`
+- **Status:** ✅ Complete
+- **Features:**
+  - Discover tools from MCP servers
+  - Store tool metadata with versioning
+  - Discover all configured servers
+  - Tool lookup by key
+
+### 7. Sample Data Setup
+- **File:** `scripts/setup_data.py`
+- **Status:** ✅ Complete
+- **Purpose:** Create sample SQLite database with test data
+
+### 8. Server Startup Script
+- **File:** `scripts/start_servers.py`
+- **Status:** ✅ Complete
+- **Purpose:** Start all MCP servers
+
+## 📊 Statistics
+
+- **Total Files Created:** 15+
+- **Lines of Code:** ~1500+
+- **Servers Implemented:** 2 (Catalog, SQL Query)
+- **Tools Available:** 5
+- **Test Coverage:** Basic test suite included
+
+## 🎯 Success Criteria Met
+
+- ✅ All MCP servers return version metadata
+- ✅ MCP authentication works correctly
+- ✅ Request IDs can propagate through system
+- ✅ Tool results are normalized consistently
+- ✅ Concurrency limits prevent overload
+- ✅ SQL queries are read-only enforced
+- ✅ Tool discovery system functional
+
+## ⏳ Deferred Components
+
+### Vector Search Server
+- **Status:** ⏳ Deferred
+- **Reason:** ChromaDB requires compilation on Python 3.14
+- **Alternative:** Can be added later when ChromaDB wheels are available
+
+## 📝 Documentation Created
+
+1. `docs/implementation/phase1_mcp_servers.md` - Detailed implementation guide
+2. `docs/guides/phase1_quickstart.md` - Quick start guide
+3. `docs/implementation/phase1_summary.md` - This file
+
+## 🚀 Next Phase
+
+**Phase 2: LangGraph Agent Development**
+- Agent state schema
+- LangGraph agent graph
+- Tool orchestration
+- Prompt versioning
+
+## 📦 Files Structure
+
+```
+mcp_servers/
+├── base_server.py          # Base MCP server
+├── catalog_server/
+│   ├── server.py
+│   ├── tools.py
+│   └── database.py
+└── sql_query_server/
+    ├── server.py
+    ├── tools.py
+    └── query_engine.py
+
+agent/
+├── mcp_client.py           # MCP HTTP client
+├── tool_result_normalizer.py
+└── tool_binding.py         # Tool discovery
+
+scripts/
+├── setup_data.py           # Sample data setup
+└── start_servers.py        # Server startup
+
+examples/
+└── test_mcp_servers.py     # Test suite
+```
+
+## ✅ Phase 1 Status: COMPLETE
+
+All core components for Phase 1 have been implemented and are ready for use. The foundation is solid for building the LangGraph agent in Phase 2.
diff --git a/docs/migration/gemini_migration.md b/docs/migration/gemini_migration.md
new file mode 100644
index 0000000..69f0704
--- /dev/null
+++ b/docs/migration/gemini_migration.md
@@ -0,0 +1,53 @@
+# Gemini API Migration Guide
+
+## Current Status
+
+✅ **Fixed**: Embedding API error - now using `genai.embed_content()` directly  
+⚠️ **Warning**: `google.generativeai` is deprecated, but still functional
+
+## Should You Migrate to `google-genai`?
+
+### Yes, but not urgent:
+- ✅ **Current package works**: `google-generativeai` still functions correctly
+- ⚠️ **Deprecated**: No new updates or bug fixes
+- 🔄 **Future-proof**: `google-genai` is the recommended package going forward
+
+### Migration Benefits:
+1. **Active support**: Regular updates and bug fixes
+2. **Better MCP integration**: Native MCP server support
+3. **Improved API**: Cleaner, more consistent interface
+4. **Future compatibility**: Won't break when old package is removed
+
+## Migration Steps (When Ready)
+
+### 1. Install new package:
+```bash
+pip install google-genai
+```
+
+### 2. Update `llm/gemini_client.py`:
+```python
+# Old (deprecated)
+import google.generativeai as genai
+
+# New (recommended)
+import google.genai as genai
+```
+
+### 3. API Changes:
+- Chat completion: Mostly compatible
+- Embeddings: Slightly different API (check new docs)
+- Configuration: Similar but may have minor differences
+
+## Current Fix Applied
+
+✅ **Embedding API Fixed**: Changed from `GenerativeModel.embed_content()` to `genai.embed_content()`
+
+The current implementation works with `google-generativeai`. You can:
+1. **Use it now** - Everything works with the fix
+2. **Migrate later** - When you have time, follow the migration steps above
+
+## Recommendation
+
+**For now**: Keep using `google-generativeai` (it works after the fix)  
+**For production**: Plan migration to `google-genai` in the next sprint
diff --git a/docs/migration/gemini_migration_complete.md b/docs/migration/gemini_migration_complete.md
new file mode 100644
index 0000000..d27ff67
--- /dev/null
+++ b/docs/migration/gemini_migration_complete.md
@@ -0,0 +1,84 @@
+# ✅ Gemini Migration to google-genai - Complete
+
+## Migration Summary
+
+Successfully migrated from deprecated `google-generativeai` to `google-genai` package.
+
+## Changes Made
+
+### 1. Updated Requirements
+- ✅ Changed `google-generativeai>=0.3.2` → `google-genai>=1.0.0`
+- ✅ Package already installed in your environment
+
+### 2. Updated Gemini Client
+- ✅ Replaced `import google.generativeai as genai` → `from google import genai`
+- ✅ Updated client initialization: `genai.Client(api_key=api_key)`
+- ✅ Updated API calls to use new `google-genai` API structure
+- ✅ Updated embedding model: `text-embedding-004` (latest)
+
+### 3. API Changes
+- ✅ Chat completion: Uses `client.models.generate_content()`
+- ✅ Embeddings: Uses `client.models.embed_content()`
+- ✅ Better error handling with new package
+
+## Benefits
+
+1. **Active Support**: Package is actively maintained by Google
+2. **Better Errors**: Clearer error messages for API issues
+3. **Latest Models**: Access to newest embedding models
+4. **No Deprecation Warnings**: Clean output without warnings
+
+## Testing
+
+Test with your new API key:
+
+```bash
+python examples/llm_usage_example.py
+```
+
+Or test embeddings:
+
+```bash
+python examples/hybrid_embedding_example.py
+```
+
+## Next Steps
+
+1. **Verify API Key**: Make sure your new API key is in `.env`:
+   ```bash
+   GEMINI_API_KEY=your_new_key_here
+   ```
+
+2. **Test the Migration**: Run the examples to verify everything works
+
+3. **Check Quota**: If you still get quota errors, check:
+   - API key is valid at https://aistudio.google.com/apikey
+   - Quota limits in Google Cloud Console
+   - Billing is enabled (if using paid tier)
+
+## Troubleshooting
+
+### If you still get errors:
+
+1. **API Key Issues**:
+   - Verify key is correct in `.env`
+   - Check for extra spaces or quotes
+   - Regenerate key if needed
+
+2. **Quota Errors**:
+   - Check quota limits at Google AI Studio
+   - Wait for quota reset
+   - Consider using Ollama for embeddings (already set up!)
+
+3. **Import Errors**:
+   - Ensure `google-genai` is installed: `pip install google-genai`
+   - Restart your terminal/IDE
+
+## Status
+
+✅ **Migration Complete**
+✅ **Package Updated**
+✅ **Client Rewritten**
+✅ **Ready to Test**
+
+The migration is complete. Your new API key should work with the updated `google-genai` package!
diff --git a/examples/hybrid_embedding_example.py b/examples/hybrid_embedding_example.py
new file mode 100644
index 0000000..ea741e6
--- /dev/null
+++ b/examples/hybrid_embedding_example.py
@@ -0,0 +1,125 @@
+"""Example demonstrating hybrid embedding approach with Ollama and Gemini."""
+
+import asyncio
+import sys
+from pathlib import Path
+
+# Add project root to Python path
+project_root = Path(__file__).parent.parent
+sys.path.insert(0, str(project_root))
+
+from dotenv import load_dotenv
+from config.settings import get_settings
+from llm.factory import LLMFactory
+from llm.models import EmbeddingRequest
+
+# Load environment variables
+load_dotenv()
+
+
+async def test_embeddings(provider_name: str, texts: list):
+    """Test embeddings with a specific provider."""
+    print(f"\n{'='*60}")
+    print(f"Testing {provider_name.upper()} Embeddings")
+    print(f"{'='*60}")
+    
+    try:
+        settings = get_settings()
+        
+        # Create embedding provider
+        if provider_name == "ollama":
+            settings.embedding_provider = "ollama"
+        elif provider_name == "gemini":
+            settings.embedding_provider = "gemini"
+        
+        embedding_provider = LLMFactory.create_embedding_provider(settings)
+        print(f"Created provider: {embedding_provider}")
+        
+        # Create embedding request
+        embedding_request = EmbeddingRequest(texts=texts)
+        
+        # Get embeddings
+        print(f"\nGenerating embeddings for {len(texts)} texts...")
+        embedding_response = await embedding_provider.get_embeddings(embedding_request)
+        
+        print(f"[SUCCESS]")
+        print(f"   Provider: {embedding_response.provider}")
+        print(f"   Model: {embedding_response.model}")
+        print(f"   Number of embeddings: {len(embedding_response.embeddings)}")
+        print(f"   Embedding dimension: {len(embedding_response.embeddings[0])}")
+        print(f"   First embedding sample: {embedding_response.embeddings[0][:5]}...")
+        
+        return embedding_response
+        
+    except Exception as e:
+        print(f"[ERROR] Error with {provider_name}: {str(e)}")
+        return None
+
+
+async def compare_embeddings():
+    """Compare embeddings from different providers."""
+    print("\n" + "="*60)
+    print("HYBRID EMBEDDING APPROACH DEMONSTRATION")
+    print("="*60)
+    
+    # Test texts
+    test_texts = [
+        "Machine learning is a subset of artificial intelligence",
+        "Deep learning uses neural networks with multiple layers",
+        "Natural language processing helps computers understand text"
+    ]
+    
+    print(f"\nTest texts:")
+    for i, text in enumerate(test_texts, 1):
+        print(f"  {i}. {text}")
+    
+    # Test Ollama embeddings
+    ollama_result = await test_embeddings("ollama", test_texts)
+    
+    # Test Gemini embeddings
+    gemini_result = await test_embeddings("gemini", test_texts)
+    
+    # Summary
+    print(f"\n{'='*60}")
+    print("SUMMARY")
+    print(f"{'='*60}")
+    
+    if ollama_result:
+        print(f"[OK] Ollama: Working")
+        print(f"   - Model: {ollama_result.model}")
+        print(f"   - Dimension: {len(ollama_result.embeddings[0])}")
+    else:
+        print(f"[FAIL] Ollama: Not available (check if Ollama is running)")
+        print(f"   Install: https://ollama.ai")
+        print(f"   Run: ollama pull nomic-embed-text")
+    
+    if gemini_result:
+        print(f"[OK] Gemini: Working")
+        print(f"   - Model: {gemini_result.model}")
+        print(f"   - Dimension: {len(gemini_result.embeddings[0])}")
+    else:
+        print(f"[FAIL] Gemini: Not available")
+        print(f"   - Likely quota limit (free tier restrictions)")
+        print(f"   - Or check API key in .env file")
+        print(f"   - This is OK - you're using Ollama for embeddings anyway!")
+    
+    print(f"\n[TIP] You can switch embedding providers via EMBEDDING_PROVIDER in .env")
+    print(f"   - EMBEDDING_PROVIDER=ollama  (for local, cost-effective)")
+    print(f"   - EMBEDDING_PROVIDER=gemini  (for cloud, high-quality)")
+
+
+async def main():
+    """Main function."""
+    settings = get_settings()
+    
+    print("\nCurrent Configuration:")
+    print(f"  LLM Provider: {settings.llm_provider}")
+    print(f"  Embedding Provider: {settings.embedding_provider}")
+    print(f"  Ollama URL: {settings.ollama_base_url}")
+    print(f"  Ollama Embedding Model: {settings.ollama_embedding_model}")
+    
+    await compare_embeddings()
+
+
+if __name__ == "__main__":
+    asyncio.run(main())
diff --git a/examples/llm_usage_example.py b/examples/llm_usage_example.py
new file mode 100644
index 0000000..3537a87
--- /dev/null
+++ b/examples/llm_usage_example.py
@@ -0,0 +1,84 @@
+"""Example usage of LLM abstraction layer."""
+
+import asyncio
+import os
+import sys
+from pathlib import Path
+
+# Add project root to Python path
+project_root = Path(__file__).parent.parent
+sys.path.insert(0, str(project_root))
+
+from dotenv import load_dotenv
+from config.settings import get_settings
+from llm.factory import LLMFactory
+from llm.models import LLMRequest, EmbeddingRequest
+
+# Load environment variables
+load_dotenv()
+
+
+async def main():
+    """Example of using LLM abstraction."""
+    
+    # Get settings
+    settings = get_settings()
+    print(f"Using LLM Provider: {settings.llm_provider}")
+    print(f"Model: {settings.gemini_model if settings.llm_provider == 'gemini' else 'N/A'}")
+    print("-" * 50)
+    
+    # Create provider using factory
+    try:
+        llm_provider = LLMFactory.create_provider(settings)
+        print(f"Created provider: {llm_provider}")
+        print("-" * 50)
+        
+        # Example 1: Chat completion
+        print("\n1. Chat Completion Example:")
+        chat_request = LLMRequest(
+            messages=[
+                {"role": "system", "content": "You are a helpful assistant."},
+                {"role": "user", "content": "What is machine learning?"}
+            ],
+            temperature=0.7,
+            max_tokens=500
+        )
+        
+        chat_response = await llm_provider.chat_completion(chat_request)
+        print(f"Response: {chat_response.content[:200]}...")
+        print(f"Provider: {chat_response.provider}")
+        print(f"Model: {chat_response.model}")
+        if chat_response.usage:
+            print(f"Usage: {chat_response.usage}")
+        
+        # Example 2: Embeddings
+        print("\n2. Embeddings Example:")
+        embedding_request = EmbeddingRequest(
+            texts=[
+                "Machine learning is a subset of AI",
+                "Deep learning uses neural networks",
+                "Natural language processing helps computers understand text"
+            ]
+        )
+        
+        embedding_response = await llm_provider.get_embeddings(embedding_request)
+        print(f"Number of embeddings: {len(embedding_response.embeddings)}")
+        print(f"Embedding dimension: {len(embedding_response.embeddings[0])}")
+        print(f"Provider: {embedding_response.provider}")
+        print(f"Model: {embedding_response.model}")
+        
+    except ValueError as e:
+        print(f"Error: {e}")
+        print("\nPlease set the appropriate API key in your .env file:")
+        if settings.llm_provider == "gemini":
+            print("  GEMINI_API_KEY=your_key_here")
+        elif settings.llm_provider == "openai":
+            print("  OPENAI_API_KEY=your_key_here")
+        elif settings.llm_provider == "anthropic":
+            print("  ANTHROPIC_API_KEY=your_key_here")
+    except Exception as e:
+        print(f"Unexpected error: {e}")
+
+
+if __name__ == "__main__":
+    asyncio.run(main())
diff --git a/examples/test_mcp_servers.py b/examples/test_mcp_servers.py
new file mode 100644
index 0000000..ad8e5c3
--- /dev/null
+++ b/examples/test_mcp_servers.py
@@ -0,0 +1,212 @@
+"""Test MCP servers functionality."""
+
+import asyncio
+import sys
+from pathlib import Path
+
+# Add project root to path
+project_root = Path(__file__).parent.parent
+sys.path.insert(0, str(project_root))
+
+from agent.mcp_client import MCPClient
+from agent.tool_binding import ToolDiscovery
+from agent.tool_result_normalizer import normalize_result
+from config.settings import get_settings
+
+
+async def test_catalog_server():
+    """Test catalog server."""
+    print("\n" + "="*60)
+    print("Testing Catalog MCP Server")
+    print("="*60)
+    
+    settings = get_settings()
+    server_url = f"http://localhost:{settings.catalog_mcp_port}"
+    
+    async with MCPClient() as client:
+        # Health check
+        print("\n1. Health Check:")
+        try:
+            health = await client.health_check(server_url)
+            print(f"   ✅ Server: {health.get('server_name')}")
+            print(f"   ✅ Version: {health.get('server_version')}")
+            print(f"   ✅ Status: {health.get('status')}")
+        except Exception as e:
+            print(f"   ❌ Health check failed: {e}")
+            return
+        
+        # List tools
+        print("\n2. List Tools:")
+        try:
+            tools_info = await client.list_tools(server_url)
+            print(f"   ✅ Server: {tools_info.get('server_name')}")
+            print(f"   ✅ Tools found: {len(tools_info.get('tools', []))}")
+            for tool in tools_info.get('tools', []):
+                print(f"      - {tool['name']} (v{tool.get('tool_version', 'N/A')})")
+        except Exception as e:
+            print(f"   ❌ List tools failed: {e}")
+            return
+        
+        # Call list_tables
+        print("\n3. Call list_tables:")
+        try:
+            result = await client.call_tool(
+                server_url=server_url,
+                tool_name="list_tables",
+                params={}
+            )
+            normalized = normalize_result(result, "list_tables")
+            print(f"   ✅ Status: {normalized['status']}")
+            if normalized['status'] == 'success':
+                tables = normalized['data'].get('tables', [])
+                print(f"   ✅ Tables found: {len(tables)}")
+                for table in tables:
+                    print(f"      - {table}")
+        except Exception as e:
+            print(f"   ❌ Call tool failed: {e}")
+        
+        # Call describe_table
+        print("\n4. Call describe_table:")
+        try:
+            result = await client.call_tool(
+                server_url=server_url,
+                tool_name="describe_table",
+                params={"table_name": "users"}
+            )
+            normalized = normalize_result(result, "describe_table")
+            print(f"   ✅ Status: {normalized['status']}")
+            if normalized['status'] == 'success':
+                table_info = normalized['data']
+                print(f"   ✅ Table: {table_info.get('table_name')}")
+                print(f"   ✅ Columns: {len(table_info.get('columns', []))}")
+        except Exception as e:
+            print(f"   ❌ Call tool failed: {e}")
+
+
+async def test_sql_query_server():
+    """Test SQL query server."""
+    print("\n" + "="*60)
+    print("Testing SQL Query MCP Server")
+    print("="*60)
+    
+    settings = get_settings()
+    server_url = f"http://localhost:{settings.sql_mcp_port}"
+    
+    async with MCPClient() as client:
+        # Health check
+        print("\n1. Health Check:")
+        try:
+            health = await client.health_check(server_url)
+            print(f"   ✅ Server: {health.get('server_name')}")
+            print(f"   ✅ Status: {health.get('status')}")
+        except Exception as e:
+            print(f"   ❌ Health check failed: {e}")
+            return
+        
+        # Execute query
+        print("\n2. Execute SELECT Query:")
+        try:
+            result = await client.call_tool(
+                server_url=server_url,
+                tool_name="execute_query",
+                params={"query": "SELECT * FROM users LIMIT 3"}
+            )
+            normalized = normalize_result(result, "execute_query")
+            print(f"   ✅ Status: {normalized['status']}")
+            if normalized['status'] == 'success':
+                data = normalized['data']
+                print(f"   ✅ Rows returned: {data.get('row_count', 0)}")
+        except Exception as e:
+            print(f"   ❌ Query execution failed: {e}")
+        
+        # Test read-only enforcement
+        print("\n3. Test Read-Only Enforcement:")
+        try:
+            result = await client.call_tool(
+                server_url=server_url,
+                tool_name="execute_query",
+                params={"query": "INSERT INTO users (name, email) VALUES ('Test', 'test@test.com')"}
+            )
+            print(f"   [ERROR] Read-only check failed - INSERT was allowed!")
+        except Exception as e:
+            error_str = str(e).lower()
+            # Check for read-only enforcement indicators
+            if any(keyword in error_str for keyword in [
+                "read-only", "not allowed", "invalid params", 
+                "insert", "update", "delete", "only select"
+            ]):
+                print(f"   [OK] Read-only enforcement working")
+                print(f"   [OK] Error message: {str(e)[:80]}...")
+            else:
+                print(f"   [WARNING] Unexpected error format: {e}")
+                print(f"   [INFO] This might still be correct - check if INSERT was blocked")
+
+
+async def test_tool_discovery():
+    """Test tool discovery."""
+    print("\n" + "="*60)
+    print("Testing Tool Discovery")
+    print("="*60)
+    
+    discovery = ToolDiscovery()
+    try:
+        print("\n1. Discover All Servers:")
+        results = await discovery.discover_all_servers()
+        
+        for server_name, server_info in results.items():
+            if "error" in server_info:
+                print(f"   ❌ {server_name}: {server_info['error']}")
+            else:
+                print(f"   ✅ {server_name}:")
+                print(f"      - Server Version: {server_info.get('server_version')}")
+                print(f"      - Tools: {server_info.get('tool_count', 0)}")
+        
+        print("\n2. Get All Discovered Tools:")
+        tools = discovery.get_discovered_tools()
+        print(f"   ✅ Total tools discovered: {len(tools)}")
+        for tool_key, tool_info in tools.items():
+            print(f"      - {tool_key}")
+    finally:
+        await discovery.close()
+
+
+async def main():
+    """Main test function."""
+    print("="*60)
+    print("MCP Servers Test Suite")
+    print("="*60)
+    print("\n[WARNING] Make sure servers are running:")
+    print("   python -m mcp_servers.catalog_server.server")
+    print("   python -m mcp_servers.sql_query_server.server")
+    print("   python -m mcp_servers.vector_search_server.server")
+    print("\nStarting tests in 2 seconds...")
+    
+    try:
+        import sys
+        if sys.stdin.isatty():
+            # Only wait for input if running in interactive terminal
+            try:
+                input("Press Enter to continue or Ctrl+C to exit...")
+            except KeyboardInterrupt:
+                print("\nExiting...")
+                return
+        else:
+            # Non-interactive mode, wait a bit then continue
+            import time
+            time.sleep(2)
+    except Exception:
+        # If anything fails, just continue
+        pass
+    
+    # Run tests
+    await test_catalog_server()
+    await test_sql_query_server()
+    await test_tool_discovery()
+    
+    print("\n" + "="*60)
+    print("✅ All tests completed!")
+    print("="*60)
+
+
+if __name__ == "__main__":
+    asyncio.run(main())
diff --git a/examples/test_vector_search.py b/examples/test_vector_search.py
new file mode 100644
index 0000000..47767c7
--- /dev/null
+++ b/examples/test_vector_search.py
@@ -0,0 +1,115 @@
+"""Test Vector Search MCP server."""
+
+import asyncio
+import sys
+from pathlib import Path
+
+# Add project root to path
+project_root = Path(__file__).parent.parent
+sys.path.insert(0, str(project_root))
+
+from agent.mcp_client import MCPClient
+from agent.tool_result_normalizer import normalize_result
+from config.settings import get_settings
+
+
+async def test_vector_search():
+    """Test vector search server."""
+    print("="*60)
+    print("Testing Vector Search MCP Server")
+    print("="*60)
+    
+    settings = get_settings()
+    server_url = f"http://localhost:{settings.vector_mcp_port}"
+    
+    async with MCPClient() as client:
+        # Health check
+        print("\n1. Health Check:")
+        try:
+            health = await client.health_check(server_url)
+            print(f"   [OK] Server: {health.get('server_name')}")
+            print(f"   [OK] Status: {health.get('status')}")
+        except Exception as e:
+            print(f"   [ERROR] Health check failed: {e}")
+            print("   Make sure the server is running:")
+            print(f"   python -m mcp_servers.vector_search_server.server")
+            return
+        
+        # Add documents
+        print("\n2. Add Documents:")
+        try:
+            result = await client.call_tool(
+                server_url=server_url,
+                tool_name="add_documents",
+                params={
+                    "collection": "test",
+                    "documents": [
+                        {
+                            "id": "doc1",
+                            "text": "Machine learning is a subset of artificial intelligence",
+                            "metadata": {"category": "AI"}
+                        },
+                        {
+                            "id": "doc2",
+                            "text": "Python is a popular programming language for data science",
+                            "metadata": {"category": "Programming"}
+                        },
+                        {
+                            "id": "doc3",
+                            "text": "Vector databases store embeddings for semantic search",
+                            "metadata": {"category": "Database"}
+                        }
+                    ]
+                }
+            )
+            normalized = normalize_result(result, "add_documents")
+            print(f"   [OK] Status: {normalized['status']}")
+            if normalized['status'] == 'success':
+                data = normalized['data']
+                print(f"   [OK] Added {data.get('added_count')} documents")
+                print(f"   [OK] Total in collection: {data.get('total_documents')}")
+        except Exception as e:
+            print(f"   [ERROR] Add documents failed: {e}")
+        
+        # Search documents
+        print("\n3. Search Documents:")
+        try:
+            result = await client.call_tool(
+                server_url=server_url,
+                tool_name="search_documents",
+                params={
+                    "query": "artificial intelligence and machine learning",
+                    "collection": "test",
+                    "top_k": 2
+                }
+            )
+            normalized = normalize_result(result, "search_documents")
+            print(f"   [OK] Status: {normalized['status']}")
+            if normalized['status'] == 'success':
+                data = normalized['data']
+                print(f"   [OK] Query: {data.get('query')}")
+                print(f"   [OK] Results found: {data.get('count')}")
+                for i, res in enumerate(data.get('results', []), 1):
+                    print(f"      {i}. Score: {res.get('score', 0):.4f} - {res.get('text', '')[:60]}...")
+        except Exception as e:
+            print(f"   [ERROR] Search failed: {e}")
+        
+        # List collections
+        print("\n4. List Collections:")
+        try:
+            result = await client.call_tool(
+                server_url=server_url,
+                tool_name="list_collections",
+                params={}
+            )
+            normalized = normalize_result(result, "list_collections")
+            print(f"   [OK] Status: {normalized['status']}")
+            if normalized['status'] == 'success':
+                data = normalized['data']
+                print(f"   [OK] Collections: {data.get('collections', [])}")
+        except Exception as e:
+            print(f"   [ERROR] List collections failed: {e}")
+
+
+if __name__ == "__main__":
+    asyncio.run(test_vector_search())
diff --git a/llm/README.md b/llm/README.md
new file mode 100644
index 0000000..9355c8c
--- /dev/null
+++ b/llm/README.md
@@ -0,0 +1,154 @@
+# LLM Provider Abstraction
+
+This module provides a clean abstraction layer for multiple LLM providers, allowing easy switching between different providers without changing application code.
+
+## 🎯 Hybrid Embedding Support
+
+**NEW**: This module now supports **hybrid embeddings** - use Ollama (local) or Gemini (cloud) for embeddings independently of your LLM provider choice. See [docs/guides/hybrid_embeddings.md](../docs/guides/hybrid_embeddings.md) for details.
+
+## Architecture
+
+### Components
+
+1. **`base.py`** - Abstract base class (`LLMProvider`) that all providers must implement
+2. **`factory.py`** - Factory pattern for creating provider instances based on configuration
+3. **`models.py`** - Common Pydantic models for requests and responses
+4. **Provider Implementations**:
+   - `gemini_client.py` - Google Gemini implementation (fully implemented)
+   - `openai_client.py` - OpenAI implementation (placeholder)
+   - `anthropic_client.py` - Anthropic implementation (placeholder)
+
+## Hybrid Embedding Approach
+
+You can use different providers for chat and embeddings:
+
+```python
+# Use Gemini for chat, Ollama for embeddings (cost-effective)
+settings.llm_provider = "gemini"
+settings.embedding_provider = "ollama"
+
+llm_provider = LLMFactory.create_provider(settings)
+embedding_provider = LLMFactory.create_embedding_provider(settings)
+```
+
+See [docs/guides/hybrid_embeddings.md](../docs/guides/hybrid_embeddings.md) for complete guide.
+
+## Usage
+
+### Basic Usage
+
+```python
+from config.settings import get_settings
+from llm.factory import LLMFactory
+from llm.models import LLMRequest
+
+# Get settings
+settings = get_settings()
+
+# Create provider (automatically selects based on LLM_PROVIDER env var)
+llm_provider = LLMFactory.create_provider(settings)
+
+# Make a request
+request = LLMRequest(
+    messages=[
+        {"role": "user", "content": "Hello, how are you?"}
+    ],
+    temperature=0.7,
+    max_tokens=1000
+)
+
+# Get response
+response = await llm_provider.chat_completion(request)
+print(response.content)
+```
+
+### Switching Providers
+
+Simply change the `LLM_PROVIDER` environment variable:
+
+```bash
+# Use Gemini
+LLM_PROVIDER=gemini
+
+# Use OpenAI (when implemented)
+LLM_PROVIDER=openai
+
+# Use Anthropic (when implemented)
+LLM_PROVIDER=anthropic
+```
+
+### Getting Embeddings
+
+```python
+from llm.models import EmbeddingRequest
+
+request = EmbeddingRequest(texts=["Hello world", "AI is great"])
+response = await llm_provider.get_embeddings(request)
+
+# response.embeddings is a list of embedding vectors
+for embedding in response.embeddings:
+    print(f"Embedding dimension: {len(embedding)}")
+```
+
+## Adding a New Provider
+
+1. Create a new file (e.g., `new_provider_client.py`)
+2. Inherit from `LLMProvider` and implement all abstract methods
+3. Add provider creation logic to `factory.py`
+4. Add configuration to `config/settings.py`
+5. Update `.env.example` with new provider settings
+
+Example:
+
+```python
+# llm/new_provider_client.py
+from llm.base import LLMProvider
+from llm.models import LLMRequest, LLMResponse, EmbeddingRequest, EmbeddingResponse
+
+class NewProviderClient(LLMProvider):
+    def __init__(self, api_key: str, model: str):
+        self._api_key = api_key
+        self._model_name = model
+        # Initialize your client here
+    
+    @property
+    def provider_name(self) -> str:
+        return "new_provider"
+    
+    @property
+    def model_name(self) -> str:
+        return self._model_name
+    
+    @property
+    def supports_streaming(self) -> bool:
+        return True
+    
+    async def chat_completion(self, request: LLMRequest) -> LLMResponse:
+        # Implement chat completion
+        pass
+    
+    async def get_embeddings(self, request: EmbeddingRequest) -> EmbeddingResponse:
+        # Implement embeddings
+        pass
+```
+
+## Benefits
+
+1. **Provider Agnostic**: Application code doesn't depend on specific providers
+2. **Easy Testing**: Mock providers for unit tests
+3. **Flexible Configuration**: Switch providers via environment variables
+4. **Consistent Interface**: All providers follow the same interface
+5. **Extensible**: Easy to add new providers
+
+## Configuration
+
+All provider settings are in `config/settings.py` and can be set via environment variables:
+
+- `LLM_PROVIDER` - Provider to use (gemini, openai, anthropic)
+- `GEMINI_API_KEY` - Gemini API key
+- `OPENAI_API_KEY` - OpenAI API key
+- `ANTHROPIC_API_KEY` - Anthropic API key
+- `LLM_TEMPERATURE` - Temperature for all providers
+- `LLM_MAX_TOKENS` - Max tokens for all providers
+
+See `.env.example` for all available configuration options.
diff --git a/llm/__init__.py b/llm/__init__.py
new file mode 100644
index 0000000..a80efab
--- /dev/null
+++ b/llm/__init__.py
@@ -0,0 +1,14 @@
+"""LLM integration module with provider abstraction."""
+
+from llm.base import LLMProvider
+from llm.factory import LLMFactory
+from llm.models import LLMRequest, LLMResponse, EmbeddingRequest, EmbeddingResponse
+
+__all__ = [
+    "LLMProvider",
+    "LLMFactory",
+    "LLMRequest",
+    "LLMResponse",
+    "EmbeddingRequest",
+    "EmbeddingResponse",
+]
diff --git a/llm/anthropic_client.py b/llm/anthropic_client.py
new file mode 100644
index 0000000..f4ff092
--- /dev/null
+++ b/llm/anthropic_client.py
@@ -0,0 +1,61 @@
+"""Anthropic LLM provider implementation (placeholder for future implementation)."""
+
+from typing import List, Dict, Any
+from llm.base import LLMProvider
+from llm.models import LLMRequest, LLMResponse, EmbeddingRequest, EmbeddingResponse
+
+
+class AnthropicClient(LLMProvider):
+    """Anthropic LLM provider implementation."""
+    
+    def __init__(self, api_key: str, model: str = "claude-3-5-sonnet-20241022"):
+        """Initialize Anthropic client.
+        
+        Args:
+            api_key: Anthropic API key
+            model: Model name to use
+        """
+        self._api_key = api_key
+        self._model_name = model
+        # TODO: Initialize Anthropic client when implementing
+        # import anthropic
+        # self._client = anthropic.Anthropic(api_key=api_key)
+    
+    @property
+    def provider_name(self) -> str:
+        """Return provider name."""
+        return "anthropic"
+    
+    @property
+    def model_name(self) -> str:
+        """Return model name."""
+        return self._model_name
+    
+    @property
+    def supports_streaming(self) -> bool:
+        """Anthropic supports streaming."""
+        return True
+    
+    async def chat_completion(self, request: LLMRequest) -> LLMResponse:
+        """Generate chat completion using Anthropic.
+        
+        Args:
+            request: LLM request with messages and parameters
+            
+        Returns:
+            LLMResponse with generated content
+        """
+        # TODO: Implement Anthropic API call
+        raise NotImplementedError("Anthropic client not yet implemented")
+    
+    async def get_embeddings(self, request: EmbeddingRequest) -> EmbeddingResponse:
+        """Get embeddings using Anthropic.
+        
+        Args:
+            request: Embedding request with texts
+            
+        Returns:
+            EmbeddingResponse with embeddings
+        """
+        # TODO: Implement Anthropic embedding API call
+        raise NotImplementedError("Anthropic embeddings not yet implemented")
diff --git a/llm/base.py b/llm/base.py
new file mode 100644
index 0000000..77f012f
--- /dev/null
+++ b/llm/base.py
@@ -0,0 +1,71 @@
+"""Abstract base class for LLM providers."""
+
+from abc import ABC, abstractmethod
+from typing import List, Dict, Any
+from llm.models import LLMRequest, LLMResponse, EmbeddingRequest, EmbeddingResponse
+
+
+class LLMProvider(ABC):
+    """Abstract base class for LLM providers.
+    
+    All LLM providers must implement this interface to ensure
+    consistent behavior across different providers.
+    """
+    
+    @abstractmethod
+    async def chat_completion(
+        self,
+        request: LLMRequest
+    ) -> LLMResponse:
+        """Generate chat completion.
+        
+        Args:
+            request: LLM request with messages and parameters
+            
+        Returns:
+            LLMResponse with generated content
+            
+        Raises:
+            Exception: If the API call fails
+        """
+        pass
+    
+    @abstractmethod
+    async def get_embeddings(
+        self,
+        request: EmbeddingRequest
+    ) -> EmbeddingResponse:
+        """Get embeddings for texts.
+        
+        Args:
+            request: Embedding request with texts
+            
+        Returns:
+            EmbeddingResponse with embeddings
+            
+        Raises:
+            Exception: If the API call fails
+        """
+        pass
+    
+    @property
+    @abstractmethod
+    def provider_name(self) -> str:
+        """Return provider name (e.g., 'gemini', 'openai', 'anthropic')."""
+        pass
+    
+    @property
+    @abstractmethod
+    def model_name(self) -> str:
+        """Return model name being used."""
+        pass
+    
+    @property
+    @abstractmethod
+    def supports_streaming(self) -> bool:
+        """Return whether this provider supports streaming responses."""
+        pass
+    
+    def __repr__(self) -> str:
+        """String representation."""
+        return f"{self.__class__.__name__}(provider={self.provider_name}, model={self.model_name})"
diff --git a/llm/factory.py b/llm/factory.py
new file mode 100644
index 0000000..72e8fa4
--- /dev/null
+++ b/llm/factory.py
@@ -0,0 +1,116 @@
+"""Factory for creating LLM provider instances."""
+
+from typing import Optional, List
+from llm.base import LLMProvider
+from config.settings import Settings
+
+
+class LLMFactory:
+    """Factory class for creating LLM provider instances.
+    
+    This factory abstracts the creation of different LLM providers,
+    allowing easy switching between providers via configuration.
+    """
+    
+    @staticmethod
+    def create_provider(settings: Settings) -> LLMProvider:
+        """Create an LLM provider based on settings.
+        
+        Args:
+            settings: Application settings with provider configuration
+            
+        Returns:
+            LLMProvider instance
+            
+        Raises:
+            ValueError: If provider is not supported or not configured
+        """
+        provider = settings.llm_provider.lower()
+        
+        if provider == "gemini":
+            from llm.gemini_client import GeminiClient
+            if not settings.gemini_api_key:
+                raise ValueError("Gemini API key is required but not set")
+            return GeminiClient(
+                api_key=settings.gemini_api_key,
+                model=settings.gemini_model
+            )
+        
+        elif provider == "openai":
+            from llm.openai_client import OpenAIClient
+            if not settings.openai_api_key:
+                raise ValueError("OpenAI API key is required but not set")
+            return OpenAIClient(
+                api_key=settings.openai_api_key,
+                model=settings.openai_model
+            )
+        
+        elif provider == "anthropic":
+            from llm.anthropic_client import AnthropicClient
+            if not settings.anthropic_api_key:
+                raise ValueError("Anthropic API key is required but not set")
+            return AnthropicClient(
+                api_key=settings.anthropic_api_key,
+                model=settings.anthropic_model
+            )
+        
+        elif provider == "ollama":
+            from llm.ollama_client import OllamaClient
+            return OllamaClient(
+                base_url=settings.ollama_base_url,
+                chat_model=settings.ollama_chat_model,
+                embedding_model=settings.ollama_embedding_model
+            )
+        
+        else:
+            raise ValueError(
+                f"Unsupported LLM provider: {provider}. "
+                f"Supported providers: gemini, openai, anthropic, ollama"
+            )
+    
+    @staticmethod
+    def get_available_providers() -> List[str]:
+        """Get list of available provider names.
+        
+        Returns:
+            List of provider names
+        """
+        return ["gemini", "openai", "anthropic", "ollama"]
+    
+    @staticmethod
+    def create_embedding_provider(settings: Settings) -> LLMProvider:
+        """Create an embedding provider (can be different from LLM provider).
+        
+        Args:
+            settings: Application settings with provider configuration
+            
+        Returns:
+            LLMProvider instance for embeddings
+            
+        Raises:
+            ValueError: If provider is not supported or not configured
+        """
+        provider = settings.embedding_provider.lower()
+        
+        if provider == "gemini":
+            from llm.gemini_client import GeminiClient
+            if not settings.gemini_api_key:
+                raise ValueError("Gemini API key is required but not set")
+            return GeminiClient(
+                api_key=settings.gemini_api_key,
+                model=settings.gemini_model
+            )
+        
+        elif provider == "ollama":
+            from llm.ollama_client import OllamaClient
+            return OllamaClient(
+                base_url=settings.ollama_base_url,
+                chat_model=settings.ollama_chat_model,
+                embedding_model=settings.ollama_embedding_model
+            )
+        
+        else:
+            raise ValueError(
+                f"Unsupported embedding provider: {provider}. "
+                f"Supported providers: gemini, ollama"
+            )
diff --git a/llm/gemini_client.py b/llm/gemini_client.py
new file mode 100644
index 0000000..b194b66
--- /dev/null
+++ b/llm/gemini_client.py
@@ -0,0 +1,158 @@
+"""Gemini LLM provider implementation using google-genai package."""
+
+import os
+from typing import List, Dict, Any
+from google import genai
+from llm.base import LLMProvider
+from llm.models import LLMRequest, LLMResponse, EmbeddingRequest, EmbeddingResponse
+
+
+class GeminiClient(LLMProvider):
+    """Gemini LLM provider implementation using google-genai."""
+    
+    def __init__(self, api_key: str, model: str = "gemini-2.5-pro"):
+        """Initialize Gemini client.
+        
+        Args:
+            api_key: Gemini API key
+            model: Model name to use
+        """
+        self._api_key = api_key
+        self._model_name = model
+        self._client = genai.Client(api_key=api_key)
+    
+    @property
+    def provider_name(self) -> str:
+        """Return provider name."""
+        return "gemini"
+    
+    @property
+    def model_name(self) -> str:
+        """Return model name."""
+        return self._model_name
+    
+    @property
+    def supports_streaming(self) -> bool:
+        """Gemini supports streaming."""
+        return True
+    
+    async def chat_completion(self, request: LLMRequest) -> LLMResponse:
+        """Generate chat completion using Gemini.
+        
+        Args:
+            request: LLM request with messages and parameters
+            
+        Returns:
+            LLMResponse with generated content
+        """
+        try:
+            # Convert messages to format expected by google-genai
+            contents = []
+            system_instruction = None
+            
+            for msg in request.messages:
+                role = msg.get("role", "user")
+                content = msg.get("content", "")
+                
+                if role == "system":
+                    # Store system instruction - will prepend to first user message
+                    system_instruction = content
+                elif role == "user":
+                    # Prepend system instruction to first user message if present
+                    user_content = content
+                    if system_instruction and not contents:
+                        user_content = f"{system_instruction}\n\n{content}"
+                        system_instruction = None  # Clear after using
+                    contents.append({"role": "user", "parts": [{"text": user_content}]})
+                elif role == "assistant":
+                    contents.append({"role": "model", "parts": [{"text": content}]})
+            
+            # Prepare generation config
+            config = {
+                "temperature": request.temperature,
+                "max_output_tokens": request.max_tokens,
+            }
+            
+            if request.top_p is not None:
+                config["top_p"] = request.top_p
+            
+            # Generate content using google-genai
+            # Note: system_instruction is not a direct parameter, handled above
+            response = self._client.models.generate_content(
+                model=self._model_name,
+                contents=contents,
+                config=config
+            )
+            
+            # Extract response
+            content = response.text if hasattr(response, 'text') else str(response)
+            
+            # Build usage info if available
+            usage = None
+            if hasattr(response, 'usage_metadata'):
+                usage_metadata = response.usage_metadata
+                usage = {
+                    "prompt_tokens": getattr(usage_metadata, 'prompt_token_count', 0),
+                    "completion_tokens": getattr(usage_metadata, 'completion_token_count', 0),
+                    "total_tokens": getattr(usage_metadata, 'total_token_count', 0),
+                }
+            
+            return LLMResponse(
+                content=content,
+                model=self._model_name,
+                provider=self.provider_name,
+                usage=usage,
+                finish_reason=getattr(response, 'finish_reason', None)
+            )
+            
+        except Exception as e:
+            raise Exception(f"Gemini API error: {str(e)}")
+    
+    async def get_embeddings(self, request: EmbeddingRequest) -> EmbeddingResponse:
+        """Get embeddings using Gemini.
+        
+        Args:
+            request: Embedding request with texts
+            
+        Returns:
+            EmbeddingResponse with embeddings
+        """
+        try:
+            embeddings = []
+            
+            for text in request.texts:
+                # Use the embedding API with google-genai
+                # The correct API call structure for google-genai package
+                response = self._client.models.embed_content(
+                    model="text-embedding-004",
+                    contents=text  # Note: 'contents' (plural) is the correct parameter
+                )
+                
+                # Extract embedding from EmbedContentResponse
+                # Response has 'embeddings' attribute (list of Embedding objects)
+                if hasattr(response, 'embeddings') and response.embeddings:
+                    # embeddings is a list, get the first one
+                    embedding_obj = response.embeddings[0]
+                    # Each embedding has 'values' attribute (list of floats)
+                    if hasattr(embedding_obj, 'values'):
+                        embeddings.append(list(embedding_obj.values))
+                    elif isinstance(embedding_obj, list):
+                        embeddings.append(embedding_obj)
+                    else:
+                        embeddings.append(list(embedding_obj))
+                else:
+                    raise ValueError(
+                        f"No embeddings found in response. "
+                        f"Response type: {type(response)}, "
+                        f"Has 'embeddings': {hasattr(response, 'embeddings')}"
+                    )
+            
+            return EmbeddingResponse(
+                embeddings=embeddings,
+                model="text-embedding-004",
+                provider=self.provider_name,
+                usage=None  # Gemini embedding API doesn't provide usage
+            )
+            
+        except Exception as e:
+            raise Exception(f"Gemini embedding API error: {str(e)}")
diff --git a/llm/models.py b/llm/models.py
new file mode 100644
index 0000000..c6391cb
--- /dev/null
+++ b/llm/models.py
@@ -0,0 +1,37 @@
+"""Common models for LLM providers."""
+
+from typing import List, Dict, Any, Optional
+from pydantic import BaseModel
+
+
+class LLMRequest(BaseModel):
+    """Request model for LLM chat completion."""
+    messages: List[Dict[str, str]]
+    temperature: float = 0.7
+    max_tokens: int = 2000
+    top_p: Optional[float] = None
+    frequency_penalty: Optional[float] = None
+    presence_penalty: Optional[float] = None
+
+
+class LLMResponse(BaseModel):
+    """Response model for LLM chat completion."""
+    content: str
+    model: str
+    provider: str
+    usage: Optional[Dict[str, Any]] = None
+    finish_reason: Optional[str] = None
+
+
+class EmbeddingRequest(BaseModel):
+    """Request model for embeddings."""
+    texts: List[str]
+    model: Optional[str] = None
+
+
+class EmbeddingResponse(BaseModel):
+    """Response model for embeddings."""
+    embeddings: List[List[float]]
+    model: str
+    provider: str
+    usage: Optional[Dict[str, Any]] = None
diff --git a/llm/ollama_client.py b/llm/ollama_client.py
new file mode 100644
index 0000000..7d381ed
--- /dev/null
+++ b/llm/ollama_client.py
@@ -0,0 +1,174 @@
+"""Ollama LLM provider implementation."""
+
+import httpx
+from typing import List, Dict, Any
+from llm.base import LLMProvider
+from llm.models import LLMRequest, LLMResponse, EmbeddingRequest, EmbeddingResponse
+
+
+class OllamaClient(LLMProvider):
+    """Ollama LLM provider implementation for local inference."""
+    
+    def __init__(
+        self,
+        base_url: str = "http://localhost:11434",
+        chat_model: str = "llama3",
+        embedding_model: str = "nomic-embed-text"
+    ):
+        """Initialize Ollama client.
+        
+        Args:
+            base_url: Ollama server URL
+            chat_model: Model name for chat completion
+            embedding_model: Model name for embeddings
+        """
+        self._base_url = base_url.rstrip('/')
+        self._chat_model = chat_model
+        self._embedding_model = embedding_model
+        self._client = httpx.AsyncClient(timeout=60.0)
+    
+    @property
+    def provider_name(self) -> str:
+        """Return provider name."""
+        return "ollama"
+    
+    @property
+    def model_name(self) -> str:
+        """Return model name."""
+        return self._chat_model
+    
+    @property
+    def supports_streaming(self) -> bool:
+        """Ollama supports streaming."""
+        return True
+    
+    async def chat_completion(self, request: LLMRequest) -> LLMResponse:
+        """Generate chat completion using Ollama.
+        
+        Args:
+            request: LLM request with messages and parameters
+            
+        Returns:
+            LLMResponse with generated content
+        """
+        try:
+            # Convert messages to Ollama format
+            messages = []
+            for msg in request.messages:
+                role = msg.get("role", "user")
+                content = msg.get("content", "")
+                
+                # Ollama uses 'system', 'user', 'assistant' roles
+                if role in ["system", "user", "assistant"]:
+                    messages.append({"role": role, "content": content})
+                elif role == "model":
+                    # Convert 'model' to 'assistant' for Ollama
+                    messages.append({"role": "assistant", "content": content})
+            
+            # Prepare request
+            payload = {
+                "model": self._chat_model,
+                "messages": messages,
+                "options": {
+                    "temperature": request.temperature,
+                    "num_predict": request.max_tokens,
+                }
+            }
+            
+            if request.top_p is not None:
+                payload["options"]["top_p"] = request.top_p
+            
+            # Make API call
+            response = await self._client.post(
+                f"{self._base_url}/api/chat",
+                json=payload
+            )
+            response.raise_for_status()
+            result = response.json()
+            
+            # Extract response
+            content = result.get("message", {}).get("content", "")
+            
+            # Build usage info if available
+            usage = None
+            if "prompt_eval_count" in result or "eval_count" in result:
+                usage = {
+                    "prompt_tokens": result.get("prompt_eval_count", 0),
+                    "completion_tokens": result.get("eval_count", 0),
+                    "total_tokens": result.get("prompt_eval_count", 0) + result.get("eval_count", 0),
+                }
+            
+            return LLMResponse(
+                content=content,
+                model=self._chat_model,
+                provider=self.provider_name,
+                usage=usage,
+                finish_reason=result.get("done_reason")
+            )
+            
+        except httpx.HTTPError as e:
+            raise Exception(f"Ollama API error: {str(e)}")
+        except Exception as e:
+            raise Exception(f"Ollama error: {str(e)}")
+    
+    async def get_embeddings(self, request: EmbeddingRequest) -> EmbeddingResponse:
+        """Get embeddings using Ollama.
+        
+        Args:
+            request: Embedding request with texts
+            
+        Returns:
+            EmbeddingResponse with embeddings
+        """
+        try:
+            embeddings = []
+            
+            # Ollama processes one text at a time
+            for text in request.texts:
+                payload = {
+                    "model": self._embedding_model,
+                    "prompt": text
+                }
+                
+                response = await self._client.post(
+                    f"{self._base_url}/api/embeddings",
+                    json=payload
+                )
+                response.raise_for_status()
+                result = response.json()
+                
+                # Extract embedding
+                embedding = result.get("embedding", [])
+                if not embedding:
+                    raise ValueError(f"No embedding returned for text: {text[:50]}...")
+                
+                embeddings.append(embedding)
+            
+            return EmbeddingResponse(
+                embeddings=embeddings,
+                model=self._embedding_model,
+                provider=self.provider_name,
+                usage=None  # Ollama doesn't provide detailed usage for embeddings
+            )
+            
+        except httpx.HTTPError as e:
+            raise Exception(f"Ollama embedding API error: {str(e)}")
+        except Exception as e:
+            raise Exception(f"Ollama embedding error: {str(e)}")
+    
+    async def __aenter__(self):
+        """Async context manager entry."""
+        return self
+    
+    async def __aexit__(self, exc_type, exc_val, exc_tb):
+        """Async context manager exit."""
+        await self._client.aclose()
+    
+    def __del__(self):
+        """Cleanup on deletion."""
+        try:
+            if hasattr(self, '_client'):
+                # Note: httpx client cleanup should be done explicitly
+                pass
+        except:
+            pass
diff --git a/llm/openai_client.py b/llm/openai_client.py
new file mode 100644
index 0000000..794091e
--- /dev/null
+++ b/llm/openai_client.py
@@ -0,0 +1,61 @@
+"""OpenAI LLM provider implementation (placeholder for future implementation)."""
+
+from typing import List, Dict, Any
+from llm.base import LLMProvider
+from llm.models import LLMRequest, LLMResponse, EmbeddingRequest, EmbeddingResponse
+
+
+class OpenAIClient(LLMProvider):
+    """OpenAI LLM provider implementation."""
+    
+    def __init__(self, api_key: str, model: str = "gpt-4"):
+        """Initialize OpenAI client.
+        
+        Args:
+            api_key: OpenAI API key
+            model: Model name to use
+        """
+        self._api_key = api_key
+        self._model_name = model
+        # TODO: Initialize OpenAI client when implementing
+        # import openai
+        # self._client = openai.OpenAI(api_key=api_key)
+    
+    @property
+    def provider_name(self) -> str:
+        """Return provider name."""
+        return "openai"
+    
+    @property
+    def model_name(self) -> str:
+        """Return model name."""
+        return self._model_name
+    
+    @property
+    def supports_streaming(self) -> bool:
+        """OpenAI supports streaming."""
+        return True
+    
+    async def chat_completion(self, request: LLMRequest) -> LLMResponse:
+        """Generate chat completion using OpenAI.
+        
+        Args:
+            request: LLM request with messages and parameters
+            
+        Returns:
+            LLMResponse with generated content
+        """
+        # TODO: Implement OpenAI API call
+        raise NotImplementedError("OpenAI client not yet implemented")
+    
+    async def get_embeddings(self, request: EmbeddingRequest) -> EmbeddingResponse:
+        """Get embeddings using OpenAI.
+        
+        Args:
+            request: Embedding request with texts
+            
+        Returns:
+            EmbeddingResponse with embeddings
+        """
+        # TODO: Implement OpenAI embedding API call
+        raise NotImplementedError("OpenAI embeddings not yet implemented")
diff --git a/mcp_servers/__init__.py b/mcp_servers/__init__.py
new file mode 100644
index 0000000..7d6bb1d
--- /dev/null
+++ b/mcp_servers/__init__.py
@@ -0,0 +1,3 @@
+"""MCP Servers module."""
+
+__version__ = "1.0.0"
diff --git a/mcp_servers/base_server.py b/mcp_servers/base_server.py
new file mode 100644
index 0000000..22b04fc
--- /dev/null
+++ b/mcp_servers/base_server.py
@@ -0,0 +1,214 @@
+"""Base MCP server with HTTP + Auth + Versioning."""
+
+import logging
+from typing import List, Dict, Any, Optional
+from fastapi import FastAPI, Request, HTTPException, Header
+from fastapi.responses import JSONResponse
+from fastapi.middleware.cors import CORSMiddleware
+import uuid
+from datetime import datetime
+from config.settings import get_settings
+
+# Setup logging
+logger = logging.getLogger(__name__)
+
+# Version metadata
+SERVER_VERSION = "1.0.0"
+PROTOCOL_VERSION = "2024-11-05"  # MCP protocol version
+
+
+class BaseMCPServer:
+    """Base class for MCP servers with HTTP + Auth + Versioning."""
+    
+    def __init__(self, server_name: str, tools: List[Dict[str, Any]]):
+        """Initialize base MCP server.
+        
+        Args:
+            server_name: Name of the server
+            tools: List of tool definitions with versioning
+        """
+        self.server_name = server_name
+        self.tools = tools
+        self.settings = get_settings()
+        self.app = FastAPI(title=f"{server_name} MCP Server")
+        
+        # Add CORS middleware
+        self.app.add_middleware(
+            CORSMiddleware,
+            allow_origins=["*"],  # Configure as needed
+            allow_credentials=True,
+            allow_methods=["*"],
+            allow_headers=["*"],
+        )
+        
+        # Add authentication middleware
+        self._setup_middleware()
+        
+        # Setup routes
+        self._setup_routes()
+    
+    def _setup_middleware(self):
+        """Setup authentication middleware."""
+        
+        @self.app.middleware("http")
+        async def verify_mcp_key(request: Request, call_next):
+            """Verify MCP API key if configured."""
+            if self.settings.mcp_api_key:
+                api_key = request.headers.get("X-MCP-KEY")
+                if api_key != self.settings.mcp_api_key:
+                    return JSONResponse(
+                        {"error": "Unauthorized", "message": "Invalid MCP API key"},
+                        status_code=401
+                    )
+            return await call_next(request)
+    
+    def _setup_routes(self):
+        """Setup HTTP routes."""
+        
+        @self.app.get("/health")
+        async def health():
+            """Health check endpoint with version info."""
+            return {
+                "status": "healthy",
+                "server_name": self.server_name,
+                "server_version": SERVER_VERSION,
+                "protocol_version": PROTOCOL_VERSION,
+                "timestamp": datetime.utcnow().isoformat()
+            }
+        
+        @self.app.get("/tools")
+        async def list_tools(
+            x_mcp_key: Optional[str] = Header(None, alias="X-MCP-KEY")
+        ):
+            """List all available tools with versioning."""
+            return {
+                "server_name": self.server_name,
+                "server_version": SERVER_VERSION,
+                "protocol_version": PROTOCOL_VERSION,
+                "tools": self.tools
+            }
+        
+        @self.app.post("/execute")
+        async def execute_tool(
+            request: Request,
+            x_mcp_key: Optional[str] = Header(None, alias="X-MCP-KEY"),
+            x_request_id: Optional[str] = Header(None, alias="X-Request-ID")
+        ):
+            """Execute a tool using JSON-RPC 2.0 format.
+            
+            Expected JSON-RPC 2.0 request:
+            {
+                "jsonrpc": "2.0",
+                "id": "unique-id",
+                "method": "tool_name",
+                "params": {...}
+            }
+            """
+            try:
+                # Get request ID from header or generate new one
+                request_id = x_request_id or str(uuid.uuid4())
+                
+                # Parse JSON-RPC 2.0 request
+                body = await request.json()
+                
+                # Validate JSON-RPC 2.0 format
+                if body.get("jsonrpc") != "2.0":
+                    return JSONResponse({
+                        "jsonrpc": "2.0",
+                        "id": body.get("id"),
+                        "error": {
+                            "code": -32600,
+                            "message": "Invalid Request",
+                            "data": "jsonrpc must be '2.0'"
+                        }
+                    }, status_code=400)
+                
+                method = body.get("method")
+                params = body.get("params", {})
+                request_id_jsonrpc = body.get("id")
+                
+                if not method:
+                    return JSONResponse({
+                        "jsonrpc": "2.0",
+                        "id": request_id_jsonrpc,
+                        "error": {
+                            "code": -32600,
+                            "message": "Invalid Request",
+                            "data": "method is required"
+                        }
+                    }, status_code=400)
+                
+                # Execute tool
+                result = await self._execute_tool_internal(
+                    method=method,
+                    params=params,
+                    request_id=request_id
+                )
+                
+                # Return JSON-RPC 2.0 response
+                return {
+                    "jsonrpc": "2.0",
+                    "id": request_id_jsonrpc,
+                    "result": result,
+                    "metadata": {
+                        "request_id": request_id,
+                        "server_version": SERVER_VERSION,
+                        "timestamp": datetime.utcnow().isoformat()
+                    }
+                }
+                
+            except ValueError as e:
+                # Return JSON-RPC 2.0 error response for validation errors
+                # This is expected behavior for invalid requests (e.g., read-only enforcement)
+                logger.info(
+                    f"Validation error for method '{method}': {str(e)[:100]}",
+                    extra={"request_id": request_id, "method": method}
+                )
+                return JSONResponse({
+                    "jsonrpc": "2.0",
+                    "id": body.get("id") if 'body' in locals() else None,
+                    "error": {
+                        "code": -32602,
+                        "message": "Invalid params",
+                        "data": str(e)
+                    }
+                }, status_code=400)
+            except Exception as e:
+                # Return JSON-RPC 2.0 error response for unexpected errors
+                logger.error(
+                    f"Internal error executing method '{method}': {str(e)}",
+                    extra={"request_id": request_id, "method": method},
+                    exc_info=True
+                )
+                return JSONResponse({
+                    "jsonrpc": "2.0",
+                    "id": body.get("id") if 'body' in locals() else None,
+                    "error": {
+                        "code": -32603,
+                        "message": "Internal Error",
+                        "data": str(e)
+                    }
+                }, status_code=500)
+    
+    async def _execute_tool_internal(
+        self,
+        method: str,
+        params: Dict[str, Any],
+        request_id: str
+    ) -> Any:
+        """Execute tool internally. Override in subclasses.
+        
+        Args:
+            method: Tool method name
+            params: Tool parameters
+            request_id: Request ID for correlation
+            
+        Returns:
+            Tool execution result
+            
+        Raises:
+            NotImplementedError: If not overridden
+        """
+        raise NotImplementedError(
+            "Subclasses must implement _execute_tool_internal"
+        )
diff --git a/mcp_servers/catalog_server/__init__.py b/mcp_servers/catalog_server/__init__.py
new file mode 100644
index 0000000..47b1321
--- /dev/null
+++ b/mcp_servers/catalog_server/__init__.py
@@ -0,0 +1 @@
+"""Catalog MCP server."""
diff --git a/mcp_servers/catalog_server/database.py b/mcp_servers/catalog_server/database.py
new file mode 100644
index 0000000..ed519c9
--- /dev/null
+++ b/mcp_servers/catalog_server/database.py
@@ -0,0 +1,80 @@
+"""SQLite database operations for catalog server."""
+
+import aiosqlite
+from typing import List, Dict, Any, Optional
+from pathlib import Path
+from config.settings import get_settings
+
+
+class CatalogDatabase:
+    """Database operations for catalog server."""
+    
+    def __init__(self, db_path: Optional[str] = None):
+        """Initialize catalog database.
+        
+        Args:
+            db_path: Path to SQLite database file
+        """
+        self.settings = get_settings()
+        self.db_path = db_path or self.settings.database_path
+        # Ensure directory exists
+        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
+    
+    async def list_tables(self) -> List[str]:
+        """List all tables in the database.
+        
+        Returns:
+            List of table names
+        """
+        async with aiosqlite.connect(self.db_path) as db:
+            cursor = await db.execute(
+                "SELECT name FROM sqlite_master WHERE type='table' ORDER BY name"
+            )
+            rows = await cursor.fetchall()
+            return [row[0] for row in rows]
+    
+    async def describe_table(self, table_name: str) -> Dict[str, Any]:
+        """Get schema information for a table.
+        
+        Args:
+            table_name: Name of the table
+            
+        Returns:
+            Dictionary with table schema information
+        """
+        async with aiosqlite.connect(self.db_path) as db:
+            # Get table info
+            cursor = await db.execute(
+                f"PRAGMA table_info({table_name})"
+            )
+            columns = await cursor.fetchall()
+            
+            # Get column details
+            column_info = []
+            for col in columns:
+                column_info.append({
+                    "name": col[1],
+                    "type": col[2],
+                    "not_null": bool(col[3]),
+                    "default_value": col[4],
+                    "primary_key": bool(col[5])
+                })
+            
+            return {
+                "table_name": table_name,
+                "columns": column_info
+            }
+    
+    async def get_table_row_count(self, table_name: str) -> int:
+        """Get the number of rows in a table.
+        
+        Args:
+            table_name: Name of the table
+            
+        Returns:
+            Number of rows
+        """
+        async with aiosqlite.connect(self.db_path) as db:
+            cursor = await db.execute(f"SELECT COUNT(*) FROM {table_name}")
+            row = await cursor.fetchone()
+            return row[0] if row else 0
diff --git a/mcp_servers/catalog_server/server.py b/mcp_servers/catalog_server/server.py
new file mode 100644
index 0000000..abfe91e
--- /dev/null
+++ b/mcp_servers/catalog_server/server.py
@@ -0,0 +1,83 @@
+"""Catalog MCP server implementation."""
+
+import uvicorn
+from typing import Dict, Any
+from mcp_servers.base_server import BaseMCPServer
+from mcp_servers.catalog_server.tools import get_tools
+from mcp_servers.catalog_server.database import CatalogDatabase
+from config.settings import get_settings
+
+
+class CatalogMCPServer(BaseMCPServer):
+    """Catalog MCP server for database catalog operations."""
+    
+    def __init__(self):
+        """Initialize catalog MCP server."""
+        self.db = CatalogDatabase()
+        super().__init__(
+            server_name="Catalog MCP Server",
+            tools=get_tools()
+        )
+    
+    async def _execute_tool_internal(
+        self,
+        method: str,
+        params: Dict[str, Any],
+        request_id: str
+    ) -> Any:
+        """Execute catalog tool.
+        
+        Args:
+            method: Tool method name
+            params: Tool parameters
+            request_id: Request ID for correlation
+            
+        Returns:
+            Tool execution result
+        """
+        if method == "list_tables":
+            tables = await self.db.list_tables()
+            return {
+                "tables": tables,
+                "count": len(tables)
+            }
+        
+        elif method == "describe_table":
+            table_name = params.get("table_name")
+            if not table_name:
+                raise ValueError("table_name parameter is required")
+            return await self.db.describe_table(table_name)
+        
+        elif method == "get_table_row_count":
+            table_name = params.get("table_name")
+            if not table_name:
+                raise ValueError("table_name parameter is required")
+            count = await self.db.get_table_row_count(table_name)
+            return {
+                "table_name": table_name,
+                "row_count": count
+            }
+        
+        else:
+            raise ValueError(f"Unknown tool method: {method}")
+
+
+def create_app():
+    """Create FastAPI app instance."""
+    server = CatalogMCPServer()
+    return server.app
+
+
+def run_server():
+    """Run the catalog MCP server."""
+    settings = get_settings()
+    uvicorn.run(
+        "mcp_servers.catalog_server.server:create_app",
+        host="0.0.0.0",
+        port=settings.catalog_mcp_port,
+        reload=False
+    )
+
+
+if __name__ == "__main__":
+    run_server()
diff --git a/mcp_servers/catalog_server/tools.py b/mcp_servers/catalog_server/tools.py
new file mode 100644
index 0000000..4515f25
--- /dev/null
+++ b/mcp_servers/catalog_server/tools.py
@@ -0,0 +1,56 @@
+"""Catalog server tools with versioning."""
+
+from typing import List, Dict, Any
+
+# Tool version
+TOOL_VERSION = "1.0.0"
+
+
+def get_tools() -> List[Dict[str, Any]]:
+    """Get list of catalog tools with versioning.
+    
+    Returns:
+        List of tool definitions with version metadata
+    """
+    return [
+        {
+            "name": "list_tables",
+            "tool_version": TOOL_VERSION,
+            "description": "List all tables in the database",
+            "inputSchema": {
+                "type": "object",
+                "properties": {},
+                "required": []
+            }
+        },
+        {
+            "name": "describe_table",
+            "tool_version": TOOL_VERSION,
+            "description": "Get schema information for a specific table",
+            "inputSchema": {
+                "type": "object",
+                "properties": {
+                    "table_name": {
+                        "type": "string",
+                        "description": "Name of the table to describe"
+                    }
+                },
+                "required": ["table_name"]
+            }
+        },
+        {
+            "name": "get_table_row_count",
+            "tool_version": TOOL_VERSION,
+            "description": "Get the number of rows in a table",
+            "inputSchema": {
+                "type": "object",
+                "properties": {
+                    "table_name": {
+                        "type": "string",
+                        "description": "Name of the table"
+                    }
+                },
+                "required": ["table_name"]
+            }
+        }
+    ]
diff --git a/mcp_servers/sql_query_server/__init__.py b/mcp_servers/sql_query_server/__init__.py
new file mode 100644
index 0000000..27eb9c4
--- /dev/null
+++ b/mcp_servers/sql_query_server/__init__.py
@@ -0,0 +1 @@
+"""SQL Query MCP server."""
diff --git a/mcp_servers/sql_query_server/query_engine.py b/mcp_servers/sql_query_server/query_engine.py
new file mode 100644
index 0000000..45cff97
--- /dev/null
+++ b/mcp_servers/sql_query_server/query_engine.py
@@ -0,0 +1,106 @@
+"""SQL query engine with read-only enforcement."""
+
+import aiosqlite
+from typing import List, Dict, Any, Optional
+from pathlib import Path
+from config.settings import get_settings
+
+
+# Read-only SQL keywords
+READ_ONLY_KEYWORDS = [
+    "INSERT", "UPDATE", "DELETE", "DROP", "CREATE", "ALTER",
+    "TRUNCATE", "REPLACE", "GRANT", "REVOKE", "COMMIT", "ROLLBACK"
+]
+
+
+def validate_read_only(query: str) -> None:
+    """Validate that query is read-only.
+    
+    Args:
+        query: SQL query string
+        
+    Raises:
+        ValueError: If query contains write operations
+    """
+    query_upper = query.upper().strip()
+    
+    # Check for read-only keywords
+    for keyword in READ_ONLY_KEYWORDS:
+        if query_upper.startswith(keyword):
+            raise ValueError(
+                f"Read-only mode: {keyword} operations are not allowed. "
+                f"Only SELECT queries are permitted."
+            )
+    
+    # Additional check: ensure it's a SELECT query
+    if not query_upper.startswith("SELECT"):
+        raise ValueError(
+            "Read-only mode: Only SELECT queries are allowed. "
+            f"Query starts with: {query_upper.split()[0] if query_upper.split() else 'empty'}"
+        )
+
+
+class SQLQueryEngine:
+    """SQL query engine with read-only enforcement."""
+    
+    def __init__(self, db_path: Optional[str] = None):
+        """Initialize SQL query engine.
+        
+        Args:
+            db_path: Path to SQLite database file
+        """
+        self.settings = get_settings()
+        self.db_path = db_path or self.settings.database_path
+    
+    async def execute_query(self, query: str) -> Dict[str, Any]:
+        """Execute a read-only SQL query.
+        
+        Args:
+            query: SQL SELECT query
+            
+        Returns:
+            Dictionary with query results
+            
+        Raises:
+            ValueError: If query is not read-only
+        """
+        # Validate read-only
+        validate_read_only(query)
+        
+        async with aiosqlite.connect(self.db_path) as db:
+            # Enable row factory for dict-like access
+            db.row_factory = aiosqlite.Row
+            
+            cursor = await db.execute(query)
+            rows = await cursor.fetchall()
+            
+            # Convert rows to list of dictionaries
+            results = [dict(row) for row in rows]
+            
+            return {
+                "query": query,
+                "row_count": len(results),
+                "results": results
+            }
+    
+    async def explain_query(self, query: str) -> Dict[str, Any]:
+        """Get query execution plan (EXPLAIN QUERY PLAN).
+        
+        Args:
+            query: SQL SELECT query
+            
+        Returns:
+            Dictionary with execution plan
+        """
+        # Validate read-only
+        validate_read_only(query)
+        
+        async with aiosqlite.connect(self.db_path) as db:
+            db.row_factory = aiosqlite.Row
+            cursor = await db.execute(f"EXPLAIN QUERY PLAN {query}")
+            plan = await cursor.fetchall()
+            
+            return {
+                "query": query,
+                "execution_plan": [dict(row) for row in plan]
+            }
diff --git a/mcp_servers/sql_query_server/server.py b/mcp_servers/sql_query_server/server.py
new file mode 100644
index 0000000..a67d055
--- /dev/null
+++ b/mcp_servers/sql_query_server/server.py
@@ -0,0 +1,72 @@
+"""SQL Query MCP server implementation."""
+
+import uvicorn
+from typing import Dict, Any
+from mcp_servers.base_server import BaseMCPServer
+from mcp_servers.sql_query_server.tools import get_tools
+from mcp_servers.sql_query_server.query_engine import SQLQueryEngine
+from config.settings import get_settings
+
+
+class SQLQueryMCPServer(BaseMCPServer):
+    """SQL Query MCP server for read-only SQL queries."""
+    
+    def __init__(self):
+        """Initialize SQL Query MCP server."""
+        self.query_engine = SQLQueryEngine()
+        super().__init__(
+            server_name="SQL Query MCP Server",
+            tools=get_tools()
+        )
+    
+    async def _execute_tool_internal(
+        self,
+        method: str,
+        params: Dict[str, Any],
+        request_id: str
+    ) -> Any:
+        """Execute SQL query tool.
+        
+        Args:
+            method: Tool method name
+            params: Tool parameters
+            request_id: Request ID for correlation
+            
+        Returns:
+            Tool execution result
+        """
+        if method == "execute_query":
+            query = params.get("query")
+            if not query:
+                raise ValueError("query parameter is required")
+            return await self.query_engine.execute_query(query)
+        
+        elif method == "explain_query":
+            query = params.get("query")
+            if not query:
+                raise ValueError("query parameter is required")
+            return await self.query_engine.explain_query(query)
+        
+        else:
+            raise ValueError(f"Unknown tool method: {method}")
+
+
+def create_app():
+    """Create FastAPI app instance."""
+    server = SQLQueryMCPServer()
+    return server.app
+
+
+def run_server():
+    """Run the SQL Query MCP server."""
+    settings = get_settings()
+    uvicorn.run(
+        "mcp_servers.sql_query_server.server:create_app",
+        host="0.0.0.0",
+        port=settings.sql_mcp_port,
+        reload=False
+    )
+
+
+if __name__ == "__main__":
+    run_server()
diff --git a/mcp_servers/sql_query_server/tools.py b/mcp_servers/sql_query_server/tools.py
new file mode 100644
index 0000000..fe5ed14
--- /dev/null
+++ b/mcp_servers/sql_query_server/tools.py
@@ -0,0 +1,46 @@
+"""SQL Query server tools with versioning."""
+
+from typing import List, Dict, Any
+
+# Tool version
+TOOL_VERSION = "1.0.0"
+
+
+def get_tools() -> List[Dict[str, Any]]:
+    """Get list of SQL query tools with versioning.
+    
+    Returns:
+        List of tool definitions with version metadata
+    """
+    return [
+        {
+            "name": "execute_query",
+            "tool_version": TOOL_VERSION,
+            "description": "Execute a read-only SQL SELECT query",
+            "inputSchema": {
+                "type": "object",
+                "properties": {
+                    "query": {
+                        "type": "string",
+                        "description": "SQL SELECT query to execute (read-only)"
+                    }
+                },
+                "required": ["query"]
+            }
+        },
+        {
+            "name": "explain_query",
+            "tool_version": TOOL_VERSION,
+            "description": "Get execution plan for a SQL SELECT query",
+            "inputSchema": {
+                "type": "object",
+                "properties": {
+                    "query": {
+                        "type": "string",
+                        "description": "SQL SELECT query to explain (read-only)"
+                    }
+                },
+                "required": ["query"]
+            }
+        }
+    ]
diff --git a/mcp_servers/vector_search_server/__init__.py b/mcp_servers/vector_search_server/__init__.py
new file mode 100644
index 0000000..88eacca
--- /dev/null
+++ b/mcp_servers/vector_search_server/__init__.py
@@ -0,0 +1 @@
+"""Vector Search MCP server."""
diff --git a/mcp_servers/vector_search_server/server.py b/mcp_servers/vector_search_server/server.py
new file mode 100644
index 0000000..d2b4938
--- /dev/null
+++ b/mcp_servers/vector_search_server/server.py
@@ -0,0 +1,102 @@
+"""Vector Search MCP server implementation."""
+
+import uvicorn
+from typing import Dict, Any
+from mcp_servers.base_server import BaseMCPServer
+from mcp_servers.vector_search_server.tools import get_tools
+from mcp_servers.vector_search_server.vector_store import SimpleVectorStore
+from config.settings import get_settings
+
+
+class VectorSearchMCPServer(BaseMCPServer):
+    """Vector Search MCP server for semantic document search."""
+    
+    def __init__(self):
+        """Initialize Vector Search MCP server."""
+        self.vector_store = SimpleVectorStore()
+        super().__init__(
+            server_name="Vector Search MCP Server",
+            tools=get_tools()
+        )
+    
+    async def _execute_tool_internal(
+        self,
+        method: str,
+        params: Dict[str, Any],
+        request_id: str
+    ) -> Any:
+        """Execute vector search tool.
+        
+        Args:
+            method: Tool method name
+            params: Tool parameters
+            request_id: Request ID for correlation
+            
+        Returns:
+            Tool execution result
+        """
+        if method == "search_documents":
+            query = params.get("query")
+            if not query:
+                raise ValueError("query parameter is required")
+            
+            collection = params.get("collection", "default")
+            top_k = params.get("top_k", 5)
+            
+            results = await self.vector_store.search(
+                query=query,
+                collection_name=collection,
+                top_k=top_k
+            )
+            
+            return {
+                "query": query,
+                "collection": collection,
+                "results": results,
+                "count": len(results)
+            }
+        
+        elif method == "add_documents":
+            documents = params.get("documents")
+            if not documents:
+                raise ValueError("documents parameter is required")
+            
+            collection = params.get("collection", "default")
+            
+            result = await self.vector_store.add_documents(
+                collection_name=collection,
+                documents=documents
+            )
+            
+            return result
+        
+        elif method == "list_collections":
+            collections = self.vector_store.list_collections()
+            return {
+                "collections": collections,
+                "count": len(collections)
+            }
+        
+        else:
+            raise ValueError(f"Unknown tool method: {method}")
+
+
+def create_app():
+    """Create FastAPI app instance."""
+    server = VectorSearchMCPServer()
+    return server.app
+
+
+def run_server():
+    """Run the Vector Search MCP server."""
+    settings = get_settings()
+    uvicorn.run(
+        "mcp_servers.vector_search_server.server:create_app",
+        host="0.0.0.0",
+        port=settings.vector_mcp_port,
+        reload=False
+    )
+
+
+if __name__ == "__main__":
+    run_server()
diff --git a/mcp_servers/vector_search_server/tools.py b/mcp_servers/vector_search_server/tools.py
new file mode 100644
index 0000000..9624fae
--- /dev/null
+++ b/mcp_servers/vector_search_server/tools.py
@@ -0,0 +1,82 @@
+"""Vector Search server tools with versioning."""
+
+from typing import List, Dict, Any
+
+# Tool version
+TOOL_VERSION = "1.0.0"
+
+
+def get_tools() -> List[Dict[str, Any]]:
+    """Get list of vector search tools with versioning.
+    
+    Returns:
+        List of tool definitions with version metadata
+    """
+    return [
+        {
+            "name": "search_documents",
+            "tool_version": TOOL_VERSION,
+            "description": "Search for documents using semantic similarity",
+            "inputSchema": {
+                "type": "object",
+                "properties": {
+                    "query": {
+                        "type": "string",
+                        "description": "Search query text"
+                    },
+                    "collection": {
+                        "type": "string",
+                        "description": "Collection name to search in",
+                        "default": "default"
+                    },
+                    "top_k": {
+                        "type": "integer",
+                        "description": "Number of results to return",
+                        "default": 5,
+                        "minimum": 1,
+                        "maximum": 100
+                    }
+                },
+                "required": ["query"]
+            }
+        },
+        {
+            "name": "add_documents",
+            "tool_version": TOOL_VERSION,
+            "description": "Add documents to a collection",
+            "inputSchema": {
+                "type": "object",
+                "properties": {
+                    "collection": {
+                        "type": "string",
+                        "description": "Collection name",
+                        "default": "default"
+                    },
+                    "documents": {
+                        "type": "array",
+                        "items": {
+                            "type": "object",
+                            "properties": {
+                                "id": {"type": "string"},
+                                "text": {"type": "string"},
+                                "metadata": {"type": "object"}
+                            },
+                            "required": ["id", "text"]
+                        },
+                        "description": "List of documents to add"
+                    }
+                },
+                "required": ["documents"]
+            }
+        },
+        {
+            "name": "list_collections",
+            "tool_version": TOOL_VERSION,
+            "description": "List all available collections",
+            "inputSchema": {
+                "type": "object",
+                "properties": {},
+                "required": []
+            }
+        }
+    ]
diff --git a/mcp_servers/vector_search_server/vector_store.py b/mcp_servers/vector_search_server/vector_store.py
new file mode 100644
index 0000000..a853bd1
--- /dev/null
+++ b/mcp_servers/vector_search_server/vector_store.py
@@ -0,0 +1,159 @@
+"""Vector store implementation using simple in-memory storage with Gemini embeddings."""
+
+import json
+import os
+from typing import List, Dict, Any, Optional
+from pathlib import Path
+import numpy as np
+from config.settings import get_settings
+from llm.factory import LLMFactory
+from llm.models import EmbeddingRequest
+
+
+class SimpleVectorStore:
+    """Simple in-memory vector store with Gemini embeddings.
+    
+    This is a lightweight implementation that doesn't require ChromaDB.
+    For production use, consider migrating to ChromaDB when available.
+    """
+    
+    def __init__(self, store_path: Optional[str] = None):
+        """Initialize vector store.
+        
+        Args:
+            store_path: Path to store vector data
+        """
+        self.settings = get_settings()
+        self.store_path = Path(store_path or self.settings.vector_store_path)
+        self.store_path.mkdir(parents=True, exist_ok=True)
+        
+        # Initialize LLM provider for embeddings
+        self.llm_provider = LLMFactory.create_embedding_provider(self.settings)
+        
+        # In-memory storage: {collection_name: {doc_id: {text, embedding, metadata}}}
+        self._collections: Dict[str, Dict[str, Dict[str, Any]]] = {}
+        
+        # Load existing data if available
+        self._load_data()
+    
+    def _get_collection_file(self, collection_name: str) -> Path:
+        """Get file path for collection data."""
+        return self.store_path / f"{collection_name}.json"
+    
+    def _load_data(self):
+        """Load collections from disk."""
+        if not self.store_path.exists():
+            return
+        
+        for file_path in self.store_path.glob("*.json"):
+            collection_name = file_path.stem
+            try:
+                with open(file_path, 'r', encoding='utf-8') as f:
+                    data = json.load(f)
+                    self._collections[collection_name] = data
+            except Exception as e:
+                print(f"Warning: Failed to load collection {collection_name}: {e}")
+    
+    def _save_collection(self, collection_name: str):
+        """Save collection to disk."""
+        if collection_name in self._collections:
+            file_path = self._get_collection_file(collection_name)
+            with open(file_path, 'w', encoding='utf-8') as f:
+                json.dump(self._collections[collection_name], f, indent=2)
+    
+    async def add_documents(
+        self,
+        collection_name: str,
+        documents: List[Dict[str, Any]]
+    ) -> Dict[str, Any]:
+        """Add documents to a collection.
+        
+        Args:
+            collection_name: Name of the collection
+            documents: List of documents with id, text, and optional metadata
+            
+        Returns:
+            Dictionary with add results
+        """
+        if collection_name not in self._collections:
+            self._collections[collection_name] = {}
+        
+        # Get embeddings for all documents
+        texts = [doc["text"] for doc in documents]
+        embedding_request = EmbeddingRequest(texts=texts)
+        embedding_response = await self.llm_provider.get_embeddings(embedding_request)
+        
+        # Store documents with embeddings
+        added_count = 0
+        for i, doc in enumerate(documents):
+            doc_id = doc.get("id", f"doc_{len(self._collections[collection_name])}")
+            self._collections[collection_name][doc_id] = {
+                "text": doc["text"],
+                "embedding": embedding_response.embeddings[i],
+                "metadata": doc.get("metadata", {})
+            }
+            added_count += 1
+        
+        # Save to disk
+        self._save_collection(collection_name)
+        
+        return {
+            "collection": collection_name,
+            "added_count": added_count,
+            "total_documents": len(self._collections[collection_name])
+        }
+    
+    async def search(
+        self,
+        query: str,
+        collection_name: str = "default",
+        top_k: int = 5
+    ) -> List[Dict[str, Any]]:
+        """Search for similar documents.
+        
+        Args:
+            query: Search query text
+            collection_name: Collection to search in
+            top_k: Number of results to return
+            
+        Returns:
+            List of similar documents with scores
+        """
+        if collection_name not in self._collections:
+            return []
+        
+        # Get query embedding
+        embedding_request = EmbeddingRequest(texts=[query])
+        embedding_response = await self.llm_provider.get_embeddings(embedding_request)
+        query_embedding = np.array(embedding_response.embeddings[0])
+        
+        # Calculate similarities
+        results = []
+        collection = self._collections[collection_name]
+        
+        for doc_id, doc_data in collection.items():
+            doc_embedding = np.array(doc_data["embedding"])
+            
+            # Cosine similarity
+            similarity = np.dot(query_embedding, doc_embedding) / (
+                np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
+            )
+            
+            results.append({
+                "id": doc_id,
+                "text": doc_data["text"],
+                "score": float(similarity),
+                "metadata": doc_data.get("metadata", {})
+            })
+        
+        # Sort by score (descending) and return top_k
+        results.sort(key=lambda x: x["score"], reverse=True)
+        return results[:top_k]
+    
+    def list_collections(self) -> List[str]:
+        """List all collection names.
+        
+        Returns:
+            List of collection names
+        """
+        return list(self._collections.keys())
diff --git a/requirements.txt b/requirements.txt
new file mode 100644
index 0000000..63cb0ec
--- /dev/null
+++ b/requirements.txt
@@ -0,0 +1,50 @@
+# Core dependencies
+fastapi>=0.115.0
+uvicorn[standard]>=0.32.0
+pydantic>=2.10.0
+pydantic-settings>=2.6.0
+
+# LLM Providers
+google-genai>=1.0.0
+# openai==1.3.0  # Uncomment when implementing OpenAI
+# anthropic==0.7.0  # Uncomment when implementing Anthropic
+
+# LangGraph and LangChain (optional - install when implementing agent)
+# langchain>=0.3.0
+# langchain-google-genai>=2.0.0
+# langgraph>=0.2.0
+
+# Vector Store (optional - install separately if needed)
+# chromadb>=0.5.0
+
+# Database
+aiosqlite>=0.19.0
+
+# Vector Operations
+numpy>=1.24.0
+
+# MLflow (optional - install when implementing evaluation)
+# mlflow>=2.15.0
+
+# HTTP Client
+httpx>=0.27.0
+aiohttp>=3.10.0
+
+# JSON-RPC
+# jsonrpc-async is deprecated, using jsonrpcclient/jsonrpcserver instead
+jsonrpcclient>=4.0.0
+
+# Utilities
+python-dotenv==1.0.0
+python-multipart==0.0.6
+
+# Testing
+pytest>=8.3.0
+pytest-asyncio>=0.24.0
+pytest-cov>=6.0.0
+
+# Rate Limiting
+slowapi==0.1.9
+
+# Logging
+structlog==23.2.0
diff --git a/scripts/__init__.py b/scripts/__init__.py
new file mode 100644
index 0000000..c07cabb
--- /dev/null
+++ b/scripts/__init__.py
@@ -0,0 +1 @@
+"""Scripts module."""
diff --git a/scripts/setup_data.py b/scripts/setup_data.py
new file mode 100644
index 0000000..b56db3a
--- /dev/null
+++ b/scripts/setup_data.py
@@ -0,0 +1,105 @@
+"""Setup sample data for MCP servers."""
+
+import asyncio
+import aiosqlite
+import sys
+from pathlib import Path
+
+# Add project root to path
+project_root = Path(__file__).parent.parent
+sys.path.insert(0, str(project_root))
+
+from config.settings import get_settings
+
+
+async def setup_sample_database():
+    """Create sample SQLite database with test data."""
+    settings = get_settings()
+    db_path = Path(settings.database_path)
+    
+    # Ensure directory exists
+    db_path.parent.mkdir(parents=True, exist_ok=True)
+    
+    # Remove existing database if it exists
+    if db_path.exists():
+        db_path.unlink()
+    
+    async with aiosqlite.connect(str(db_path)) as db:
+        # Create sample tables
+        await db.execute("""
+            CREATE TABLE users (
+                id INTEGER PRIMARY KEY AUTOINCREMENT,
+                name TEXT NOT NULL,
+                email TEXT UNIQUE NOT NULL,
+                age INTEGER,
+                created_at TEXT DEFAULT CURRENT_TIMESTAMP
+            )
+        """)
+        
+        await db.execute("""
+            CREATE TABLE products (
+                id INTEGER PRIMARY KEY AUTOINCREMENT,
+                name TEXT NOT NULL,
+                description TEXT,
+                price REAL NOT NULL,
+                category TEXT,
+                stock INTEGER DEFAULT 0,
+                created_at TEXT DEFAULT CURRENT_TIMESTAMP
+            )
+        """)
+        
+        await db.execute("""
+            CREATE TABLE orders (
+                id INTEGER PRIMARY KEY AUTOINCREMENT,
+                user_id INTEGER NOT NULL,
+                product_id INTEGER NOT NULL,
+                quantity INTEGER NOT NULL,
+                total_price REAL NOT NULL,
+                order_date TEXT DEFAULT CURRENT_TIMESTAMP,
+                FOREIGN KEY (user_id) REFERENCES users(id),
+                FOREIGN KEY (product_id) REFERENCES products(id)
+            )
+        """)
+        
+        # Insert sample data
+        await db.execute("""
+            INSERT INTO users (name, email, age) VALUES
+            ('Alice Johnson', 'alice@example.com', 30),
+            ('Bob Smith', 'bob@example.com', 25),
+            ('Charlie Brown', 'charlie@example.com', 35),
+            ('Diana Prince', 'diana@example.com', 28)
+        """)
+        
+        await db.execute("""
+            INSERT INTO products (name, description, price, category, stock) VALUES
+            ('Laptop', 'High-performance laptop', 999.99, 'Electronics', 10),
+            ('Mouse', 'Wireless mouse', 29.99, 'Electronics', 50),
+            ('Keyboard', 'Mechanical keyboard', 79.99, 'Electronics', 30),
+            ('Monitor', '27-inch 4K monitor', 399.99, 'Electronics', 15),
+            ('Headphones', 'Noise-cancelling headphones', 199.99, 'Electronics', 20)
+        """)
+        
+        await db.execute("""
+            INSERT INTO orders (user_id, product_id, quantity, total_price) VALUES
+            (1, 1, 1, 999.99),
+            (1, 2, 2, 59.98),
+            (2, 3, 1, 79.99),
+            (3, 4, 1, 399.99),
+            (4, 5, 1, 199.99)
+        """)
+        
+        await db.commit()
+        print(f"[OK] Sample database created at: {db_path}")
+        print("   - Created tables: users, products, orders")
+        print("   - Inserted sample data")
+
+
+async def main():
+    """Main function."""
+    print("Setting up sample data for MCP servers...")
+    await setup_sample_database()
+    print("[OK] Setup complete!")
+
+
+if __name__ == "__main__":
+    asyncio.run(main())
diff --git a/scripts/start_servers.py b/scripts/start_servers.py
new file mode 100644
index 0000000..affa2b1
--- /dev/null
+++ b/scripts/start_servers.py
@@ -0,0 +1,63 @@
+"""Start all MCP servers."""
+
+import asyncio
+import subprocess
+import sys
+from pathlib import Path
+from config.settings import get_settings
+
+
+def start_server(module_path: str, port: int, name: str):
+    """Start a single MCP server.
+    
+    Args:
+        module_path: Python module path to the server
+        port: Port number
+        name: Server name for logging
+    """
+    print(f"Starting {name} on port {port}...")
+    subprocess.Popen(
+        [sys.executable, "-m", module_path],
+        stdout=subprocess.PIPE,
+        stderr=subprocess.PIPE
+    )
+
+
+def main():
+    """Start all MCP servers."""
+    settings = get_settings()
+    
+    print("🚀 Starting MCP Servers...")
+    print("-" * 50)
+    
+    # Start Catalog server
+    start_server(
+        "mcp_servers.catalog_server.server",
+        settings.catalog_mcp_port,
+        "Catalog MCP Server"
+    )
+    
+    # Start SQL Query server
+    start_server(
+        "mcp_servers.sql_query_server.server",
+        settings.sql_mcp_port,
+        "SQL Query MCP Server"
+    )
+    
+    # Start Vector Search server
+    start_server(
+        "mcp_servers.vector_search_server.server",
+        settings.vector_mcp_port,
+        "Vector Search MCP Server"
+    )
+    
+    print("-" * 50)
+    print("[OK] All servers started!")
+    print(f"   Catalog Server: http://localhost:{settings.catalog_mcp_port}")
+    print(f"   SQL Query Server: http://localhost:{settings.sql_mcp_port}")
+    print(f"   Vector Search Server: http://localhost:{settings.vector_mcp_port}")
+    print("\nPress Ctrl+C to stop all servers")
+
+
+if __name__ == "__main__":
+    main()
diff --git a/tests/__init__.py b/tests/__init__.py
new file mode 100644
index 0000000..e329ef7
--- /dev/null
+++ b/tests/__init__.py
@@ -0,0 +1 @@
+"""Test suite for multi-tool orchestration."""
diff --git a/tests/test_llm_abstraction.py b/tests/test_llm_abstraction.py
new file mode 100644
index 0000000..7a60954
--- /dev/null
+++ b/tests/test_llm_abstraction.py
@@ -0,0 +1,88 @@
+"""Tests for LLM abstraction layer."""
+
+import pytest
+from unittest.mock import Mock, patch
+from llm.base import LLMProvider
+from llm.factory import LLMFactory
+from llm.models import LLMRequest, EmbeddingRequest
+from config.settings import Settings
+
+
+class MockLLMProvider(LLMProvider):
+    """Mock LLM provider for testing."""
+    
+    def __init__(self, provider_name: str, model_name: str):
+        self._provider_name = provider_name
+        self._model_name = model_name
+    
+    @property
+    def provider_name(self) -> str:
+        return self._provider_name
+    
+    @property
+    def model_name(self) -> str:
+        return self._model_name
+    
+    @property
+    def supports_streaming(self) -> bool:
+        return True
+    
+    async def chat_completion(self, request):
+        from llm.models import LLMResponse
+        return LLMResponse(
+            content="Test response",
+            model=self._model_name,
+            provider=self._provider_name
+        )
+    
+    async def get_embeddings(self, request):
+        from llm.models import EmbeddingResponse
+        return EmbeddingResponse(
+            embeddings=[[0.1, 0.2, 0.3] for _ in request.texts],
+            model=self._model_name,
+            provider=self._provider_name
+        )
+
+
+def test_llm_factory_available_providers():
+    """Test that factory returns available providers."""
+    providers = LLMFactory.get_available_providers()
+    assert "gemini" in providers
+    assert "openai" in providers
+    assert "anthropic" in providers
+    assert "ollama" in providers
+
+
+def test_llm_factory_unsupported_provider():
+    """Test that factory raises error for unsupported provider."""
+    settings = Settings(llm_provider="invalid_provider")
+    with pytest.raises(ValueError, match="Unsupported LLM provider"):
+        LLMFactory.create_provider(settings)
+
+
+def test_llm_factory_missing_api_key():
+    """Test that factory raises error when API key is missing."""
+    settings = Settings(llm_provider="gemini", gemini_api_key=None)
+    with pytest.raises(ValueError, match="API key is required"):
+        LLMFactory.create_provider(settings)
+
+
+@pytest.mark.asyncio
+async def test_llm_provider_interface():
+    """Test that LLM provider interface works correctly."""
+    provider = MockLLMProvider("test", "test-model")
+    
+    request = LLMRequest(
+        messages=[{"role": "user", "content": "Hello"}],
+        temperature=0.7
+    )
+    
+    response = await provider.chat_completion(request)
+    assert response.content == "Test response"
+    assert response.provider == "test"
+    assert response.model == "test-model"
+    
+    embedding_request = EmbeddingRequest(texts=["test"])
+    embedding_response = await provider.get_embeddings(embedding_request)
+    assert len(embedding_response.embeddings) == 1
+    assert embedding_response.provider == "test"

```

`git_diff_processor/sample_diffs/diff_example.txt`

```
diff --git a/backend/agent/agent_pool.py b/backend/agent/agent_pool.py
index b89b174..b22cb55 100644
--- a/backend/agent/agent_pool.py
+++ b/backend/agent/agent_pool.py
@@ -37,7 +37,7 @@ async def get_agent() -> LangGraphAgent:
     global _agent_instance, _agent_initialized, _agent_last_used   
:...skipping...
diff --git a/backend/agent/agent_pool.py b/backend/agent/agent_pool.py
index b89b174..b22cb55 100644
--- a/backend/agent/agent_pool.py
+++ b/backend/agent/agent_pool.py
@@ -37,7 +37,7 @@ async def get_agent() -> LangGraphAgent:
     global _agent_instance, _agent_initialized, _agent_last_used
     
     async with _agent_lock:
-        # Check if agent exis ts and is still valid
+        # Check if agent exists and is still valid
         if _agent_instance is not None and _agent_initialized:
             # Check if agent has expired (not used for TTL period)
             if _agent_last_used:
diff --git a/backend/agent/langgraph_agent.py b/backend/agent/langgraph_agent.py
index 57ed550..e85a9db 100644
--- a/backend/agent/langgraph_agent.py
+++ b/backend/agent/langgraph_agent.py
@@ -9,29 +9,11 @@ from typing import Optional, AsyncIterator, Dict, Any
 import asyncio
 from datetime import datetime
 
-# Try multiple import paths for RunnableConfig (LangGraph version differences)
-RunnableConfig = None
-LANGGRAPH_CONFIG_AVAILABLE = False
-try:
-    from langgraph.graph import RunnableConfig
-    LANGGRAPH_CONFIG_AVAILABLE = True
```

`git_diff_processor/sample_diffs/README.md`

```markdown
# Sample Diffs Folder

This folder contains git diff files for testing the git diff processor.

## How to Create a Git Diff File

### Option 1: Diff between two commits
```bash
git diff commit1 commit2 > git_diff_processor/sample_diffs/diff_commit1.txt
```

### Option 2: Diff for a single commit
```bash
git show commit_hash > git_diff_processor/sample_diffs/diff_commit1.txt
```

### Option 3: Diff for uncommitted changes
```bash
git diff > git_diff_processor/sample_diffs/diff_uncommitted.txt
```

### Option 4: Diff between branches
```bash
git diff branch1 branch2 > git_diff_processor/sample_diffs/diff_branches.txt
```

## File Naming Convention

- `diff_commit1.txt` - First commit diff
- `diff_commit2.txt` - Second commit diff
- `diff_uncommitted.txt` - Uncommitted changes
- `diff_branches.txt` - Branch comparison

## Example Git Diff Format

The processor expects standard git unified diff format:

```diff
diff --git a/path/to/file.py b/path/to/file.py
index abc123..def456 100644
--- a/path/to/file.py
+++ b/path/to/file.py
@@ -10,6 +10,8 @@ def function_name():
+    # New line
     existing_code
+    return value
```

## Usage

Once you have a diff file, run:

```bash
python git_diff_processor/git_diff_processor.py sample_diffs/diff_commit1.txt
```

Or let it use the default:

```bash
python git_diff_processor/git_diff_processor.py
```

```

`git_diff_processor/utils/deduplicate_tests.py`

```python
"""
Utility to find and remove duplicate test entries from database.

This module helps identify and clean up duplicate tests that may have been
created due to path mismatches (e.g., same file indexed from different locations).
"""

from pathlib import Path
from typing import Dict, List, Any, Set, Tuple
import sys

# Add parent directories to path for imports
current_dir = Path(__file__).parent
parent_dir = current_dir.parent
root_dir = parent_dir.parent

sys.path.insert(0, str(root_dir))

from deterministic.db_connection import get_connection, DB_SCHEMA


def _normalize_path_for_dedup(file_path: str) -> str:
    """
    Normalize file path for duplicate detection.
    
    Strategy:
    1. Extract relative path from test_repository (if present)
    2. Otherwise use filename + parent directory name
    3. This handles cases where same file is indexed from different absolute paths
    
    Examples:
    - C:/Users/.../Downloads/.../test_repository/unit/test_langgraph_nodes.py
      -> unit/test_langgraph_nodes.py
    - C:/Users/.../OneDrive/.../test_repository/unit/test_langgraph_nodes.py
      -> unit/test_langgraph_nodes.py
    """
    path = Path(file_path)
    
    # Try to extract relative path from test_repository
    path_str = str(path).replace('\\', '/')
    if 'test_repository' in path_str:
        # Extract everything after test_repository
        parts = path_str.split('test_repository', 1)
        if len(parts) > 1:
            relative = parts[1].lstrip('/\\')
            # Normalize separators
            normalized = relative.replace('\\', '/')
            return normalized
    
    # Fallback: use filename + parent directory
    # This handles cases where test_repository is not in path
    # but we can still identify by directory structure
    if path.parent.name in ['unit', 'integration', 'e2e', 'tests', 'test']:
        return f"{path.parent.name}/{path.name}"
    
    # Last resort: just filename (less reliable but better than nothing)
    return path.name


def find_duplicate_tests(conn=None) -> Dict[str, Any]:
    """
    Find duplicate test entries in the database.
    
    Duplicates are identified by:
    - Same normalized file path (relative to test_repository)
    - Same class_name (or both None)
    - Same method_name
    
    Args:
        conn: Optional database connection
    
    Returns:
        Dictionary with duplicate information
    """
    should_close = False
    if conn is None:
        conn = get_connection().__enter__()
        should_close = True
    
    try:
        with conn.cursor() as cursor:
            # Get all tests
            cursor.execute(f"""
                SELECT test_id, file_path, class_name, method_name, test_type
                FROM {DB_SCHEMA}.test_registry
                ORDER BY file_path, class_name, method_name
            """)
            all_tests = cursor.fetchall()
            
            # Group by normalized path + class + method
            test_groups = {}
            for row in all_tests:
                test_id, file_path, class_name, method_name, test_type = row
                # Normalize path using relative path strategy
                normalized_path = _normalize_path_for_dedup(file_path)
                
                key = (normalized_path, class_name or '', method_name)
                
                if key not in test_groups:
                    test_groups[key] = []
                test_groups[key].append({
                    'test_id': test_id,
                    'file_path': file_path,
                    'class_name': class_name,
                    'method_name': method_name,
                    'test_type': test_type
                })
            
            # Find duplicates (groups with more than 1 test)
            duplicates = {}
            total_duplicate_tests = 0
            
            for key, tests in test_groups.items():
                if len(tests) > 1:
                    normalized_path, class_name, method_name = key
                    duplicates[key] = tests
                    total_duplicate_tests += len(tests) - 1  # Keep 1, remove the rest
            
            return {
                'total_tests': len(all_tests),
                'unique_tests': len(test_groups),
                'duplicate_groups': len(duplicates),
                'duplicate_tests': total_duplicate_tests,
                'duplicates': duplicates
            }
    finally:
        if should_close:
            conn.__exit__(None, None, None)


def remove_duplicate_tests(conn=None, dry_run: bool = True) -> Dict[str, Any]:
    """
    Remove duplicate test entries, keeping the one with the lowest test_id.
    
    Args:
        conn: Optional database connection
        dry_run: If True, only report what would be removed without actually removing
    
    Returns:
        Dictionary with removal results
    """
    duplicates_info = find_duplicate_tests(conn)
    
    if duplicates_info['duplicate_groups'] == 0:
        return {
            'removed': 0,
            'kept': 0,
            'dry_run': dry_run
        }
    
    should_close = False
    if conn is None:
        conn = get_connection().__enter__()
        should_close = True
    
    try:
        test_ids_to_remove = []
        test_ids_to_keep = []
        
        for key, tests in duplicates_info['duplicates'].items():
            # Sort by test_id to keep the lowest (oldest)
            sorted_tests = sorted(tests, key=lambda x: x['test_id'])
            test_ids_to_keep.append(sorted_tests[0]['test_id'])
            # Mark others for removal
            for test in sorted_tests[1:]:
                test_ids_to_remove.append(test['test_id'])
        
        if not dry_run and test_ids_to_remove:
            # Also need to remove from related tables
            with conn.cursor() as cursor:
                # Remove from reverse_index
                placeholders = ','.join(['%s'] * len(test_ids_to_remove))
                cursor.execute(f"""
                    DELETE FROM {DB_SCHEMA}.reverse_index
                    WHERE test_id IN ({placeholders})
                """, test_ids_to_remove)
                
                # Remove from test_dependencies
                cursor.execute(f"""
                    DELETE FROM {DB_SCHEMA}.test_dependencies
                    WHERE test_id IN ({placeholders})
                """, test_ids_to_remove)
                
                # Remove from test_metadata
                cursor.execute(f"""
                    DELETE FROM {DB_SCHEMA}.test_metadata
                    WHERE test_id IN ({placeholders})
                """, test_ids_to_remove)
                
                # Finally remove from test_registry
                cursor.execute(f"""
                    DELETE FROM {DB_SCHEMA}.test_registry
                    WHERE test_id IN ({placeholders})
                """, test_ids_to_remove)
                
                conn.commit()
        
        return {
            'removed': len(test_ids_to_remove),
            'kept': len(test_ids_to_keep),
            'dry_run': dry_run,
            'test_ids_removed': test_ids_to_remove if dry_run else []
        }
    finally:
        if should_close:
            conn.__exit__(None, None, None)

```

`git_diff_processor/utils/diff_parser.py`

```python
"""
Git Diff Parser

This module parses git diff output (unified diff format) to extract:
- Changed files
- Changed classes (from class definitions)
- Changed methods (from method signatures)
- File status (added, deleted, modified)
- Line numbers

It handles standard git diff format and extracts production code references
that can be used to query the database.
"""

import re
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path


def parse_git_diff(diff_content: str) -> Dict[str, Any]:
    """
    Parse git diff content to extract changed information.
    
    Args:
        diff_content: Raw git diff output as string
    
    Returns:
        Dictionary with parsed diff information:
        - changed_files: List of changed file paths
        - changed_classes: List of changed class names
        - changed_methods: List of changed method names
        - file_changes: List of detailed file change information
    
    Example:
        >>> diff = "diff --git a/file.py b/file.py\\n..."
        >>> result = parse_git_diff(diff)
        >>> result['changed_files']
        ['file.py']
    """
    if not diff_content.strip():
        return {
            'changed_files': [],
            'changed_classes': [],
            'changed_methods': [],
            'file_changes': []
        }
    
    lines = diff_content.split('\n')
    file_changes = []
    changed_files = []
    changed_classes = set()
    changed_methods = set()
    
    current_file = None
    current_file_info = None
    in_hunk = False
    
    i = 0
    while i < len(lines):
        line = lines[i]
        
        # Match file header: "diff --git a/path b/path"
        file_match = re.match(r'diff --git a/(.+?) b/(.+?)$', line)
        if file_match:
            # Save previous file if exists
            if current_file_info:
                file_changes.append(current_file_info)
            
            # Start new file
            old_path = file_match.group(1)
            new_path = file_match.group(2)
            current_file = new_path if new_path != '/dev/null' else old_path
            changed_files.append(current_file)
            
            # Determine file status
            status = 'modified'
            if i + 1 < len(lines):
                next_line = lines[i + 1]
                if 'new file' in next_line or 'deleted file' in next_line:
                    if 'new file' in next_line:
                        status = 'added'
                    elif 'deleted file' in next_line:
                        status = 'deleted'
            
            current_file_info = {
                'file': current_file,
                'status': status,
                'additions': 0,
                'deletions': 0,
                'changed_lines': [],
                'changed_classes': [],
                'changed_methods': []
            }
            in_hunk = False
            i += 1
            continue
        
        # Match hunk header: "@@ -start,count +start,count @@"
        hunk_match = re.match(r'@@ -(\d+)(?:,(\d+))? \+(\d+)(?:,(\d+))? @@', line)
        if hunk_match and current_file_info:
            in_hunk = True
            old_start = int(hunk_match.group(1))
            old_count = int(hunk_match.group(2)) if hunk_match.group(2) else 1
            new_start = int(hunk_match.group(3))
            new_count = int(hunk_match.group(4)) if hunk_match.group(4) else 1
            
            # Track line numbers in this hunk
            old_line = old_start
            new_line = new_start
            i += 1
            continue
        
        # Process hunk content
        if in_hunk and current_file_info:
            if line.startswith('+') and not line.startswith('+++'):
                # Added line
                current_file_info['additions'] += 1
                new_line += 1
                
                # Check for class definition
                class_match = re.search(r'class\s+(\w+)', line)
                if class_match:
                    class_name = class_match.group(1)
                    changed_classes.add(class_name)
                    if class_name not in current_file_info['changed_classes']:
                        current_file_info['changed_classes'].append(class_name)
                
                # Check for method/function definition
                method_match = re.search(r'def\s+(\w+)', line)
                if method_match:
                    method_name = method_match.group(1)
                    changed_methods.add(method_name)
                    if method_name not in current_file_info['changed_methods']:
                        current_file_info['changed_methods'].append(method_name)
                
                current_file_info['changed_lines'].append(new_line)
            
            elif line.startswith('-') and not line.startswith('---'):
                # Deleted line
                current_file_info['deletions'] += 1
                old_line += 1
                current_file_info['changed_lines'].append(old_line)
            
            elif line.startswith(' '):
                # Context line (unchanged)
                old_line += 1
                new_line += 1
            
            i += 1
            continue
        
        i += 1
    
    # Save last file
    if current_file_info:
        file_changes.append(current_file_info)
    
    return {
        'changed_files': list(set(changed_files)),
        'changed_classes': sorted(list(changed_classes)),
        'changed_methods': sorted(list(changed_methods)),
        'file_changes': file_changes
    }


def is_production_python_file(file_path: str) -> bool:
    """
    Check if a file is a Python production code file.
    
    Filters out:
    - Non-Python files (.css, .tsx, .txt, .db, .bin, .json, etc.)
    - Test files
    - Artifact/data files
    - Frontend files (TypeScript/React)
    - Configuration files
    
    Args:
        file_path: File path to check
    
    Returns:
        True if it's a Python production file, False otherwise
    """
    if not file_path or file_path == '/dev/null':
        return False
    
    # Must be a Python file
    if not file_path.endswith('.py'):
        return False
    
    # Skip test files
    file_lower = file_path.lower()
    if 'test' in file_lower or file_path.endswith('_test.py'):
        return False
    
    # Skip artifact/data directories
    skip_dirs = [
        'mlartifacts',
        'artifacts',
        'data/',
        'chromadb_data',
        'node_modules',
        '__pycache__',
        '.git',
        'venv',
        'env',
        'frontend',  # Frontend code (TypeScript/React)
        'static',
        'templates'
    ]
    
    for skip_dir in skip_dirs:
        if skip_dir in file_lower:
            return False
    
    return True


def extract_production_classes_from_file(file_path: str) -> List[str]:
    """
    Extract production class/module names from a file path.
    
    This converts file paths to import-style module names that match
    what's stored in the database.
    
    Only processes Python production files (filters out test files,
    artifacts, data files, frontend files, etc.)
    
    Args:
        file_path: File path from git diff (e.g., "agent/agent_pool.py")
    
    Returns:
        List of possible production class/module names to search
    
    Example:
        >>> extract_production_classes_from_file("agent/agent_pool.py")
        ['agent.agent_pool', 'agent']
        >>> extract_production_classes_from_file("frontend/src/ChatPage.tsx")
        []
    """
    if not file_path or file_path == '/dev/null':
        return []
    
    # Only process Python production files
    if not is_production_python_file(file_path):
        return []
    
    # Remove .py extension
    file_path = file_path[:-3]
    
    # Convert path to module name
    module_name = file_path.replace('/', '.').replace('\\', '.')
    
    # Database stores module paths WITHOUT "backend." prefix
    # So "backend/agent/agent_pool.py" becomes "agent.agent_pool"
    if module_name.startswith('backend.'):
        module_name = module_name[8:]  # Remove "backend." prefix
    
    # Generate search candidates
    candidates = []
    
    # Add full module path (e.g., "agent.agent_pool")
    candidates.append(module_name)
    
    # Add parent module (e.g., "agent" from "agent.agent_pool")
    parts = module_name.split('.')
    if len(parts) > 1:
        parent_module = parts[0]
        candidates.append(parent_module)
    
    return list(set(candidates))  # Remove duplicates


def extract_test_file_candidates(file_path: str) -> List[str]:
    """
    Extract potential test file names from a production file path.
    
    Enhanced with multiple strategies to find test files in any repository structure.
    
    Args:
        file_path: Production file path (e.g., "backend/agent/agent_pool.py")
    
    Returns:
        List of potential test file names to search for
    
    Example:
        >>> extract_test_file_candidates("backend/agent/agent_pool.py")
        ['test_agent_pool.py', 'test_agent_agent_pool.py', 'test_agent_pool_*.py']
        >>> extract_test_file_candidates("backend/api/routes.py")
        ['test_routes.py', 'test_api_routes.py', 'test_routes_*.py']
    """
    if not file_path or file_path == '/dev/null':
        return []
    
    # Only for Python files
    if not file_path.endswith('.py'):
        return []
    
    path_obj = Path(file_path)
    candidates = set()
    
    # Get file name without extension
    file_stem = path_obj.stem  # e.g., 'agent_pool'
    
    # Strategy 1: Direct test file name: test_<filename>.py
    candidates.add(f"test_{file_stem}.py")
    
    # Strategy 2: If file is in a subdirectory, check parent module pattern
    # e.g., backend/agent/agent_pool.py -> test_agent_agent_pool.py
    if len(path_obj.parts) > 1:
        parent_dir = path_obj.parts[-2]  # e.g., 'agent'
        candidates.add(f"test_{parent_dir}_{file_stem}.py")
    
    # Strategy 3: Check for test_<parent>_<file>.py pattern
    # e.g., backend/api/routes.py -> test_api_routes.py
    if len(path_obj.parts) >= 2:
        parent = path_obj.parts[-2]
        candidates.add(f"test_{parent}_{file_stem}.py")
    
    # Strategy 4: For nested modules, try full module path
    # e.g., backend/agent/agent_pool.py -> test_agent_agent_pool.py (already covered)
    # But also try: test_agent_pool_*.py for parameterized tests
    candidates.add(f"test_{file_stem}_*.py")
    
    # Strategy 5: Try without underscores (for camelCase files)
    if '_' in file_stem:
        camel_case = file_stem.replace('_', '')
        candidates.add(f"test_{camel_case}.py")
    
    # Strategy 6: Try with module prefix if in subdirectory
    if len(path_obj.parts) >= 2:
        # Build module path: agent.agent_pool
        module_parts = path_obj.parts[-2:]  # ['agent', 'agent_pool.py']
        module_name = '.'.join([p.replace('.py', '') for p in module_parts])
        # Convert to test pattern: test_agent_agent_pool.py
        test_module_name = module_name.replace('.', '_')
        candidates.add(f"test_{test_module_name}.py")
    
    return sorted(list(candidates))  # Return sorted list for consistency


def analyze_file_change_type(file_change: Dict) -> str:
    """
    Determine the type of change in a file.
    
    Returns:
        - 'code': Actual code changes (classes, methods, logic)
        - 'import_only': Only import statements changed
        - 'comment_only': Only comments changed
        - 'deleted': File was deleted
        - 'added': New file added
    """
    if file_change['status'] == 'deleted':
        return 'deleted'
    
    if file_change['status'] == 'added':
        return 'added'
    
    # Check if there are actual code changes (classes, methods)
    has_code_changes = (
        file_change.get('changed_classes') or 
        file_change.get('changed_methods') or
        file_change.get('additions', 0) > 0
    )
    
    if not has_code_changes:
        return 'comment_only'
    
    # Check if changes are primarily in import section (typically first 50 lines)
    changed_lines = file_change.get('changed_lines', [])
    if changed_lines:
        import_section_changes = [line for line in changed_lines if line <= 50]
        # If all changes are in import section and no classes/methods changed
        if (len(import_section_changes) == len(changed_lines) and 
            not file_change.get('changed_classes') and 
            not file_change.get('changed_methods')):
            return 'import_only'
    
    return 'code'


def build_search_queries(file_changes: List[Dict]) -> Dict[str, List[str]]:
    """
    Build database search queries from file changes.
    
    Only processes Python production files, filtering out:
    - Non-Python files (CSS, TSX, TXT, DB, etc.)
    - Test files
    - Artifact/data files
    - Frontend files
    
    Args:
        file_changes: List of file change dictionaries from parse_git_diff
    
    Returns:
        Dictionary with search strategies:
        - exact_matches: Exact production class names
        - module_matches: Module-level patterns (agent.*)
        - file_patterns: File name patterns
        - test_file_candidates: Direct test file names to search
    """
    exact_matches = []
    module_matches = []
    file_patterns = []
    test_file_candidates = []
    
    for file_change in file_changes:
        file_path = file_change['file']
        
        # Skip import-only changes (they don't affect production code behavior)
        change_type = analyze_file_change_type(file_change)
        if change_type == 'import_only':
            continue  # Don't match tests for import-only changes
        
        # Extract production classes (this function already filters)
        classes = extract_production_classes_from_file(file_path)
        
        # If no classes extracted, skip this file (not a production Python file)
        if not classes:
            continue
        
        for class_name in classes:
            # Exact match
            exact_matches.append(class_name)
            
            # Module-level match (if has dots)
            if '.' in class_name:
                module_part = class_name.split('.')[0]
                module_pattern = f"{module_part}.*"
                if module_pattern not in module_matches:
                    module_matches.append(module_pattern)
        
        # File pattern (for fallback searches) - only for Python files
        if file_path.endswith('.py'):
            file_name = Path(file_path).stem
            if file_name:
                file_patterns.append(file_name)
        
        # Extract direct test file candidates
        test_candidates = extract_test_file_candidates(file_path)
        test_file_candidates.extend(test_candidates)
    
    # Extract changed functions with modules (for function-level matching)
    changed_functions = extract_changed_functions_with_modules(file_changes)
    
    return {
        'exact_matches': list(set(exact_matches)),
        'module_matches': list(set(module_matches)),
        'file_patterns': list(set(file_patterns)),
        'test_file_candidates': list(set(test_file_candidates)),
        'changed_functions': changed_functions  # NEW: Function-level changes
    }


def extract_changed_functions_with_modules(file_changes: List[Dict]) -> List[Dict[str, str]]:
    """
    Extract changed functions with their module names from file changes.
    
    Converts file paths and changed methods to module.function format
    that can be used to query the test_function_mapping table.
    
    Args:
        file_changes: List of file change dictionaries from parse_git_diff
    
    Returns:
        List of dictionaries with:
        - module: Module name (e.g., 'agent.langgraph_agent')
        - function: Function name (e.g., 'initialize')
    
    Example:
        >>> file_changes = [{
        ...     'file': 'agent/langgraph_agent.py',
        ...     'changed_methods': ['initialize', 'invoke']
        ... }]
        >>> extract_changed_functions_with_modules(file_changes)
        [
        ...     {'module': 'agent.langgraph_agent', 'function': 'initialize'},
        ...     {'module': 'agent.langgraph_agent', 'function': 'invoke'}
        ... ]
    """
    changed_functions = []
    
    for file_change in file_changes:
        file_path = file_change['file']
        
        # Skip non-Python files
        if not file_path.endswith('.py'):
            continue
        
        # Skip import-only changes
        change_type = analyze_file_change_type(file_change)
        if change_type == 'import_only':
            continue
        
        # Only process production Python files
        if not is_production_python_file(file_path):
            continue
        
        # Get changed methods from this file
        changed_methods = file_change.get('changed_methods', [])
        if not changed_methods:
            continue
        
        # Convert file path to module name
        # e.g., 'agent/langgraph_agent.py' -> 'agent.langgraph_agent'
        module_name = extract_production_classes_from_file(file_path)
        if not module_name:
            continue
        
        # Use the first (most specific) module name
        primary_module = module_name[0] if isinstance(module_name, list) else module_name
        
        # Create module.function entries for each changed method
        for method_name in changed_methods:
            changed_functions.append({
                'module': primary_module,
                'function': method_name
            })
    
    # Remove duplicates (same module.function combination)
    seen = set()
    unique_functions = []
    for func in changed_functions:
        key = (func['module'], func['function'])
        if key not in seen:
            seen.add(key)
            unique_functions.append(func)
    
    return unique_functions


def read_diff_file(file_path: Path) -> str:
    """
    Read git diff content from a file.
    
    Args:
        file_path: Path to the diff file
    
    Returns:
        Diff content as string
    
    Raises:
        FileNotFoundError: If file doesn't exist
        IOError: If file can't be read
    """
    if not file_path.exists():
        raise FileNotFoundError(f"Diff file not found: {file_path}")
    
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        return f.read()

```

`git_diff_processor/utils/indexing_utils.py`

```python
"""
Indexing utilities for verifying and re-indexing test files.

This module provides functions to:
- Verify indexing completeness
- Re-index missing test files
- Diagnose integration test issues
"""

from pathlib import Path
from typing import Dict, List, Any, Optional
import sys

# Add parent directories to path for imports
current_dir = Path(__file__).parent
parent_dir = current_dir.parent
root_dir = parent_dir.parent

sys.path.insert(0, str(root_dir))
sys.path.insert(0, str(root_dir / "test_analysis"))

from deterministic.db_connection import get_connection, DB_SCHEMA
from test_analysis.utils.file_scanner import scan_directory_comprehensive, get_file_metadata, _categorize_directory
from test_analysis.utils.ast_parser import parse_file, extract_test_classes, extract_test_methods


def extract_test_type_enhanced(filepath: Path) -> str:
    """Extract test type from file path."""
    category = _categorize_directory(filepath)
    if category == 'integration':
        return 'integration'
    elif category == 'e2e':
        return 'e2e'
    else:
        return 'unit'


def verify_indexing_completeness(test_repo_path: str, conn=None) -> Dict[str, Any]:
    """
    Verify that all test files in repository are indexed.
    
    FIXED: Now properly normalizes paths and filters out __init__.py and conftest.py.
    
    Args:
        test_repo_path: Path to test repository
        conn: Optional database connection (will create if not provided)
    
    Returns:
        Dictionary with verification results
    """
    test_repo = Path(test_repo_path)
    if not test_repo.exists():
        return {
            'error': f"Test repository path does not exist: {test_repo_path}",
            'total_on_disk': 0,
            'total_indexed': 0,
            'missing_files': [],
            'coverage_percent': 0
        }
    
    # Find all test files on disk (excluding __init__.py and conftest.py)
    test_files_on_disk = scan_directory_comprehensive(test_repo)
    # Filter out __init__.py and conftest.py
    test_files_on_disk = [f for f in test_files_on_disk if f.name not in ['__init__.py', 'conftest.py']]
    test_files_on_disk_str = [str(f.resolve()) for f in test_files_on_disk]
    
    # Get all test files in database
    should_close = False
    if conn is None:
        conn = get_connection().__enter__()
        should_close = True
    
    try:
        with conn.cursor() as cursor:
            cursor.execute(f"""
                SELECT DISTINCT file_path
                FROM {DB_SCHEMA}.test_registry
            """)
            indexed_files = [row[0] for row in cursor.fetchall()]
        
        # Normalize paths for comparison (handle both absolute and relative paths)
        indexed_paths_normalized = set()
        for p in indexed_files:
            try:
                # Try to resolve the path
                resolved = Path(p).resolve()
                indexed_paths_normalized.add(str(resolved))
            except:
                # If resolution fails, use as-is (might be relative)
                indexed_paths_normalized.add(str(Path(p)))
        
        disk_paths_normalized = {str(Path(p).resolve()) for p in test_files_on_disk_str}
        
        # Find missing files
        missing_files = []
        for disk_path in disk_paths_normalized:
            # Check if this file is indexed (by normalized path)
            if disk_path not in indexed_paths_normalized:
                missing_files.append(disk_path)
        
        coverage = (len(indexed_paths_normalized) / len(disk_paths_normalized) * 100) if disk_paths_normalized else 0
        
        return {
            'total_on_disk': len(test_files_on_disk),
            'total_indexed': len(indexed_paths_normalized),
            'missing_files': missing_files,
            'coverage_percent': coverage
        }
    finally:
        if should_close:
            conn.__exit__(None, None, None)


def reindex_missing_files(test_repo_path: str, conn=None) -> Dict[str, Any]:
    """
    Re-index only files that are missing from database.
    
    FIXED: Now checks for existing tests by (file_path, class_name, method_name)
    to avoid creating duplicates when the same file has different absolute paths.
    
    Args:
        test_repo_path: Path to test repository
        conn: Optional database connection
    
    Returns:
        Dictionary with re-indexing results
    """
    from deterministic.utils.db_helpers import batch_insert_test_registry
    
    verification = verify_indexing_completeness(test_repo_path, conn)
    missing_files = verification.get('missing_files', [])
    
    if not missing_files:
        return {'indexed': 0, 'skipped': 0, 'errors': [], 'duplicates_avoided': 0}
    
    should_close = False
    if conn is None:
        conn = get_connection().__enter__()
        should_close = True
    
    try:
        indexed_count = 0
        error_count = 0
        errors = []
        all_tests = []
        duplicates_avoided = 0
        
        # Get all existing tests to check for duplicates
        # Use normalized file paths for comparison
        with conn.cursor() as cursor:
            cursor.execute(f"""
                SELECT test_id, file_path, class_name, method_name
                FROM {DB_SCHEMA}.test_registry
            """)
            existing_tests = {}
            for row in cursor.fetchall():
                test_id, file_path, class_name, method_name = row
                # Create a key from normalized path + class + method
                normalized_path = str(Path(file_path).resolve())
                key = (normalized_path, class_name or '', method_name)
                existing_tests[key] = test_id
        
        # Get current max test_id to continue numbering (only for truly new tests)
        with conn.cursor() as cursor:
            cursor.execute(f"""
                SELECT test_id FROM {DB_SCHEMA}.test_registry
                ORDER BY test_id DESC LIMIT 1
            """)
            result = cursor.fetchone()
            if result:
                last_id = result[0]
                test_id_counter = int(last_id.split('_')[1]) + 1
            else:
                test_id_counter = 1
        
        # Process each missing file
        for file_path_str in missing_files:
            try:
                filepath = Path(file_path_str)
                normalized_filepath = str(filepath.resolve())
                
                # Skip __init__.py and conftest.py (not actual test files)
                if filepath.name in ['__init__.py', 'conftest.py']:
                    continue
                
                # Parse the file
                tree = parse_file(filepath)
                if not tree:
                    continue
                
                # Get test type
                test_type = extract_test_type_enhanced(filepath)
                
                # Extract test classes
                test_classes = extract_test_classes(tree)
                all_test_methods = extract_test_methods(tree)
                
                # Extract tests
                file_tests = []
                if test_classes:
                    for test_class in test_classes:
                        class_name = test_class['name']
                        for method_name in test_class['methods']:
                            if method_name.startswith('test_'):
                                # Check if this test already exists
                                key = (normalized_filepath, class_name, method_name)
                                if key in existing_tests:
                                    duplicates_avoided += 1
                                    continue
                                
                                test_id = f"test_{test_id_counter:04d}"
                                test_id_counter += 1
                                file_tests.append({
                                    'test_id': test_id,
                                    'file_path': str(filepath),  # Store as provided, but check normalized
                                    'class_name': class_name,
                                    'method_name': method_name,
                                    'test_type': test_type,
                                    'line_number': None
                                })
                                # Add to existing_tests to avoid duplicates in same batch
                                existing_tests[key] = test_id
                else:
                    for test_method in all_test_methods:
                        # Check if this test already exists
                        key = (normalized_filepath, '', test_method['name'])
                        if key in existing_tests:
                            duplicates_avoided += 1
                            continue
                        
                        test_id = f"test_{test_id_counter:04d}"
                        test_id_counter += 1
                        file_tests.append({
                            'test_id': test_id,
                            'file_path': str(filepath),
                            'class_name': None,
                            'method_name': test_method['name'],
                            'test_type': test_type,
                            'line_number': test_method.get('line_number')
                        })
                        # Add to existing_tests to avoid duplicates in same batch
                        existing_tests[key] = test_id
                
                if file_tests:
                    all_tests.extend(file_tests)
                    indexed_count += 1
                    
            except Exception as e:
                error_count += 1
                errors.append({'file': file_path_str, 'error': str(e)})
        
        # Batch insert all tests
        if all_tests:
            batch_insert_test_registry(conn, all_tests)
            conn.commit()
        
        return {
            'indexed': indexed_count,
            'tests_added': len(all_tests),
            'skipped': len(missing_files) - indexed_count,
            'duplicates_avoided': duplicates_avoided,
            'errors': errors
        }
    finally:
        if should_close:
            conn.__exit__(None, None, None)


def diagnose_integration_tests(conn=None) -> Dict[str, Any]:
    """
    Diagnose why integration/e2e tests aren't being found.
    
    Args:
        conn: Optional database connection
    
    Returns:
        Dictionary with diagnostic information
    """
    should_close = False
    if conn is None:
        conn = get_connection().__enter__()
        should_close = True
    
    try:
        with conn.cursor() as cursor:
            # Check what integration tests exist in database
            cursor.execute(f"""
                SELECT test_id, file_path, test_type, class_name, method_name
                FROM {DB_SCHEMA}.test_registry
                WHERE test_type IN ('integration', 'e2e')
                ORDER BY test_type, file_path
            """)
            integration_tests = [
                {
                    'test_id': row[0],
                    'file_path': row[1],
                    'test_type': row[2],
                    'class_name': row[3],
                    'method_name': row[4]
                }
                for row in cursor.fetchall()
            ]
            
            # Check what references they have
            cursor.execute(f"""
                SELECT DISTINCT tr.test_id, tr.file_path, tr.test_type,
                       ri.production_class, ri.reference_type
                FROM {DB_SCHEMA}.test_registry tr
                LEFT JOIN {DB_SCHEMA}.reverse_index ri ON tr.test_id = ri.test_id
                WHERE tr.test_type IN ('integration', 'e2e')
                ORDER BY tr.file_path, ri.production_class
            """)
            integration_with_refs = [
                {
                    'test_id': row[0],
                    'file_path': row[1],
                    'test_type': row[2],
                    'production_class': row[3],
                    'reference_type': row[4]
                }
                for row in cursor.fetchall()
            ]
            
            # Check for specific known integration test
            cursor.execute(f"""
                SELECT test_id, file_path, test_type, class_name, method_name
                FROM {DB_SCHEMA}.test_registry
                WHERE file_path LIKE '%agent_flow%'
                   OR file_path LIKE '%test_agent_flow%'
            """)
            agent_flow_tests = [
                {
                    'test_id': row[0],
                    'file_path': row[1],
                    'test_type': row[2],
                    'class_name': row[3],
                    'method_name': row[4]
                }
                for row in cursor.fetchall()
            ]
            
            suggestions = []
            if len(integration_tests) == 0:
                suggestions.append("No integration/e2e tests found in database")
                suggestions.append("Run test analysis steps to index integration tests")
            else:
                suggestions.append(f"Found {len(integration_tests)} integration/e2e tests")
                if len(agent_flow_tests) == 0:
                    suggestions.append("test_agent_flow.py not found - may need re-indexing")
            
            return {
                'total_integration_tests': len(integration_tests),
                'integration_tests': integration_tests,
                'integration_tests_with_refs': integration_with_refs,
                'agent_flow_tests': agent_flow_tests,
                'suggestions': suggestions
            }
    finally:
        if should_close:
            conn.__exit__(None, None, None)

```

`git_diff_processor/utils/__init__.py`

```python
"""
Utility modules for git diff processing.
"""

```

`git_diff_processor/__init__.py`

```python
"""
Git Diff Processor Package

This package processes git diff output to find which tests should be run
based on code changes. It queries the deterministic database to find
affected tests using the reverse index.
"""

__version__ = "1.0.0"

```

`o.txt`

```

```

`README.md`

```markdown
# sample_workflow_draftpoc
Draft Implementation for poc

```

`requirements.txt`

```
# Requirements for Test Impact Analysis System
# Python 3.7+ required

# PostgreSQL database adapter
# psycopg2-binary is recommended for easier installation (includes all dependencies)
# For production, you might want to use psycopg2 instead
psycopg2-binary>=2.9.0

# Environment variable management (.env file support)
python-dotenv>=1.0.0

# Optional: Enhanced AST parsing (for better docstring extraction)
# Uncomment if needed for advanced parsing features
# ast-comments>=1.1.3

# Note: Most functionality uses Python standard library:
# - ast (Abstract Syntax Tree parsing)
# - pathlib (File path handling)
# - json (JSON file operations)
# - re (Regular expressions)
# - datetime (Timestamp generation)
# - collections (Data structures)
# - typing (Type hints)
# - contextlib (Context managers)
# - sys, os (System operations)

```

`SYSTEM_FLOW_DIAGRAM.md`

```markdown
# Test Selection System - Visual Flow Diagram

## Complete System Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         PHASE 1: TEST REPOSITORY ANALYSIS                   │
└─────────────────────────────────────────────────────────────────────────────┘

    Test Repository (test_repository/)
           │
           ├─ test_*.py files
           ├─ *_test.py files
           ├─ unit/ directory
           ├─ integration/ directory
           └─ e2e/ directory
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Step 1: Scan Test Files (01_scan_test_files.py)            │
    │  • Recursively scan repository                               │
    │  • Identify test file patterns                               │
    │  • Extract file metadata                                     │
    │  • Categorize by directory type                              │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Step 2: Detect Framework (02_detect_framework.py)          │
    │  • Check pytest.ini, setup.cfg, pyproject.toml               │
    │  • Analyze conftest.py                                      │
    │  • Identify framework patterns                               │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Step 3: Build Test Registry (03_build_test_registry.py)    │
    │  • Parse files using AST                                     │
    │  • Extract test classes (class Test*)                       │
    │  • Extract test methods (def test_*)                        │
    │  • Generate unique test IDs                                  │
    │  • Categorize test types (unit/integration/e2e)             │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Step 4: Extract Dependencies (04_extract_static_dependencies)│
    │  • Parse imports from test files                             │
    │  • Extract string references (patch(), Mock())              │
    │  • Filter production code imports                            │
    │  • Map test → production code                                │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Step 5: Extract Metadata (05_extract_test_metadata.py)     │
    │  • Extract docstrings                                        │
    │  • Identify pytest markers                                   │
    │  • Detect async tests                                        │
    │  • Detect parameterized tests                                │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Step 6: Build Reverse Index (06_build_reverse_index.py)    │
    │  • Create production code → tests mapping                    │
    │  • Include reference types (direct_import, string_ref)      │
    │  • Enable fast lookup                                        │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Step 7: Map Structure (07_map_test_structure.py)            │
    │  • Map directory structure                                   │
    │  • Count files per category                                  │
    │  • Calculate statistics                                     │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Step 8: Generate Summary (08_generate_summary.py)          │
    │  • Aggregate all statistics                                  │
    │  • Create comprehensive report                               │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    JSON Output Files (test_analysis/outputs/)
    ├── 01_test_files.json
    ├── 02_framework_detection.json
    ├── 03_test_registry.json
    ├── 04_static_dependencies.json
    ├── 05_test_metadata.json
    ├── 06_reverse_index.json
    ├── 07_test_structure.json
    └── 08_summary_report.json


┌─────────────────────────────────────────────────────────────────────────────┐
│                      PHASE 2: DATABASE LOADING                               │
└─────────────────────────────────────────────────────────────────────────────┘

    JSON Output Files
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Step 1: Create Tables (01_create_tables.py)                 │
    │  • Create PostgreSQL schema                                   │
    │  • Create test_registry table                                 │
    │  • Create test_dependencies table                             │
    │  • Create reverse_index table                                 │
    │  • Create test_metadata table                                 │
    │  • Create test_structure table                                │
    │  • Create indexes for performance                            │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Step 2: Load Test Registry (02_load_test_registry.py)      │
    │  • Read 03_test_registry.json                                │
    │  • Batch insert into test_registry table                     │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Step 3: Load Dependencies (03_load_test_dependencies.py)   │
    │  • Read 04_static_dependencies.json                           │
    │  • Batch insert into test_dependencies table                 │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Step 4: Load Reverse Index (04_load_reverse_index.py)      │
    │  • Read 06_reverse_index.json                                │
    │  • Batch insert into reverse_index table                     │
    │  • Include reference_type (direct_import, string_ref)       │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Step 5: Load Metadata (05_load_test_metadata.py)           │
    │  • Read 05_test_metadata.json                                │
    │  • Batch insert into test_metadata table                     │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Step 6: Load Structure (06_load_test_structure.py)          │
    │  • Read 07_test_structure.json                               │
    │  • Batch insert into test_structure table                    │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Step 7: Verify Data (07_verify_data.py)                     │
    │  • Count records in each table                               │
    │  • Verify foreign key relationships                          │
    │  • Run sample queries                                        │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │           PostgreSQL Database (planon.planon1)                │
    │  ┌────────────────────────────────────────────────────┐     │
    │  │ test_registry                                      │     │
    │  │ • test_id (PK)                                     │     │
    │  │ • file_path                                        │     │
    │  │ • class_name                                       │     │
    │  │ • method_name                                      │     │
    │  │ • test_type                                        │     │
    │  └────────────────────────────────────────────────────┘     │
    │  ┌────────────────────────────────────────────────────┐     │
    │  │ reverse_index                                       │     │
    │  │ • production_class                                  │     │
    │  │ • test_id (FK)                                      │     │
    │  │ • reference_type                                    │     │
    │  │ • test_file_path                                    │     │
    │  └────────────────────────────────────────────────────┘     │
    │  ┌────────────────────────────────────────────────────┐     │
    │  │ test_dependencies                                   │     │
    │  │ • test_id (FK)                                      │     │
    │  │ • referenced_class                                 │     │
    │  └────────────────────────────────────────────────────┘     │
    │  ┌────────────────────────────────────────────────────┐     │
    │  │ test_metadata                                       │     │
    │  │ • test_id (FK)                                      │     │
    │  │ • description, markers, etc.                        │     │
    │  └────────────────────────────────────────────────────┘     │
    └──────────────────────────────────────────────────────────────┘


┌─────────────────────────────────────────────────────────────────────────────┐
│                    PHASE 3: GIT DIFF PROCESSING                              │
└─────────────────────────────────────────────────────────────────────────────┘

    Git Diff File (diff_commit1.txt)
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Step 1: Read Git Diff (git_diff_processor.py)              │
    │  • Read diff file                                            │
    │  • Parse unified diff format                                 │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Step 2: Parse Git Diff (diff_parser.py)                    │
    │  • Extract changed files                                     │
    │  • Extract changed classes                                   │
    │  • Extract changed methods                                   │
    │  • Filter production Python files                            │
    │  • Extract module names from file paths                      │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Step 3: Build Search Queries (diff_parser.py)              │
    │  • Generate exact class matches                              │
    │  • Generate module patterns (agent.*, api.*)                 │
    │  • Generate test file candidates                             │
    └──────────────────────────────────────────────────────────────┘


┌─────────────────────────────────────────────────────────────────────────────┐
│                      PHASE 4: TEST SELECTION                                 │
└─────────────────────────────────────────────────────────────────────────────┘

    Search Queries
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Strategy 1: Direct Test Files (find_direct_test_files)    │
    │  • Match test file names (test_agent_pool.py)               │
    │  • Multi-strategy pattern matching                           │
    │  • Search by basename, full path, etc.                      │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Strategy 2: Integration/E2E Tests                         │
    │  • Find integration tests for changed modules                │
    │  • Query by test_type = 'integration' or 'e2e'              │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Strategy 3: Exact Matches (query_tests_for_classes)       │
    │  • Query reverse_index for exact class matches               │
    │  • Match sub-paths (api.routes → api.routes.get_agent)        │
    │  • Include string references (patch/Mock calls)             │
    │  • Prioritize exact matches over sub-paths                  │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Strategy 4: Module Patterns (query_tests_module_pattern)  │
    │  • Query for module patterns (agent.*)                      │
    │  • Prefer direct references over indirect                    │
    │  • Filter by specific classes if provided                   │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Aggregate Results (find_affected_tests)                    │
    │  • Combine all strategies                                    │
    │  • Deduplicate test IDs                                      │
    │  • Categorize by confidence (high/medium)                    │
    │  • Track match details for each test                         │
    └──────────────────────────────────────────────────────────────┘


┌─────────────────────────────────────────────────────────────────────────────┐
│                      PHASE 5: OUTPUT GENERATION                              │
└─────────────────────────────────────────────────────────────────────────────┘

    Test Selection Results
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Display Results (display_results)                           │
    │  • Show high confidence matches                              │
    │  • Show medium confidence matches                            │
    │  • Show unused tests                                         │
    │  • Display summary statistics                                │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    ┌──────────────────────────────────────────────────────────────┐
    │  Save to File (save_results_to_file)                         │
    │  • Generate complete output file                             │
    │  • Include ALL tests (not truncated)                         │
    │  • Include match details                                     │
    │  • Include unused tests                                      │
    │  • Save to git_diff_processor/outputs/                       │
    └──────────────────────────────────────────────────────────────┘
           │
           ▼
    Output File (test_selection_<diff_name>_<timestamp>.txt)
    • Complete test list
    • Match reasons
    • Test metadata
    • Summary statistics


┌─────────────────────────────────────────────────────────────────────────────┐
│                            KEY COMPONENTS                                    │
└─────────────────────────────────────────────────────────────────────────────┘

    AST Parser (ast_parser.py)
    • parse_file() - Parse Python files to AST
    • extract_imports() - Extract import statements
    • extract_string_references() - Extract patch()/Mock() calls
    • extract_classes() - Extract class definitions
    • extract_functions() - Extract function definitions

    Database Helpers (db_helpers.py)
    • get_tests_for_production_class() - Query tests for a class
    • batch_insert_test_registry() - Efficient batch inserts
    • batch_insert_reverse_index() - Batch insert reverse index

    Diff Parser (diff_parser.py)
    • parse_git_diff() - Parse git diff format
    • extract_production_classes_from_file() - Extract module names
    • extract_test_file_candidates() - Generate test file patterns
    • build_search_queries() - Build search strategy

    File Scanner (file_scanner.py)
    • scan_directory() - Recursively scan test repository
    • _categorize_directory() - Categorize test types
    • Multi-strategy test file discovery


┌─────────────────────────────────────────────────────────────────────────────┐
│                          DATA FLOW SUMMARY                                   │
└─────────────────────────────────────────────────────────────────────────────┘

    Test Files → AST Parsing → JSON Files → Database → Query → Results → Output

    Key Transformations:
    1. Test files → Test registry (test_id, class, method)
    2. Test files → Dependencies (test → production code)
    3. Dependencies → Reverse index (production code → tests)
    4. Git diff → Changed modules/classes
    5. Changed modules → Database queries → Affected tests
    6. Affected tests → Formatted output (console + file)

```

`test_analysis/01_scan_test_files.py`

```python
"""
Step 1: Scan Test Files

This script discovers all test files in the test repository.
It recursively scans the directory and identifies test files based on naming patterns.

What it does:
1. Scans the test_repository directory recursively
2. Identifies test files (test_*.py, *_test.py patterns)
3. Extracts file metadata (path, size, line count)
4. Categorizes files by directory (unit, integration, e2e)
5. Displays results in console
6. Saves results to JSON file

Run this script:
    python test_analysis/01_scan_test_files.py
"""

from pathlib import Path
import sys
 
# Add utils to path so we can import our utility modules
sys.path.insert(0, str(Path(__file__).parent))

from utils.file_scanner import scan_directory, get_file_metadata, group_files_by_category
from utils.output_formatter import (
    print_header, print_section, print_item, print_list, 
    print_summary, save_json, print_progress
)

# Configuration: Path to test repository
TEST_REPO_PATH = Path(__file__).parent.parent / "test_repository"
OUTPUT_DIR = Path(__file__).parent / "outputs"
OUTPUT_FILE = OUTPUT_DIR / "01_test_files.json"


def main():
    """
    Main function to scan test files.
    
    This function:
    1. Scans the test repository directory
    2. Extracts metadata for each test file
    3. Groups files by category
    4. Displays results
    5. Saves to JSON
    """
    # Print header
    print_header("Step 1: Scanning Test Files")
    print()
    
    # Check if test repository exists
    if not TEST_REPO_PATH.exists():
        print(f"Error: Test repository not found at {TEST_REPO_PATH}")
        print("Please ensure the test_repository directory exists.")
        return
    
    print_section(f"Scanning directory: {TEST_REPO_PATH}")
    print()
    
    # Step 1: Scan for test files
    print_section("Discovering test files...")
    test_files = scan_directory(TEST_REPO_PATH)
    
    if not test_files:
        print("No test files found!")
        return
    
    print_item("Found test files:", len(test_files))
    print()
    
    # Step 2: Extract metadata for each file
    print_section("Extracting file metadata...")
    file_metadata = []
    total_lines = 0
    total_size = 0
    
    for i, filepath in enumerate(test_files):
        print_progress(i + 1, len(test_files), "files")
        metadata = get_file_metadata(filepath)
        file_metadata.append(metadata)
        total_lines += metadata['line_count']
        total_size += metadata['size_bytes']
    
    print()  # New line after progress
    print()
    
    # Step 3: Group files by category
    print_section("Categorizing files...")
    grouped_files = group_files_by_category(test_files)
    
    # Display categorized results
    for category, files in grouped_files.items():
        if files:
            print_item(f"{category.capitalize()} tests:", len(files))
    
    print()
    
    # Step 4: Display sample files
    print_section("Sample test files found:")
    print_list([f"{m['path']} ({m['line_count']} lines)" for m in file_metadata[:10]], 
               max_items=10)
    print()
    
    # Step 5: Prepare output data
    output_data = {
        "scan_directory": str(TEST_REPO_PATH),
        "total_files": len(test_files),
        "total_lines": total_lines,
        "total_size_bytes": total_size,
        "categories": {
            category: len(files) 
            for category, files in grouped_files.items()
        },
        "files": file_metadata
    }
    
    # Step 6: Display summary
    print_section("Summary:")
    print_summary({
        "total_files": len(test_files),
        "total_lines": total_lines,
        "total_size_kb": round(total_size / 1024, 2),
        "unit_tests": len(grouped_files['unit']),
        "integration_tests": len(grouped_files['integration']),
        "e2e_tests": len(grouped_files['e2e']),
        "other_tests": len(grouped_files['other'])
    })
    print()
    
    # Step 7: Save to JSON
    print_section("Saving results...")
    save_json(output_data, OUTPUT_FILE)
    print()
    
    print_header("Step 1 Complete!")
    print(f"Results saved to: {OUTPUT_FILE}")
    print(f"Found {len(test_files)} test files with {total_lines} total lines of code")


if __name__ == "__main__":
    main()

```

`test_analysis/02_detect_framework.py`

```python
"""
Step 2: Detect Test Framework

This script identifies which test framework is being used in the test repository.
It analyzes test files, configuration files, and import patterns to determine the framework.

What it does:
1. Checks for pytest.ini or setup.cfg configuration files
2. Analyzes test file imports (pytest, unittest)
3. Checks for framework-specific patterns (@pytest.mark, unittest.TestCase)
4. Analyzes conftest.py for pytest-specific configuration
5. Displays framework detection results
6. Saves results to JSON file

Run t his script:
    python test_analysis/02_detect_framework.py
"""

from pathlib import Path
import sys
import re

# Add utils to path
sys.path.insert(0, str(Path(__file__).parent))

from utils.file_scanner import scan_directory
from utils.ast_parser import parse_file, extract_imports
from utils.output_formatter import (
    print_header, print_section, print_item, print_list,
    save_json, print_progress
)

# Configuration
TEST_REPO_PATH = Path(__file__).parent.parent / "test_repository"
OUTPUT_DIR = Path(__file__).parent / "outputs"
OUTPUT_FILE = OUTPUT_DIR / "02_framework_detection.json"


def check_config_files(repo_path: Path) -> dict:
    """
    Check for test framework configuration files.
    
    Looks for:
    - pytest.ini
    - setup.cfg (may contain pytest config)
    - pyproject.toml (may contain pytest config)
    - tox.ini (may contain pytest config)
    
    Returns:
        Dictionary with configuration file findings
    """
    findings = {
        "pytest_ini": None,
        "setup_cfg": None,
        "pyproject_toml": None,
        "tox_ini": None,
        "config_found": False
    }
    
    # Check for pytest.ini
    pytest_ini = repo_path / "pytest.ini"
    if pytest_ini.exists():
        findings["pytest_ini"] = str(pytest_ini)
        findings["config_found"] = True
        # Try to read basic info
        try:
            with open(pytest_ini, 'r', encoding='utf-8') as f:
                content = f.read()
                findings["pytest_ini_content"] = content[:500]  # First 500 chars
        except:
            pass
    
    # Check for setup.cfg
    setup_cfg = repo_path / "setup.cfg"
    if setup_cfg.exists():
        findings["setup_cfg"] = str(setup_cfg)
        findings["config_found"] = True
    
    # Check for pyproject.toml
    pyproject_toml = repo_path / "pyproject.toml"
    if pyproject_toml.exists():
        findings["pyproject_toml"] = str(pyproject_toml)
        findings["config_found"] = True
    
    # Check for tox.ini
    tox_ini = repo_path / "tox.ini"
    if tox_ini.exists():
        findings["tox_ini"] = str(tox_ini)
        findings["config_found"] = True
    
    return findings


def check_conftest(repo_path: Path) -> dict:
    """
    Check for conftest.py file (pytest-specific).
    
    Returns:
        Dictionary with conftest.py findings
    """
    findings = {
        "conftest_found": False,
        "conftest_path": None,
        "has_fixtures": False,
        "has_pytest_imports": False
    }
    
    conftest = repo_path / "conftest.py"
    if conftest.exists():
        findings["conftest_found"] = True
        findings["conftest_path"] = str(conftest)
        
        # Parse conftest.py to check for pytest usage
        tree = parse_file(conftest)
        if tree:
            imports = extract_imports(tree)
            
            # Check for pytest imports
            if any('pytest' in imp.lower() for imp in imports['all_imports']):
                findings["has_pytest_imports"] = True
            
            # Check for @pytest.fixture decorators (simple regex check)
            try:
                with open(conftest, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if '@pytest.fixture' in content or 'pytest.fixture' in content:
                        findings["has_fixtures"] = True
            except:
                pass
    
    return findings


def analyze_test_files(repo_path: Path) -> dict:
    """
    Analyze test files to detect framework usage patterns.
    
    Returns:
        Dictionary with framework detection results
    """
    test_files = scan_directory(repo_path)
    
    framework_indicators = {
        "pytest": {
            "files_with_import": 0,
            "files_with_markers": 0,
            "files_with_fixtures": 0,
            "total_indicators": 0
        },
        "unittest": {
            "files_with_import": 0,
            "files_with_testcase": 0,
            "total_indicators": 0
        }
    }
    
    # Analyze each test file
    for i, filepath in enumerate(test_files):
        print_progress(i + 1, len(test_files), "files")
        
        tree = parse_file(filepath)
        if not tree:
            continue
        
        # Extract imports
        imports = extract_imports(tree)
        all_imports_str = ' '.join(imports['all_imports']).lower()
        
        # Check for pytest
        if 'pytest' in all_imports_str:
            framework_indicators["pytest"]["files_with_import"] += 1
            framework_indicators["pytest"]["total_indicators"] += 1
        
        # Check for unittest
        if 'unittest' in all_imports_str:
            framework_indicators["unittest"]["files_with_import"] += 1
            framework_indicators["unittest"]["total_indicators"] += 1
        
        # Check file content for patterns
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                
                # Check for pytest markers
                if '@pytest.mark' in content or 'pytest.mark' in content:
                    framework_indicators["pytest"]["files_with_markers"] += 1
                    framework_indicators["pytest"]["total_indicators"] += 1
                
                # Check for pytest fixtures
                if '@pytest.fixture' in content:
                    framework_indicators["pytest"]["files_with_fixtures"] += 1
                    framework_indicators["pytest"]["total_indicators"] += 1
                
                # Check for unittest.TestCase
                if 'TestCase' in content and 'unittest' in content:
                    framework_indicators["unittest"]["files_with_testcase"] += 1
                    framework_indicators["unittest"]["total_indicators"] += 1
        except:
            pass
    
    print()  # New line after progress
    
    return framework_indicators


def determine_framework(config_findings: dict, conftest_findings: dict, 
                        file_indicators: dict) -> dict:
    """
    Determine the primary test framework based on all indicators.
    
    Returns:
        Dictionary with framework determination results
    """
    result = {
        "primary_framework": "unknown",
        "confidence": "low",
        "indicators": []
    }
    
    pytest_score = 0
    unittest_score = 0
    
    # Score from config files
    if config_findings.get("pytest_ini"):
        pytest_score += 3
        result["indicators"].append("pytest.ini found")
    
    # Score from conftest.py
    if conftest_findings.get("conftest_found"):
        pytest_score += 2
        result["indicators"].append("conftest.py found")
    
    if conftest_findings.get("has_pytest_imports"):
        pytest_score += 1
        result["indicators"].append("conftest.py uses pytest")
    
    # Score from file analysis
    pytest_file_score = file_indicators["pytest"]["total_indicators"]
    unittest_file_score = file_indicators["unittest"]["total_indicators"]
    
    pytest_score += pytest_file_score
    unittest_score += unittest_file_score
    
    # Determine primary framework
    if pytest_score > unittest_score and pytest_score > 0:
        result["primary_framework"] = "pytest"
        if pytest_score >= 5:
            result["confidence"] = "high"
        elif pytest_score >= 3:
            result["confidence"] = "medium"
    elif unittest_score > pytest_score and unittest_score > 0:
        result["primary_framework"] = "unittest"
        if unittest_score >= 3:
            result["confidence"] = "high"
        else:
            result["confidence"] = "medium"
    elif pytest_score == unittest_score and pytest_score > 0:
        result["primary_framework"] = "mixed"
        result["confidence"] = "medium"
    
    result["pytest_score"] = pytest_score
    result["unittest_score"] = unittest_score
    
    return result


def main():
    """Main function to detect test framework."""
    print_header("Step 2: Detecting Test Framework")
    print()
    
    # Check if test repository exists
    if not TEST_REPO_PATH.exists():
        print(f"Error: Test repository not found at {TEST_REPO_PATH}")
        return
    
    # Step 1: Check configuration files
    print_section("Checking configuration files...")
    config_findings = check_config_files(TEST_REPO_PATH)
    
    if config_findings["config_found"]:
        print_item("Configuration files found:", "Yes")
        if config_findings["pytest_ini"]:
            print_item("  - pytest.ini:", config_findings["pytest_ini"])
        if config_findings["setup_cfg"]:
            print_item("  - setup.cfg:", config_findings["setup_cfg"])
        if config_findings["pyproject_toml"]:
            print_item("  - pyproject.toml:", config_findings["pyproject_toml"])
    else:
        print_item("Configuration files found:", "No")
    print()
    
    # Step 2: Check for conftest.py
    print_section("Checking for conftest.py...")
    conftest_findings = check_conftest(TEST_REPO_PATH)
    
    if conftest_findings["conftest_found"]:
        print_item("conftest.py found:", "Yes")
        print_item("  Path:", conftest_findings["conftest_path"])
        print_item("  Has pytest imports:", conftest_findings["has_pytest_imports"])
        print_item("  Has fixtures:", conftest_findings["has_fixtures"])
    else:
        print_item("conftest.py found:", "No")
    print()
    
    # Step 3: Analyze test files
    print_section("Analyzing test files for framework patterns...")
    file_indicators = analyze_test_files(TEST_REPO_PATH)
    
    print_section("Framework indicators found:")
    print_item("Pytest indicators:", file_indicators["pytest"]["total_indicators"])
    print_item("  - Files with pytest import:", file_indicators["pytest"]["files_with_import"])
    print_item("  - Files with pytest markers:", file_indicators["pytest"]["files_with_markers"])
    print_item("  - Files with pytest fixtures:", file_indicators["pytest"]["files_with_fixtures"])
    print()
    print_item("Unittest indicators:", file_indicators["unittest"]["total_indicators"])
    print_item("  - Files with unittest import:", file_indicators["unittest"]["files_with_import"])
    print_item("  - Files with TestCase:", file_indicators["unittest"]["files_with_testcase"])
    print()
    
    # Step 4: Determine primary framework
    print_section("Determining primary framework...")
    framework_result = determine_framework(config_findings, conftest_findings, file_indicators)
    
    print_item("Primary framework:", framework_result["primary_framework"])
    print_item("Confidence:", framework_result["confidence"])
    print_item("Pytest score:", framework_result["pytest_score"])
    print_item("Unittest score:", framework_result["unittest_score"])
    print()
    
    if framework_result["indicators"]:
        print_section("Key indicators:")
        for indicator in framework_result["indicators"]:
            print_item("  -", indicator)
        print()
    
    # Step 5: Prepare output data
    output_data = {
        "primary_framework": framework_result["primary_framework"],
        "confidence": framework_result["confidence"],
        "pytest_score": framework_result["pytest_score"],
        "unittest_score": framework_result["unittest_score"],
        "config_files": config_findings,
        "conftest": conftest_findings,
        "file_indicators": file_indicators,
        "indicators": framework_result["indicators"]
    }
    
    # Step 6: Save to JSON
    print_section("Saving results...")
    save_json(output_data, OUTPUT_FILE)
    print()
    
    print_header("Step 2 Complete!")
    print(f"Framework detected: {framework_result['primary_framework']} (confidence: {framework_result['confidence']})")
    print(f"Results saved to: {OUTPUT_FILE}")


if __name__ == "__main__":
    main()

```

`test_analysis/03_build_test_registry.py`

```python
"""
Step 3: Build Test Registry

This script creates a complete inventory of all tests in the repository.
It extracts test classes and test methods from each test file and assigns unique IDs.

What it does:
1. Loads test files from Step 1 output (or scans directly)
2. Parses each test file using AST
3. Extracts test classes (class Test*)
4. Extracts test methods (def test_*)
5. Generates unique test IDs
6. Maps test_id → file_path, class_name, method_name, test_type
7. Displays test count summary
8. Saves complete registry to JSON file

Run this script:
    python test_analysis/03_build_test_registry.py
"""

from pathlib import Path
import sys
import json

# Add utils to path
sys.path.insert(0, str(Path(__file__).parent))

from utils.file_scanner import scan_directory, get_file_metadata
from utils.ast_parser import parse_file, extract_test_classes, extract_test_methods
from utils.output_formatter import (
    print_header, print_section, print_item, print_list,
    save_json, print_progress, print_summary
)

# Configuration
TEST_REPO_PATH = Path(__file__).parent.parent / "test_repository"
OUTPUT_DIR = Path(__file__).parent / "outputs"
STEP1_OUTPUT = OUTPUT_DIR / "01_test_files.json"
OUTPUT_FILE = OUTPUT_DIR / "03_test_registry.json"


def load_step1_output() -> list:
    """
    Load test files from Step 1 output if available.
    
    Returns:
        List of file paths from Step 1, or empty list if not found
    """
    if STEP1_OUTPUT.exists():
        try:
            with open(STEP1_OUTPUT, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # Extract file paths from Step 1 output
                files_data = data.get('data', {}).get('files', [])
                return [Path(f['path']) for f in files_data]
        except Exception as e:
            print(f"Warning: Could not load Step 1 output: {e}")
    
    return []


def extract_test_type_enhanced(filepath: Path) -> str:
    """
    Enhanced test type detection that works with any repository structure.
    
    Args:
        filepath: Path to the test file
    
    Returns:
        Test type: 'unit', 'integration', or 'e2e'
    """
    from utils.file_scanner import _categorize_directory
    category = _categorize_directory(filepath)
    
    # Map category to test_type
    if category == 'integration':
        return 'integration'
    elif category == 'e2e':
        return 'e2e'
    else:
        return 'unit'  # Default


def extract_tests_from_file(filepath: Path, test_id_counter: int) -> tuple:
    """
    Extract all tests from a single test file.
    
    Args:
        filepath: Path to the test file
        test_id_counter: Starting counter for test IDs
    
    Returns:
        Tuple of (list of test dictionaries, new counter value)
    """
    tests = []
    
    # Parse the file
    tree = parse_file(filepath)
    if not tree:
        return tests, test_id_counter
    
    # Get file metadata
    file_metadata = get_file_metadata(filepath)
    # Use enhanced test type detection
    test_type = extract_test_type_enhanced(filepath)
    
    # Extract test classes
    test_classes = extract_test_classes(tree)
    
    # Extract standalone test methods (not in classes)
    all_test_methods = extract_test_methods(tree)
    
    # If there are test classes, extract methods from classes
    if test_classes:
        for test_class in test_classes:
            class_name = test_class['name']
            
            # Get methods for this class
            for method_name in test_class['methods']:
                if method_name.startswith('test_'):
                    test_id = f"test_{test_id_counter:04d}"
                    test_id_counter += 1
                    
                    tests.append({
                        "test_id": test_id,
                        "file_path": str(filepath),
                        "class_name": class_name,
                        "method_name": method_name,
                        "test_type": test_type,
                        "line_number": None  # Could be extracted if needed
                    })
    else:
        # No test classes, these are standalone test functions
        for test_method in all_test_methods:
            test_id = f"test_{test_id_counter:04d}"
            test_id_counter += 1
            
            tests.append({
                "test_id": test_id,
                "file_path": str(filepath),
                "class_name": None,  # Standalone function
                "method_name": test_method['name'],
                "test_type": test_type,
                "line_number": test_method.get('line_number')
            })
    
    return tests, test_id_counter


def build_test_registry() -> dict:
    """
    Build complete test registry from all test files.
    
    Returns:
        Dictionary with test registry data
    """
    # Try to load from Step 1, otherwise scan directly
    test_files = load_step1_output()
    if not test_files:
        print_section("Step 1 output not found, scanning test repository directly...")
        test_files = scan_directory(TEST_REPO_PATH)
    
    if not test_files:
        print("Error: No test files found!")
        return {}
    
    print_section(f"Processing {len(test_files)} test files...")
    
    all_tests = []
    test_id_counter = 1
    
    # Process each test file
    for i, filepath in enumerate(test_files):
        print_progress(i + 1, len(test_files), "files")
        
        file_tests, test_id_counter = extract_tests_from_file(filepath, test_id_counter)
        all_tests.extend(file_tests)
    
    print()  # New line after progress
    
    # Count statistics
    total_tests = len(all_tests)
    total_classes = len(set(t['class_name'] for t in all_tests if t['class_name']))
    
    # Count by test type
    by_type = {}
    for test in all_tests:
        test_type = test['test_type']
        by_type[test_type] = by_type.get(test_type, 0) + 1
    
    # Count by file
    by_file = {}
    for test in all_tests:
        file_path = test['file_path']
        by_file[file_path] = by_file.get(file_path, 0) + 1
    
    return {
        "total_tests": total_tests,
        "total_classes": total_classes,
        "total_files": len(test_files),
        "tests_by_type": by_type,
        "tests_by_file": by_file,
        "tests": all_tests
    }


def main():
    """Main function to build test registry."""
    print_header("Step 3: Building Test Registry")
    print()
    
    # Check if test repository exists
    if not TEST_REPO_PATH.exists():
        print(f"Error: Test repository not found at {TEST_REPO_PATH}")
        return
    
    # Step 1: Build registry
    print_section("Extracting tests from files...")
    registry_data = build_test_registry()
    
    if not registry_data:
        print("Error: Failed to build test registry!")
        return
    
    print()
    
    # Step 2: Display summary
    print_section("Registry Summary:")
    print_summary({
        "total_tests": registry_data['total_tests'],
        "total_classes": registry_data['total_classes'],
        "total_files": registry_data['total_files']
    })
    print()
    
    # Step 3: Display breakdown by type
    print_section("Tests by Type:")
    for test_type, count in sorted(registry_data['tests_by_type'].items()):
        print_item(f"{test_type.capitalize()}:", count)
    print()
    
    # Step 4: Display top files by test count
    print_section("Top Files by Test Count:")
    sorted_files = sorted(
        registry_data['tests_by_file'].items(),
        key=lambda x: x[1],
        reverse=True
    )[:10]
    
    for file_path, count in sorted_files:
        file_name = Path(file_path).name
        print_item(f"{file_name}:", f"{count} tests")
    print()
    
    # Step 5: Display sample tests
    print_section("Sample Tests (first 10):")
    sample_tests = registry_data['tests'][:10]
    for test in sample_tests:
        test_desc = f"{test['method_name']}"
        if test['class_name']:
            test_desc = f"{test['class_name']}.{test_desc}"
        print_item(f"{test['test_id']}:", test_desc)
    print()
    
    # Step 6: Save to JSON
    print_section("Saving results...")
    save_json(registry_data, OUTPUT_FILE)
    print()
    
    print_header("Step 3 Complete!")
    print(f"Registered {registry_data['total_tests']} tests from {registry_data['total_files']} files")
    print(f"Results saved to: {OUTPUT_FILE}")


if __name__ == "__main__":
    main()

```

`test_analysis/04b_extract_function_calls.py`

```python
"""
Step 4b: Extract Function Calls

This script extracts function-level mappings from test files.
It identifies which specific functions each test calls or patches.

What it does:
1. Loads test registry from Step 3
2. Parses each test file using AST
3. Extracts function calls from test method bodies (e.g., agent.initialize())
4. Extracts string references from patch() calls (e.g., 'agent.langgraph_agent.initialize')
5. Maps test → module.function relationships
6. Displays function call statistics
7. Saves mappings to JSON file

Run this script:
    python test_analysis/04b_extract_function_calls.py
"""

from pathlib import Path
import sys
import json
from typing import Optional

# Add utils to path
sys.path.insert(0, str(Path(__file__).parent))

from utils.ast_parser import parse_file, extract_function_calls, extract_string_references
from utils.output_formatter import (
    print_header, print_section, print_item, print_list,
    save_json, print_progress, print_summary
)

# Configuration
OUTPUT_DIR = Path(__file__).parent / "outputs"
STEP3_OUTPUT = OUTPUT_DIR / "03_test_registry.json"
OUTPUT_FILE = OUTPUT_DIR / "04b_function_calls.json"

# Test framework imports to exclude (these are not production code)
TEST_FRAMEWORK_IMPORTS = {
    'pytest', 'unittest', 'mock', 'unittest.mock',
    'pytest_mock', 'pytest_asyncio', 'pytest_cov',
    'test', 'tests', 'testing'
}


def is_production_import(import_name: str) -> bool:
    """
    Check if an import is likely production code (not test framework).
    
    Args:
        import_name: The import module name
    
    Returns:
        True if it's likely production code, False if it's test framework
    """
    # Check if it starts with any test framework name
    for test_framework in TEST_FRAMEWORK_IMPORTS:
        if import_name.startswith(test_framework):
            return False
    
    # Check if it's a standard library import (common ones)
    stdlib_modules = {
        'os', 'sys', 'pathlib', 'json', 'datetime', 'typing',
        'collections', 'itertools', 'functools', 'asyncio',
        'abc', 'dataclasses', 'enum', 'logging', 're'
    }
    
    # Split by dot to check first part
    first_part = import_name.split('.')[0]
    if first_part in stdlib_modules:
        return False  # Standard library, not production code
    
    return True


def extract_function_from_string_ref(string_ref: str) -> tuple:
    """
    Extract module and function name from a string reference.
    
    Examples:
        'agent.langgraph_agent.initialize' → ('agent.langgraph_agent', 'initialize')
        'agent.langgraph_agent.LangGraphAgent' → ('agent.langgraph_agent', 'LangGraphAgent')
        'agent' → (None, None)  # Too short, likely not a function
    
    Args:
        string_ref: String reference like 'agent.langgraph_agent.initialize'
    
    Returns:
        Tuple of (module_name, function_name) or (None, None) if can't parse
    """
    if not string_ref or '.' not in string_ref:
        return (None, None)
    
    # Split on last dot to separate module from function/class
    parts = string_ref.rsplit('.', 1)
    if len(parts) != 2:
        return (None, None)
    
    module_name, function_name = parts
    
    # Filter out non-production modules
    if not is_production_import(module_name):
        return (None, None)
    
    # Module should have at least one dot (e.g., 'agent.langgraph_agent')
    # Single word like 'agent' is too broad
    if '.' not in module_name:
        return (None, None)
    
    return (module_name, function_name)


def resolve_object_to_module(object_name: str, imports_data: dict, file_path: str) -> Optional[str]:
    """
    Attempt to resolve an object name to its module.
    
    For example, if test has 'from agent.langgraph_agent import LangGraphAgent'
    and calls 'agent.initialize()', try to resolve 'agent' to 'agent.langgraph_agent'.
    
    This is a best-effort approach. For MVP, we'll store the object name as-is
    and rely on string references for precise matching.
    
    Args:
        object_name: Object name (e.g., 'agent')
        imports_data: Import data from extract_imports()
        file_path: Path to test file (for context)
    
    Returns:
        Module name if resolved, None otherwise
    """
    # Check from_imports for exact match
    for module, names in imports_data.get('from_imports', []):
        if object_name in names and is_production_import(module):
            return module
    
    # Check if object_name matches a class name in imports
    # This is heuristic - in real code, we'd need more sophisticated analysis
    return None


def extract_function_mappings_from_file(filepath: Path, test_methods: list, imports_data: dict) -> list:
    """
    Extract function-level mappings from a test file.
    
    Args:
        filepath: Path to the test file
        test_methods: List of test method names in this file
        imports_data: Import data for resolving object names
    
    Returns:
        List of function mapping dictionaries
    """
    tree = parse_file(filepath)
    if not tree:
        return []
    
    mappings = []
    
    # Extract function calls from test methods
    function_calls = extract_function_calls(tree)
    
    # Create a map of test_method -> calls
    calls_by_method = {fc['test_method']: fc['calls'] for fc in function_calls}
    
    # Extract string references (patch() calls)
    string_refs = extract_string_references(tree)
    
    # Process each test method
    for test_method in test_methods:
        # Get function calls for this test method
        calls = calls_by_method.get(test_method, [])
        
        # Process direct calls and method calls
        for call in calls:
            function_name = call['function']
            object_name = call.get('object')
            call_type = call['type']
            
            # Try to resolve object to module
            module_name = None
            if object_name:
                module_name = resolve_object_to_module(object_name, imports_data, filepath)
            
            # If we couldn't resolve, we'll store it with object_name
            # The database query can match on function_name alone
            mappings.append({
                'test_method': test_method,
                'function_name': function_name,
                'module_name': module_name,  # May be None
                'object_name': object_name,  # For context
                'call_type': call_type,  # 'direct' or 'method'
                'source': 'method_call',
                'line_number': call.get('line_number')
            })
        
        # Process string references (patch() calls)
        # These are more precise - they have full module.function paths
        for string_ref in string_refs:
            module_name, function_name = extract_function_from_string_ref(string_ref)
            
            if module_name and function_name:
                mappings.append({
                    'test_method': test_method,
                    'function_name': function_name,
                    'module_name': module_name,
                    'object_name': None,
                    'call_type': 'patch_ref',
                    'source': 'patch_ref',
                    'line_number': None
                })
    
    return mappings


def build_function_mapping() -> dict:
    """
    Build function-level mapping for all tests.
    
    Returns:
        Dictionary with function mapping data
    """
    # Load test registry from Step 3
    if not STEP3_OUTPUT.exists():
        print("Error: Step 3 output not found. Please run Step 3 first.")
        return {}
    
    with open(STEP3_OUTPUT, 'r', encoding='utf-8') as f:
        registry_data = json.load(f)['data']
    
    tests = registry_data['tests']
    test_files = {}
    
    # Group tests by file
    for test in tests:
        file_path = test['file_path']
        if file_path not in test_files:
            test_files[file_path] = []
        test_files[file_path].append(test)
    
    print_section(f"Processing {len(test_files)} test files...")
    
    # Extract function mappings for each file
    all_mappings = []
    file_mappings = {}
    
    for i, (filepath_str, file_tests) in enumerate(test_files.items()):
        print_progress(i + 1, len(test_files), "files")
        
        filepath = Path(filepath_str)
        
        # Get imports for this file (to help resolve object names)
        tree = parse_file(filepath)
        imports_data = {}
        if tree:
            from utils.ast_parser import extract_imports
            imports_data = extract_imports(tree)
        
        # Get test method names
        test_method_names = [t['method_name'] for t in file_tests]
        
        # Extract function mappings
        file_function_mappings = extract_function_mappings_from_file(
            filepath, test_method_names, imports_data
        )
        
        # Map to test_ids
        for mapping in file_function_mappings:
            # Find the test that corresponds to this method
            matching_test = next(
                (t for t in file_tests if t['method_name'] == mapping['test_method']),
                None
            )
            
            if matching_test:
                all_mappings.append({
                    'test_id': matching_test['test_id'],
                    'file_path': filepath_str,
                    'class_name': matching_test.get('class_name'),
                    'method_name': mapping['test_method'],
                    'module_name': mapping['module_name'],
                    'function_name': mapping['function_name'],
                    'object_name': mapping.get('object_name'),
                    'call_type': mapping['call_type'],
                    'source': mapping['source'],
                    'line_number': mapping.get('line_number')
                })
        
        file_mappings[filepath_str] = file_function_mappings
    
    print()  # New line after progress
    
    # Statistics
    total_mappings = len(all_mappings)
    tests_with_function_calls = len(set(m['test_id'] for m in all_mappings))
    
    # Count by source type
    by_source = {}
    for mapping in all_mappings:
        source = mapping['source']
        by_source[source] = by_source.get(source, 0) + 1
    
    # Count by call type
    by_call_type = {}
    for mapping in all_mappings:
        call_type = mapping['call_type']
        by_call_type[call_type] = by_call_type.get(call_type, 0) + 1
    
    # Most called functions
    function_counts = {}
    for mapping in all_mappings:
        if mapping['module_name'] and mapping['function_name']:
            key = f"{mapping['module_name']}.{mapping['function_name']}"
            function_counts[key] = function_counts.get(key, 0) + 1
    
    top_functions = sorted(function_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    
    return {
        "total_tests": len(tests),
        "tests_with_function_calls": tests_with_function_calls,
        "total_mappings": total_mappings,
        "average_mappings_per_test": round(total_mappings / len(tests), 2) if tests else 0,
        "mappings_by_source": by_source,
        "mappings_by_call_type": by_call_type,
        "top_functions": dict(top_functions),
        "test_function_mappings": all_mappings
    }


def main():
    """Main function to extract function calls."""
    print_header("Step 4b: Extracting Function Calls")
    print()
    
    # Step 1: Build function mapping
    print_section("Analyzing test files for function calls...")
    mapping_data = build_function_mapping()
    
    if not mapping_data:
        print("Error: Failed to build function mapping!")
        return
    
    print()
    
    # Step 2: Display summary
    print_section("Function Mapping Summary:")
    print_summary({
        "total_tests": mapping_data['total_tests'],
        "tests_with_function_calls": mapping_data['tests_with_function_calls'],
        "total_mappings": mapping_data['total_mappings'],
        "average_mappings_per_test": mapping_data['average_mappings_per_test']
    })
    print()
    
    # Step 3: Display by source type
    print_section("Mappings by Source Type:")
    for source, count in sorted(mapping_data['mappings_by_source'].items(), key=lambda x: x[1], reverse=True):
        print_item(f"{source}:", count)
    print()
    
    # Step 4: Display by call type
    print_section("Mappings by Call Type:")
    for call_type, count in sorted(mapping_data['mappings_by_call_type'].items(), key=lambda x: x[1], reverse=True):
        print_item(f"{call_type}:", count)
    print()
    
    # Step 5: Display top functions
    if mapping_data['top_functions']:
        print_section("Most Called Functions (Top 10):")
        for func, count in list(mapping_data['top_functions'].items())[:10]:
            print_item(f"{func}:", count)
        print()
    
    # Step 6: Display sample mappings
    print_section("Sample Function Mappings (first 5):")
    sample_mappings = [m for m in mapping_data['test_function_mappings'] if m['module_name']][:5]
    for mapping in sample_mappings:
        test_desc = mapping['method_name']
        if mapping['class_name']:
            test_desc = f"{mapping['class_name']}.{test_desc}"
        func_desc = f"{mapping['module_name']}.{mapping['function_name']}"
        print_item(f"{mapping['test_id']} ({test_desc}):", func_desc)
        print_item("  Source:", mapping['source'])
    print()
    
    # Step 7: Save to JSON
    print_section("Saving results...")
    save_json(mapping_data, OUTPUT_FILE)
    print()
    
    print_header("Step 4b Complete!")
    print(f"Extracted {mapping_data['total_mappings']} function mappings from {mapping_data['total_tests']} tests")
    print(f"Results saved to: {OUTPUT_FILE}")


if __name__ == "__main__":
    main()

```

`test_analysis/04_extract_static_dependencies.py`

```python
"""
Step 4: Extract Static Dependencies

This script extracts static dependencies from test files.
It identifies which production code classes/modules each test references.

What it does:
1. Loads test registry from Step 3
2. Parses each test file using AST
3. Extracts import statements (from X import Y)
4. Extracts referenced classes in test code
5. Filters out test framework imports (pytest, unittest)
6. Builds test → production_code mapping
7. Displays dependency statistics
8. Saves dependencies to JSON file

Run this script:
    python test_analysis/04_extract_static_dependencies.py
"""

from pathlib import Path
import sys
import json

# Add utils to path
sys.path.insert(0, str(Path(__file__).parent))

from utils.ast_parser import parse_file, extract_imports, extract_string_references
from utils.output_formatter import (
    print_header, print_section, print_item, print_list,
    save_json, print_progress, print_summary
)

# Configuration
OUTPUT_DIR = Path(__file__).parent / "outputs"
STEP3_OUTPUT = OUTPUT_DIR / "03_test_registry.json"
OUTPUT_FILE = OUTPUT_DIR / "04_static_dependencies.json"

# Test framework imports to exclude (these are not production code)
TEST_FRAMEWORK_IMPORTS = {
    'pytest', 'unittest', 'mock', 'unittest.mock',
    'pytest_mock', 'pytest_asyncio', 'pytest_cov',
    'test', 'tests', 'testing'
}


def is_production_import(import_name: str) -> bool:
    """
    Check if an import is likely production code (not test framework).
    
    Args:
        import_name: The import module name
    
    Returns:
        True if it's likely production code, False if it's test framework
    """
    # Check if it starts with any test framework name
    for test_framework in TEST_FRAMEWORK_IMPORTS:
        if import_name.startswith(test_framework):
            return False
    
    # Check if it's a standard library import (common ones)
    stdlib_modules = {
        'os', 'sys', 'pathlib', 'json', 'datetime', 'typing',
        'collections', 'itertools', 'functools', 'asyncio',
        'abc', 'dataclasses', 'enum', 'logging', 're'
    }
    
    # Split by dot to check first part
    first_part = import_name.split('.')[0]
    if first_part in stdlib_modules:
        return False  # Standard library, not production code
    
    return True


def extract_dependencies_from_file(filepath: Path) -> dict:
    """
    Extract dependencies from a single test file.
    
    Args:
        filepath: Path to the test file
    
    Returns:
        Dictionary with dependency information
    """
    tree = parse_file(filepath)
    if not tree:
        return {
            "file_path": str(filepath),
            "imports": [],
            "production_imports": [],
            "from_imports": []
        }
    
    # Extract all imports
    imports_data = extract_imports(tree)
    
    # Extract string-based references (patch() calls, etc.)
    string_refs = extract_string_references(tree)
    
    # Filter for production code imports
    all_imports = imports_data['all_imports']
    production_imports = [
        imp for imp in all_imports
        if is_production_import(imp)
    ]
    
    # Filter string references for production code
    production_string_refs = [
        ref for ref in string_refs
        if is_production_import(ref)
    ]
    
    # Filter from_imports for production code
    production_from_imports = []
    for module, names in imports_data['from_imports']:
        if module and is_production_import(module):
            production_from_imports.append((module, names))
    
    # Combine all production references (imports + string refs)
    all_production_refs = set(production_imports)
    all_production_refs.update(production_string_refs)
    # Also add module names from from_imports
    for module, _ in production_from_imports:
        all_production_refs.add(module)
    
    return {
        "file_path": str(filepath),
        "imports": all_imports,
        "production_imports": production_imports,
        "string_references": string_refs,  # NEW: All string refs
        "production_string_references": production_string_refs,  # NEW: Filtered string refs
        "from_imports": imports_data['from_imports'],
        "production_from_imports": production_from_imports,
        "all_production_references": sorted(list(all_production_refs))  # NEW: Combined refs
    }


def build_dependency_mapping() -> dict:
    """
    Build dependency mapping for all tests.
    
    Returns:
        Dictionary with test dependencies
    """
    # Load test registry from Step 3
    if not STEP3_OUTPUT.exists():
        print("Error: Step 3 output not found. Please run Step 3 first.")
        return {}
    
    with open(STEP3_OUTPUT, 'r', encoding='utf-8') as f:
        registry_data = json.load(f)['data']
    
    tests = registry_data['tests']
    test_files = set(t['file_path'] for t in tests)
    
    print_section(f"Processing {len(test_files)} test files...")
    
    # Extract dependencies for each file
    file_dependencies = {}
    for i, filepath_str in enumerate(test_files):
        print_progress(i + 1, len(test_files), "files")
        filepath = Path(filepath_str)
        file_dependencies[filepath_str] = extract_dependencies_from_file(filepath)
    
    print()  # New line after progress
    
    # Map dependencies to individual tests
    test_dependencies = []
    for test in tests:
        file_path = test['file_path']
        file_deps = file_dependencies.get(file_path, {})
        
        # Get production imports for this test
        production_imports = file_deps.get('production_imports', [])
        production_from_imports = file_deps.get('production_from_imports', [])
        production_string_refs = file_deps.get('production_string_references', [])
        
        # Extract all referenced classes/modules (imports + string refs)
        referenced_classes = set(production_imports)
        referenced_classes.update(production_string_refs)  # Add string-based references
        
        for module, names in production_from_imports:
            referenced_classes.add(module)
            # Also add individual names if they're classes
            referenced_classes.update(names)
        
        # Track reference types for each class
        reference_types = {}
        for ref in production_imports:
            reference_types[ref] = 'direct_import'
        for ref in production_string_refs:
            reference_types[ref] = 'string_ref'
        for module, _ in production_from_imports:
            if module not in reference_types:
                reference_types[module] = 'direct_import'
        
        test_dependencies.append({
            "test_id": test['test_id'],
            "file_path": file_path,
            "class_name": test.get('class_name'),
            "method_name": test['method_name'],
            "referenced_classes": sorted(list(referenced_classes)),
            "reference_types": reference_types,  # NEW: Track how each class is referenced
            "import_count": len(referenced_classes)
        })
    
    # Statistics
    total_references = sum(len(td['referenced_classes']) for td in test_dependencies)
    tests_with_deps = sum(1 for td in test_dependencies if td['import_count'] > 0)
    
    # Most referenced modules
    module_counts = {}
    for td in test_dependencies:
        for module in td['referenced_classes']:
            # Get top-level module
            top_module = module.split('.')[0]
            module_counts[top_module] = module_counts.get(top_module, 0) + 1
    
    top_modules = sorted(module_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    
    return {
        "total_tests": len(test_dependencies),
        "tests_with_dependencies": tests_with_deps,
        "total_references": total_references,
        "average_references_per_test": round(total_references / len(test_dependencies), 2) if test_dependencies else 0,
        "top_modules": dict(top_modules),
        "test_dependencies": test_dependencies,
        "file_dependencies": file_dependencies
    }


def main():
    """Main function to extract static dependencies."""
    print_header("Step 4: Extracting Static Dependencies")
    print()
    
    # Step 1: Build dependency mapping
    print_section("Analyzing test files for dependencies...")
    dependency_data = build_dependency_mapping()
    
    if not dependency_data:
        print("Error: Failed to build dependency mapping!")
        return
    
    print()
    
    # Step 2: Display summary
    print_section("Dependency Summary:")
    print_summary({
        "total_tests": dependency_data['total_tests'],
        "tests_with_dependencies": dependency_data['tests_with_dependencies'],
        "total_references": dependency_data['total_references'],
        "average_references": dependency_data['average_references_per_test']
    })
    print()
    
    # Step 3: Display top modules
    print_section("Most Referenced Modules (Top 10):")
    for module, count in list(dependency_data['top_modules'].items())[:10]:
        print_item(f"{module}:", count)
    print()
    
    # Step 4: Display sample dependencies
    print_section("Sample Test Dependencies (first 5):")
    sample_tests = [td for td in dependency_data['test_dependencies'] if td['import_count'] > 0][:5]
    for test_dep in sample_tests:
        test_desc = test_dep['method_name']
        if test_dep['class_name']:
            test_desc = f"{test_dep['class_name']}.{test_desc}"
        print_item(f"{test_dep['test_id']} ({test_desc}):", 
                  f"{test_dep['import_count']} references")
        if test_dep['referenced_classes']:
            refs_preview = ', '.join(test_dep['referenced_classes'][:3])
            if len(test_dep['referenced_classes']) > 3:
                refs_preview += f", ... (+{len(test_dep['referenced_classes']) - 3} more)"
            print_item("  References:", refs_preview)
    print()
    
    # Step 5: Save to JSON
    print_section("Saving results...")
    save_json(dependency_data, OUTPUT_FILE)
    print()
    
    print_header("Step 4 Complete!")
    print(f"Extracted dependencies for {dependency_data['total_tests']} tests")
    print(f"Results saved to: {OUTPUT_FILE}")


if __name__ == "__main__":
    main()

```

`test_analysis/05_extract_test_metadata.py`

```python
"""
Step 5: Extract Test Metadata

This script extracts metadata from tests including names, descriptions, tags, and characteristics.

What it does:
1. Loads test registry from Step 3
2. Parses each test file using AST
3. Extracts test method names and patterns
4. Extracts docstrings (test descriptions)
5. Extracts pytest markers (@pytest.mark.slow, @pytest.mark.asyncio)
6. Extracts test parameters (parameterized tests)
7. Identifies test patterns and characteristics
8. Displays metadata summary
9. Saves metadata to JSON file

Run this script:
    python test_analysis/05_extract_test_metadata.py
"""

from pathlib import Path
import sys
import json
import re

# Add utils to path
sys.path.insert(0, str(Path(__file__).parent))

from utils.ast_parser import parse_file, extract_functions, extract_docstrings
from utils.output_formatter import (
    print_header, print_section, print_item, print_list,
    save_json, print_progress, print_summary
)

# Configuration
OUTPUT_DIR = Path(__file__).parent / "outputs"
STEP3_OUTPUT = OUTPUT_DIR / "03_test_registry.json"
OUTPUT_FILE = OUTPUT_DIR / "05_test_metadata.json"


def extract_test_metadata_from_file(filepath: Path, test_methods: list) -> dict:
    """
    Extract metadata for test methods in a file.
    
    Args:
        filepath: Path to the test file
        test_methods: List of test method names to extract metadata for
    
    Returns:
        Dictionary mapping method names to their metadata
    """
    tree = parse_file(filepath)
    if not tree:
        return {}
    
    # Extract all functions
    all_functions = extract_functions(tree)
    
    # Extract docstrings
    docstrings = extract_docstrings(tree)
    
    # Build metadata for each test method
    metadata = {}
    
    for func in all_functions:
        func_name = func['name']
        if func_name.startswith('test_'):
            # Extract markers from decorators
            markers = []
            for decorator in func['decorators']:
                if 'pytest.mark' in decorator.lower():
                    # Extract marker name (e.g., @pytest.mark.asyncio -> asyncio)
                    marker_match = re.search(r'mark\.(\w+)', decorator, re.IGNORECASE)
                    if marker_match:
                        markers.append(marker_match.group(1).lower())
            
            # Get docstring
            description = docstrings['functions'].get(func_name, '')
            
            # Identify test pattern from name
            pattern = _identify_test_pattern(func_name)
            
            # Check if async
            is_async = func['is_async']
            
            # Check if parameterized (has parameters beyond self)
            is_parameterized = len(func['parameters']) > 1 or 'parametrize' in str(func['decorators']).lower()
            
            metadata[func_name] = {
                "name": func_name,
                "description": description,
                "markers": markers,
                "is_async": is_async,
                "is_parameterized": is_parameterized,
                "parameters": func['parameters'],
                "decorators": func['decorators'],
                "pattern": pattern,
                "line_number": func['line_number']
            }
    
    return metadata


def _identify_test_pattern(test_name: str) -> str:
    """
    Identify test pattern from test name.
    
    Args:
        test_name: Name of the test method
    
    Returns:
        Pattern identifier (e.g., 'should', 'when', 'test_', etc.)
    """
    test_name_lower = test_name.lower()
    
    if 'should' in test_name_lower:
        return 'should_pattern'
    elif 'when' in test_name_lower or 'given' in test_name_lower:
        return 'bdd_pattern'
    elif test_name.startswith('test_'):
        return 'test_prefix'
    else:
        return 'other'


def build_test_metadata() -> dict:
    """
    Build metadata for all tests.
    
    Returns:
        Dictionary with test metadata
    """
    # Load test registry from Step 3
    if not STEP3_OUTPUT.exists():
        print("Error: Step 3 output not found. Please run Step 3 first.")
        return {}
    
    with open(STEP3_OUTPUT, 'r', encoding='utf-8') as f:
        registry_data = json.load(f)['data']
    
    tests = registry_data['tests']
    test_files = {}
    
    # Group tests by file
    for test in tests:
        file_path = test['file_path']
        if file_path not in test_files:
            test_files[file_path] = []
        test_files[file_path].append(test)
    
    print_section(f"Processing {len(test_files)} test files...")
    
    # Extract metadata for each file
    all_metadata = []
    file_metadata_map = {}
    
    for i, (filepath_str, file_tests) in enumerate(test_files.items()):
        print_progress(i + 1, len(test_files), "files")
        
        filepath = Path(filepath_str)
        test_method_names = [t['method_name'] for t in file_tests]
        
        # Extract metadata from file
        file_metadata = extract_test_metadata_from_file(filepath, test_method_names)
        file_metadata_map[filepath_str] = file_metadata
        
        # Map metadata to tests
        for test in file_tests:
            method_name = test['method_name']
            method_metadata = file_metadata.get(method_name, {})
            
            test_metadata = {
                "test_id": test['test_id'],
                "file_path": test['file_path'],
                "class_name": test.get('class_name'),
                "method_name": method_name,
                "name": method_metadata.get('name', method_name),
                "description": method_metadata.get('description', ''),
                "markers": method_metadata.get('markers', []),
                "is_async": method_metadata.get('is_async', False),
                "is_parameterized": method_metadata.get('is_parameterized', False),
                "pattern": method_metadata.get('pattern', 'unknown'),
                "line_number": method_metadata.get('line_number')
            }
            
            all_metadata.append(test_metadata)
    
    print()  # New line after progress
    
    # Statistics
    tests_with_descriptions = sum(1 for m in all_metadata if m['description'])
    tests_with_markers = sum(1 for m in all_metadata if m['markers'])
    async_tests = sum(1 for m in all_metadata if m['is_async'])
    parameterized_tests = sum(1 for m in all_metadata if m['is_parameterized'])
    
    # Marker counts
    marker_counts = {}
    for m in all_metadata:
        for marker in m['markers']:
            marker_counts[marker] = marker_counts.get(marker, 0) + 1
    
    # Pattern counts
    pattern_counts = {}
    for m in all_metadata:
        pattern = m['pattern']
        pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1
    
    return {
        "total_tests": len(all_metadata),
        "tests_with_descriptions": tests_with_descriptions,
        "tests_with_markers": tests_with_markers,
        "async_tests": async_tests,
        "parameterized_tests": parameterized_tests,
        "marker_counts": marker_counts,
        "pattern_counts": pattern_counts,
        "test_metadata": all_metadata
    }


def main():
    """Main function to extract test metadata."""
    print_header("Step 5: Extracting Test Metadata")
    print()
    
    # Step 1: Build metadata
    print_section("Extracting metadata from test files...")
    metadata_data = build_test_metadata()
    
    if not metadata_data:
        print("Error: Failed to extract metadata!")
        return
    
    print()
    
    # Step 2: Display summary
    print_section("Metadata Summary:")
    print_summary({
        "total_tests": metadata_data['total_tests'],
        "tests_with_descriptions": metadata_data['tests_with_descriptions'],
        "tests_with_markers": metadata_data['tests_with_markers'],
        "async_tests": metadata_data['async_tests'],
        "parameterized_tests": metadata_data['parameterized_tests']
    })
    print()
    
    # Step 3: Display marker counts
    if metadata_data['marker_counts']:
        print_section("Pytest Markers Found:")
        for marker, count in sorted(metadata_data['marker_counts'].items(), key=lambda x: x[1], reverse=True):
            print_item(f"@{marker}:", count)
        print()
    
    # Step 4: Display pattern counts
    print_section("Test Naming Patterns:")
    for pattern, count in sorted(metadata_data['pattern_counts'].items(), key=lambda x: x[1], reverse=True):
        print_item(f"{pattern}:", count)
    print()
    
    # Step 5: Display sample metadata
    print_section("Sample Test Metadata (first 5):")
    sample_tests = metadata_data['test_metadata'][:5]
    for test_meta in sample_tests:
        test_desc = test_meta['method_name']
        if test_meta['class_name']:
            test_desc = f"{test_meta['class_name']}.{test_desc}"
        print_item(f"{test_meta['test_id']} ({test_desc}):", "")
        if test_meta['description']:
            desc_preview = test_meta['description'][:60] + "..." if len(test_meta['description']) > 60 else test_meta['description']
            print_item("  Description:", desc_preview)
        if test_meta['markers']:
            print_item("  Markers:", ", ".join(test_meta['markers']))
        if test_meta['is_async']:
            print_item("  Type:", "Async")
    print()
    
    # Step 6: Save to JSON
    print_section("Saving results...")
    save_json(metadata_data, OUTPUT_FILE)
    print()
    
    print_header("Step 5 Complete!")
    print(f"Extracted metadata for {metadata_data['total_tests']} tests")
    print(f"Results saved to: {OUTPUT_FILE}")


if __name__ == "__main__":
    main()

```

`test_analysis/06_build_reverse_index.py`

```python
"""
Step 6: Build Reverse Index

This script creates a reverse index mapping production code to tests.
This allows fast lookup of which tests reference a specific production class/module.

What it does:
1. Loads static dependencies from Step 4
2. Inverts the mapping: code → [list of tests]
3. Groups by production module/class
4. Creates index for fast lookup
5. Displays reverse index statistics
6. Saves reverse index to JSON file

Run this script:
    python test_analysis/06_build_reverse_index.py
"""

from pathlib import Path
import sys
import json
from collections import defaultdict

# Add utils to path
sys.path.insert(0, str(Path(__file__).parent))

from utils.output_formatter import (
    print_header, print_section, print_item, print_list,
    save_json, print_summary
)

# Configuration
OUTPUT_DIR = Path(__file__).parent / "outputs"
STEP4_OUTPUT = OUTPUT_DIR / "04_static_dependencies.json"
OUTPUT_FILE = OUTPUT_DIR / "06_reverse_index.json"


def build_reverse_index() -> dict:
    """
    Build reverse index from test dependencies.
    
    Returns:
        Dictionary with reverse index data
    """
    # Load dependencies from Step 4
    if not STEP4_OUTPUT.exists():
        print("Error: Step 4 output not found. Please run Step 4 first.")
        return {}
    
    with open(STEP4_OUTPUT, 'r', encoding='utf-8') as f:
        dependency_data = json.load(f)['data']
    
    test_dependencies = dependency_data['test_dependencies']
    
    print_section("Building reverse index...")
    
    # Build reverse mapping: production_code → [test_ids]
    reverse_index = defaultdict(list)
    
    for test_dep in test_dependencies:
        test_id = test_dep['test_id']
        referenced_classes = test_dep['referenced_classes']
        reference_types = test_dep.get('reference_types', {})  # Get reference types
        
        for ref_class in referenced_classes:
            # Get reference type for this class (default to 'direct_import')
            ref_type = reference_types.get(ref_class, 'direct_import')
            
            reverse_index[ref_class].append({
                "test_id": test_id,
                "file_path": test_dep['file_path'],
                "class_name": test_dep.get('class_name'),
                "method_name": test_dep['method_name'],
                "reference_type": ref_type  # NEW: Include reference type
            })
    
    # Convert defaultdict to regular dict and sort
    reverse_index_dict = {
        code: sorted(tests, key=lambda x: x['test_id'])
        for code, tests in reverse_index.items()
    }
    
    # Statistics
    total_mappings = sum(len(tests) for tests in reverse_index_dict.values())
    most_referenced = sorted(
        reverse_index_dict.items(),
        key=lambda x: len(x[1]),
        reverse=True
    )[:20]
    
    # Group by top-level module
    module_groups = defaultdict(list)
    for code, tests in reverse_index_dict.items():
        top_module = code.split('.')[0]
        module_groups[top_module].extend([code])
    
    return {
        "total_production_classes": len(reverse_index_dict),
        "total_mappings": total_mappings,
        "average_tests_per_class": round(total_mappings / len(reverse_index_dict), 2) if reverse_index_dict else 0,
        "most_referenced_classes": [
            {
                "class": code,
                "test_count": len(tests),
                "tests": tests[:5]  # First 5 tests
            }
            for code, tests in most_referenced
        ],
        "module_groups": {
            module: len(classes)
            for module, classes in module_groups.items()
        },
        "reverse_index": reverse_index_dict
    }


def main():
    """Main function to build reverse index."""
    print_header("Step 6: Building Reverse Index")
    print()
    
    # Step 1: Build reverse index
    reverse_index_data = build_reverse_index()
    
    if not reverse_index_data:
        print("Error: Failed to build reverse index!")
        return
    
    print()
    
    # Step 2: Display summary
    print_section("Reverse Index Summary:")
    print_summary({
        "total_production_classes": reverse_index_data['total_production_classes'],
        "total_mappings": reverse_index_data['total_mappings'],
        "average_tests_per_class": reverse_index_data['average_tests_per_class']
    })
    print()
    
    # Step 3: Display most referenced classes
    print_section("Most Referenced Production Classes (Top 10):")
    for ref_class in reverse_index_data['most_referenced_classes'][:10]:
        print_item(f"{ref_class['class']}:", f"{ref_class['test_count']} tests")
        # Show sample tests
        if ref_class['tests']:
            test_names = [t['method_name'] for t in ref_class['tests']]
            test_preview = ', '.join(test_names[:3])
            if len(ref_class['tests']) > 3:
                test_preview += f", ... (+{len(ref_class['tests']) - 3} more)"
            print_item("  Sample tests:", test_preview)
    print()
    
    # Step 4: Display module groups
    print_section("Production Modules (Top 10):")
    sorted_modules = sorted(
        reverse_index_data['module_groups'].items(),
        key=lambda x: x[1],
        reverse=True
    )[:10]
    for module, class_count in sorted_modules:
        print_item(f"{module}:", f"{class_count} classes")
    print()
    
    # Step 5: Save to JSON
    print_section("Saving results...")
    save_json(reverse_index_data, OUTPUT_FILE)
    print()
    
    print_header("Step 6 Complete!")
    print(f"Created reverse index for {reverse_index_data['total_production_classes']} production classes")
    print(f"Results saved to: {OUTPUT_FILE}")


if __name__ == "__main__":
    main()

```

`test_analysis/07_map_test_structure.py`

```python
"""
Step 7: Map Test Structure

This script maps the test repository structure and organization.
It identifies directory hierarchy, package organization, and test relationships.

What it does:
1. Loads test files from Step 1
2. Analyzes directory structure (unit/, integration/, e2e/)
3. Maps package/module organization
4. Identifies shared utilities (conftest.py, fixtures)
5. Maps test inheritance patterns
6. Displays structure visualization
7. Saves structure map to JSON file

Run this script:
    python test_analysis/07_map_test_structure.py
"""

from pathlib import Path
import sys
import json
from collections import defaultdict

# Add utils to path
sys.path.insert(0, str(Path(__file__).parent))

from utils.file_scanner import scan_directory, get_file_metadata, group_files_by_category
from utils.output_formatter import (
    print_header, print_section, print_item, print_list,
    save_json, print_summary
)

# Configuration
TEST_REPO_PATH = Path(__file__).parent.parent / "test_repository"
OUTPUT_DIR = Path(__file__).parent / "outputs"
STEP1_OUTPUT = OUTPUT_DIR / "01_test_files.json"
OUTPUT_FILE = OUTPUT_DIR / "07_test_structure.json"


def analyze_directory_structure(repo_path: Path) -> dict:
    """
    Analyze the directory structure of the test repository.
    
    Returns:
        Dictionary with directory structure information
    """
    structure = {
        "root_path": str(repo_path),
        "directories": {},
        "files_by_directory": defaultdict(list),
        "package_structure": {}
    }
    
    # Scan all test files
    test_files = scan_directory(repo_path)
    
    # Group by category
    grouped = group_files_by_category(test_files)
    
    # Analyze each file
    for filepath in test_files:
        metadata = get_file_metadata(filepath)
        directory = metadata['directory']
        
        # Get relative path parts
        relative_path = filepath.relative_to(repo_path)
        path_parts = relative_path.parts
        
        # Store file in directory group
        structure["files_by_directory"][directory].append({
            "path": str(relative_path),
            "name": filepath.name,
            "line_count": metadata['line_count']
        })
        
        # Build package structure
        if len(path_parts) > 1:
            package = path_parts[0]  # First directory level
            if package not in structure["package_structure"]:
                structure["package_structure"][package] = {
                    "files": [],
                    "subdirectories": set()
                }
            
            structure["package_structure"][package]["files"].append({
                "name": filepath.name,
                "path": str(relative_path),
                "category": directory
            })
            
            # Track subdirectories
            if len(path_parts) > 2:
                structure["package_structure"][package]["subdirectories"].add(path_parts[1])
    
    # Convert sets to lists for JSON serialization
    for package in structure["package_structure"]:
        structure["package_structure"][package]["subdirectories"] = list(
            structure["package_structure"][package]["subdirectories"]
        )
    
    # Add directory statistics
    structure["directories"] = {
        category: {
            "file_count": len(files),
            "total_lines": sum(f['line_count'] for f in files)
        }
        for category, files in structure["files_by_directory"].items()
    }
    
    return structure


def find_shared_utilities(repo_path: Path) -> dict:
    """
    Find shared test utilities like conftest.py and fixtures.
    
    Returns:
        Dictionary with shared utility information
    """
    utilities = {
        "conftest_files": [],
        "fixture_directories": [],
        "shared_modules": []
    }
    
    # Find conftest.py files
    for conftest in repo_path.rglob("conftest.py"):
        utilities["conftest_files"].append({
            "path": str(conftest.relative_to(repo_path)),
            "directory": str(conftest.parent.relative_to(repo_path))
        })
    
    # Find fixture directories
    fixtures_dir = repo_path / "fixtures"
    if fixtures_dir.exists():
        utilities["fixture_directories"].append({
            "path": str(fixtures_dir.relative_to(repo_path)),
            "files": [f.name for f in fixtures_dir.glob("*.json")] + 
                     [f.name for f in fixtures_dir.glob("*.py") if f.name != "__init__.py"]
        })
    
    return utilities


def build_structure_map() -> dict:
    """
    Build complete structure map.
    
    Returns:
        Dictionary with complete structure information
    """
    print_section("Analyzing directory structure...")
    directory_structure = analyze_directory_structure(TEST_REPO_PATH)
    
    print_section("Finding shared utilities...")
    shared_utilities = find_shared_utilities(TEST_REPO_PATH)
    
    return {
        "directory_structure": directory_structure,
        "shared_utilities": shared_utilities,
        "summary": {
            "total_directories": len(directory_structure["package_structure"]),
            "total_files": sum(
                len(files) for files in directory_structure["files_by_directory"].values()
            ),
            "categories": list(directory_structure["directories"].keys())
        }
    }


def main():
    """Main function to map test structure."""
    print_header("Step 7: Mapping Test Structure")
    print()
    
    # Check if test repository exists
    if not TEST_REPO_PATH.exists():
        print(f"Error: Test repository not found at {TEST_REPO_PATH}")
        return
    
    # Step 1: Build structure map
    structure_data = build_structure_map()
    
    print()
    
    # Step 2: Display summary
    print_section("Structure Summary:")
    print_summary({
        "total_directories": structure_data['summary']['total_directories'],
        "total_files": structure_data['summary']['total_files'],
        "categories": len(structure_data['summary']['categories'])
    })
    print()
    
    # Step 3: Display directory breakdown
    print_section("Directory Breakdown:")
    for category, stats in structure_data['directory_structure']['directories'].items():
        print_item(f"{category.capitalize()}:", 
                  f"{stats['file_count']} files, {stats['total_lines']} lines")
    print()
    
    # Step 4: Display package structure
    print_section("Package Structure:")
    for package, info in structure_data['directory_structure']['package_structure'].items():
        print_item(f"{package}/:", f"{len(info['files'])} files")
        if info['subdirectories']:
            print_item("  Subdirectories:", ", ".join(info['subdirectories']))
    print()
    
    # Step 5: Display shared utilities
    print_section("Shared Utilities:")
    if structure_data['shared_utilities']['conftest_files']:
        print_item("conftest.py files:", len(structure_data['shared_utilities']['conftest_files']))
        for conftest in structure_data['shared_utilities']['conftest_files']:
            print_item(f"  - {conftest['path']}:", f"in {conftest['directory']}")
    
    if structure_data['shared_utilities']['fixture_directories']:
        print_item("Fixture directories:", len(structure_data['shared_utilities']['fixture_directories']))
        for fixture_dir in structure_data['shared_utilities']['fixture_directories']:
            print_item(f"  - {fixture_dir['path']}:", f"{len(fixture_dir['files'])} files")
    print()
    
    # Step 6: Save to JSON
    print_section("Saving results...")
    save_json(structure_data, OUTPUT_FILE)
    print()
    
    print_header("Step 7 Complete!")
    print(f"Mapped structure for {structure_data['summary']['total_files']} files")
    print(f"Results saved to: {OUTPUT_FILE}")


if __name__ == "__main__":
    main()

```

`test_analysis/08_generate_summary.py`

```python
"""
Step 8: Generate Summary Report

This script generates a comprehensive summary report combining all previous analysis steps.
It provides an overview of the entire test repository analysis.

What it does:
1. Loads all previous step outputs
2. Combines data from all steps
3. Generates statistics and insights
4. Creates human-readable summary
5. Validates data consistency
6. Displays comprehensive report
7. Saves summary to JSON file

Run this script:
    python test_analysis/08_generate_summary.py
"""

from pathlib import Path
import sys
import json
from datetime import datetime

# Add utils to path
sys.path.insert(0, str(Path(__file__).parent))

from utils.output_formatter import (
    print_header, print_section, print_item, print_list,
    save_json, print_summary
)

# Configuration
OUTPUT_DIR = Path(__file__).parent / "outputs"
OUTPUT_FILE = OUTPUT_DIR / "08_summary_report.json"

# Step output files
STEP_FILES = {
    "step1": OUTPUT_DIR / "01_test_files.json",
    "step2": OUTPUT_DIR / "02_framework_detection.json",
    "step3": OUTPUT_DIR / "03_test_registry.json",
    "step4": OUTPUT_DIR / "04_static_dependencies.json",
    "step5": OUTPUT_DIR / "05_test_metadata.json",
    "step6": OUTPUT_DIR / "06_reverse_index.json",
    "step7": OUTPUT_DIR / "07_test_structure.json"
}


def load_step_output(step_file: Path) -> dict:
    """
    Load output from a previous step.
    
    Args:
        step_file: Path to the step output JSON file
    
    Returns:
        Dictionary with step data, or empty dict if not found
    """
    if not step_file.exists():
        return {}
    
    try:
        with open(step_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('data', {})
    except Exception as e:
        print(f"Warning: Could not load {step_file}: {e}")
        return {}


def generate_summary_report() -> dict:
    """
    Generate comprehensive summary report.
    
    Returns:
        Dictionary with complete summary
    """
    print_section("Loading all analysis results...")
    
    # Load all step outputs
    step1_data = load_step_output(STEP_FILES["step1"])
    step2_data = load_step_output(STEP_FILES["step2"])
    step3_data = load_step_output(STEP_FILES["step3"])
    step4_data = load_step_output(STEP_FILES["step4"])
    step5_data = load_step_output(STEP_FILES["step5"])
    step6_data = load_step_output(STEP_FILES["step6"])
    step7_data = load_step_output(STEP_FILES["step7"])
    
    # Build summary
    summary = {
        "generated_at": datetime.now().isoformat(),
        "test_repository_overview": {
            "total_test_files": step1_data.get("total_files", 0),
            "total_lines_of_code": step1_data.get("total_lines", 0),
            "test_framework": step2_data.get("primary_framework", "unknown"),
            "framework_confidence": step2_data.get("confidence", "unknown")
        },
        "test_inventory": {
            "total_tests": step3_data.get("total_tests", 0),
            "total_test_classes": step3_data.get("total_classes", 0),
            "tests_by_type": step3_data.get("tests_by_type", {})
        },
        "dependencies": {
            "total_production_classes_referenced": step6_data.get("total_production_classes", 0),
            "total_dependency_mappings": step6_data.get("total_mappings", 0),
            "average_tests_per_class": step6_data.get("average_tests_per_class", 0),
            "tests_with_dependencies": step4_data.get("tests_with_dependencies", 0)
        },
        "metadata": {
            "tests_with_descriptions": step5_data.get("tests_with_descriptions", 0),
            "tests_with_markers": step5_data.get("tests_with_markers", 0),
            "async_tests": step5_data.get("async_tests", 0),
            "parameterized_tests": step5_data.get("parameterized_tests", 0)
        },
        "structure": {
            "test_categories": step7_data.get("summary", {}).get("categories", []),
            "package_count": step7_data.get("summary", {}).get("total_directories", 0)
        },
        "key_insights": []
    }
    
    # Generate insights
    insights = []
    
    # Test coverage insight
    if summary["dependencies"]["total_production_classes_referenced"] > 0:
        coverage_ratio = summary["test_inventory"]["total_tests"] / summary["dependencies"]["total_production_classes_referenced"]
        insights.append({
            "type": "coverage",
            "message": f"Average of {coverage_ratio:.1f} tests per production class",
            "value": coverage_ratio
        })
    
    # Framework insight
    if summary["test_repository_overview"]["test_framework"] != "unknown":
        insights.append({
            "type": "framework",
            "message": f"Using {summary['test_repository_overview']['test_framework']} framework",
            "confidence": summary["test_repository_overview"]["framework_confidence"]
        })
    
    # Test organization insight
    if summary["structure"]["test_categories"]:
        insights.append({
            "type": "organization",
            "message": f"Tests organized into {len(summary['structure']['test_categories'])} categories",
            "categories": summary["structure"]["test_categories"]
        })
    
    # Documentation insight
    if summary["metadata"]["tests_with_descriptions"] > 0:
        doc_ratio = summary["metadata"]["tests_with_descriptions"] / summary["test_inventory"]["total_tests"] * 100
        insights.append({
            "type": "documentation",
            "message": f"{doc_ratio:.1f}% of tests have descriptions",
            "value": doc_ratio
        })
    
    summary["key_insights"] = insights
    
    return summary


def main():
    """Main function to generate summary report."""
    print_header("Step 8: Generating Summary Report")
    print()
    
    # Step 1: Generate summary
    summary_data = generate_summary_report()
    
    print()
    
    # Step 2: Display overview
    print_section("Test Repository Overview:")
    overview = summary_data["test_repository_overview"]
    print_item("Total test files:", overview["total_test_files"])
    print_item("Total lines of code:", overview["total_lines_of_code"])
    print_item("Test framework:", overview["test_framework"])
    print_item("Framework confidence:", overview["framework_confidence"])
    print()
    
    # Step 3: Display test inventory
    print_section("Test Inventory:")
    inventory = summary_data["test_inventory"]
    print_item("Total tests:", inventory["total_tests"])
    print_item("Total test classes:", inventory["total_test_classes"])
    print_item("Tests by type:", "")
    for test_type, count in inventory["tests_by_type"].items():
        print_item(f"  {test_type}:", count)
    print()
    
    # Step 4: Display dependencies
    print_section("Dependencies:")
    deps = summary_data["dependencies"]
    print_item("Production classes referenced:", deps["total_production_classes_referenced"])
    print_item("Total dependency mappings:", deps["total_dependency_mappings"])
    print_item("Average tests per class:", deps["average_tests_per_class"])
    print_item("Tests with dependencies:", deps["tests_with_dependencies"])
    print()
    
    # Step 5: Display metadata
    print_section("Test Metadata:")
    metadata = summary_data["metadata"]
    print_item("Tests with descriptions:", metadata["tests_with_descriptions"])
    print_item("Tests with markers:", metadata["tests_with_markers"])
    print_item("Async tests:", metadata["async_tests"])
    print_item("Parameterized tests:", metadata["parameterized_tests"])
    print()
    
    # Step 6: Display structure
    print_section("Repository Structure:")
    structure = summary_data["structure"]
    print_item("Test categories:", len(structure["test_categories"]))
    print_item("Categories:", ", ".join(structure["test_categories"]))
    print_item("Package count:", structure["package_count"])
    print()
    
    # Step 7: Display insights
    print_section("Key Insights:")
    for insight in summary_data["key_insights"]:
        print_item(f"[{insight['type']}]", insight["message"])
    print()
    
    # Step 8: Save to JSON
    print_section("Saving results...")
    save_json(summary_data, OUTPUT_FILE)
    print()
    
    print_header("Step 8 Complete!")
    print("Comprehensive summary report generated")
    print(f"Results saved to: {OUTPUT_FILE}")
    print()
    print("=" * 50)
    print("ALL ANALYSIS STEPS COMPLETE!")
    print("=" * 50)
    print()
    print("Summary:")
    print(f"  - {overview['total_test_files']} test files analyzed")
    print(f"  - {inventory['total_tests']} tests registered")
    print(f"  - {deps['total_production_classes_referenced']} production classes mapped")
    print(f"  - Framework: {overview['test_framework']} ({overview['framework_confidence']} confidence)")
    print()
    print("All output files saved to:", OUTPUT_DIR)


if __name__ == "__main__":
    main()

```

`test_analysis/outputs/01_test_files.json`

```json
{
  "generated_at": "2026-02-26T19:32:04.671075",
  "data": {
    "scan_directory": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository",
    "total_files": 14,
    "total_lines": 1692,
    "total_size_bytes": 66426,
    "categories": {
      "unit": 12,
      "integration": 2,
      "e2e": 0,
      "other": 0
    },
    "files": [
      {
        "path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "absolute_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "size_bytes": 12971,
        "line_count": 280,
        "directory": "unit"
      },
      {
        "path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "absolute_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "size_bytes": 4595,
        "line_count": 98,
        "directory": "unit"
      },
      {
        "path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "absolute_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "size_bytes": 6420,
        "line_count": 167,
        "directory": "unit"
      },
      {
        "path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "absolute_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "size_bytes": 7294,
        "line_count": 218,
        "directory": "unit"
      },
      {
        "path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py",
        "absolute_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py",
        "size_bytes": 2190,
        "line_count": 59,
        "directory": "unit"
      },
      {
        "path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "absolute_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "size_bytes": 6477,
        "line_count": 180,
        "directory": "unit"
      },
      {
        "path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "absolute_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "size_bytes": 3265,
        "line_count": 86,
        "directory": "unit"
      },
      {
        "path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\conftest.py",
        "absolute_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\conftest.py",
        "size_bytes": 6439,
        "line_count": 214,
        "directory": "unit"
      },
      {
        "path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "absolute_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "size_bytes": 1911,
        "line_count": 60,
        "directory": "unit"
      },
      {
        "path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\__init__.py",
        "absolute_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\__init__.py",
        "size_bytes": 26,
        "line_count": 1,
        "directory": "integration"
      },
      {
        "path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "absolute_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "size_bytes": 6267,
        "line_count": 111,
        "directory": "integration"
      },
      {
        "path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "absolute_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "size_bytes": 4692,
        "line_count": 120,
        "directory": "unit"
      },
      {
        "path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "absolute_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "size_bytes": 2419,
        "line_count": 57,
        "directory": "unit"
      },
      {
        "path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
        "absolute_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
        "size_bytes": 1460,
        "line_count": 41,
        "directory": "unit"
      }
    ]
  }
}
```

`test_analysis/outputs/02_framework_detection.json`

```json
{
  "generated_at": "2026-02-26T19:33:48.416926",
  "data": {
    "primary_framework": "pytest",
    "confidence": "high",
    "pytest_score": 36,
    "unittest_score": 13,
    "config_files": {
      "pytest_ini": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\pytest.ini",
      "setup_cfg": null,
      "pyproject_toml": null,
      "tox_ini": null,
      "config_found": true,
      "pytest_ini_content": "[pytest]\n# Pytest configuration file\n\n# Test discovery patterns\npython_files = test_*.py\npython_classes = Test*\npython_functions = test_*\n\n# Test paths\ntestpaths = .\n\n# Output options\naddopts = \n    -v\n    --strict-markers\n    --tb=short\n    --disable-warnings\n\n# Markers\nmarkers =\n    asyncio: marks tests as async (using pytest-asyncio)\n    integration: marks tests as integration tests\n    unit: marks tests as unit tests\n    slow: marks tests as slow running\n\n# Asyncio configuration\nasyncio_mode"
    },
    "conftest": {
      "conftest_found": true,
      "conftest_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\conftest.py",
      "has_fixtures": true,
      "has_pytest_imports": true
    },
    "file_indicators": {
      "pytest": {
        "files_with_import": 13,
        "files_with_markers": 7,
        "files_with_fixtures": 10,
        "total_indicators": 30
      },
      "unittest": {
        "files_with_import": 13,
        "files_with_testcase": 0,
        "total_indicators": 13
      }
    },
    "indicators": [
      "pytest.ini found",
      "conftest.py found",
      "conftest.py uses pytest"
    ]
  }
}
```

`test_analysis/outputs/03_test_registry.json`

```json
{
  "generated_at": "2026-02-26T19:34:10.178960",
  "data": {
    "total_tests": 74,
    "total_classes": 12,
    "total_files": 14,
    "tests_by_type": {
      "unit": 72,
      "integration": 2
    },
    "tests_by_file": {
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py": 12,
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py": 6,
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py": 11,
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py": 9,
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py": 2,
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py": 9,
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py": 7,
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py": 3,
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py": 2,
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py": 8,
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py": 3,
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py": 2
    },
    "tests": [
      {
        "test_id": "test_0001",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0002",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_failure_no_servers",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0003",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_no_tools",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0004",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0005",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke_with_session_id",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0006",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0007",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke_error",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0008",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_close",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0009",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_context_manager",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0010",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_agent_thinking",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0011",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_executing",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0012",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_completed",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0013",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0014",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization_no_tools",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0015",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0016",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph_no_tools",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0017",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0018",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph_not_built",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0019",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_string",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0020",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_list",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0021",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_list_simple",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0022",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_none",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0023",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_dict_with_text",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0024",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_to_langchain_messages",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0025",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_to_langchain_messages_tool",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0026",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_from_langchain_messages",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0027",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_from_langchain_messages_with_tool_calls",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0028",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_langgraph_state_to_agent",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0029",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_langgraph_state_to_agent_with_tool_calls",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0030",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_string",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0031",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_integer",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0032",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_array",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0033",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_empty",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0034",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_mcp_tool_to_langchain",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0035",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_mcp_tool_to_langchain_error_handling",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0036",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_convert_mcp_tools_to_langchain",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0037",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_convert_mcp_tools_to_langchain_empty",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0038",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_convert_mcp_tools_to_langchain_conversion_error",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0039",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py",
        "class_name": "TestAnalyticsAggregator",
        "method_name": "test_get_overview_stats",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0040",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py",
        "class_name": "TestAnalyticsAggregator",
        "method_name": "test_get_tool_usage_stats",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0041",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_root_endpoint",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0042",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_health_endpoint",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0043",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0044",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint_with_session",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0045",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint_invalid_request",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0046",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_stream_endpoint",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0047",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_tools_endpoint",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0048",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_health_check_endpoint",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0049",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_status_endpoint",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0050",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_default_values",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0051",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_from_env",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0052",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_validate_llm_provider",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0053",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_validate_embedding_provider",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0054",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_get_settings_singleton",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0055",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_clear_settings_cache",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0056",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_optional_fields",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0057",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "class_name": "TestInferenceLogger",
        "method_name": "test_log_request",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0058",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "class_name": "TestInferenceLogger",
        "method_name": "test_get_logs",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0059",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "class_name": "TestInferenceLogger",
        "method_name": "test_get_log_not_found",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0060",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_initialization_workflow",
        "test_type": "integration",
        "line_number": null
      },
      {
        "test_id": "test_0061",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_invocation_workflow",
        "test_type": "integration",
        "line_number": null
      },
      {
        "test_id": "test_0062",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_gemini",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0063",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_gemini_missing_key",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0064",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_ollama",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0065",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_invalid",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0066",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_get_available_providers",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0067",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_gemini",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0068",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_ollama",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0069",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_invalid",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0070",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "class_name": "TestCatalogServer",
        "method_name": "test_list_tables_tool",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0071",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "class_name": "TestCatalogServer",
        "method_name": "test_describe_table_tool",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0072",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "class_name": "TestCatalogServer",
        "method_name": "test_get_table_row_count_tool",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0073",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
        "class_name": "TestMLflowTracking",
        "method_name": "test_get_tracker",
        "test_type": "unit",
        "line_number": null
      },
      {
        "test_id": "test_0074",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
        "class_name": "TestMLflowTracking",
        "method_name": "test_tracker_disabled_when_mlflow_not_available",
        "test_type": "unit",
        "line_number": null
      }
    ]
  }
}
```

`test_analysis/outputs/04b_function_calls.json`

```json
{
  "generated_at": "2026-02-26T19:34:11.536699",
  "data": {
    "total_tests": 74,
    "tests_with_function_calls": 74,
    "total_mappings": 360,
    "average_mappings_per_test": 4.86,
    "mappings_by_source": {
      "method_call": 181,
      "patch_ref": 179
    },
    "mappings_by_call_type": {
      "method": 81,
      "patch_ref": 179,
      "direct": 100
    },
    "top_functions": {
      "agent.langgraph_agent.LangGraphAgentBuilder": 14,
      "agent.langgraph_agent.MCPSDKClient": 14,
      "agent.langgraph_agent.convert_langgraph_state_to_agent": 14,
      "agent.langgraph_agent.convert_mcp_tools_to_langchain": 14,
      "agent.langgraph_agent.create_langgraph_initial_state": 14,
      "agent.langgraph_agent.load_system_prompt": 14,
      "agent.langgraph_nodes.get_available_tools": 14,
      "api.routes.MCPSDKClient": 9,
      "api.routes.get_agent": 9,
      "llm.factory.GeminiClient": 8
    },
    "test_function_mappings": [
      {
        "test_id": "test_0001",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization",
        "module_name": null,
        "function_name": "assert_called_once",
        "object_name": "mock_mcp_client",
        "call_type": "method",
        "source": "method_call",
        "line_number": 43
      },
      {
        "test_id": "test_0001",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization",
        "module_name": null,
        "function_name": "initialize",
        "object_name": "agent",
        "call_type": "method",
        "source": "method_call",
        "line_number": 39
      },
      {
        "test_id": "test_0001",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization",
        "module_name": "agent.langgraph_agent",
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0001",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization",
        "module_name": "agent.langgraph_agent",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0001",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_langgraph_state_to_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0001",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_mcp_tools_to_langchain",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0001",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization",
        "module_name": "agent.langgraph_agent",
        "function_name": "create_langgraph_initial_state",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0001",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization",
        "module_name": "agent.langgraph_agent",
        "function_name": "load_system_prompt",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0001",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization",
        "module_name": "agent.langgraph_nodes",
        "function_name": "get_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0002",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_failure_no_servers",
        "module_name": null,
        "function_name": "RuntimeError",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 51
      },
      {
        "test_id": "test_0002",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_failure_no_servers",
        "module_name": null,
        "function_name": "initialize",
        "object_name": "agent",
        "call_type": "method",
        "source": "method_call",
        "line_number": 55
      },
      {
        "test_id": "test_0002",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_failure_no_servers",
        "module_name": "agent.langgraph_agent",
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0002",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_failure_no_servers",
        "module_name": "agent.langgraph_agent",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0002",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_failure_no_servers",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_langgraph_state_to_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0002",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_failure_no_servers",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_mcp_tools_to_langchain",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0002",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_failure_no_servers",
        "module_name": "agent.langgraph_agent",
        "function_name": "create_langgraph_initial_state",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0002",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_failure_no_servers",
        "module_name": "agent.langgraph_agent",
        "function_name": "load_system_prompt",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0002",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_failure_no_servers",
        "module_name": "agent.langgraph_nodes",
        "function_name": "get_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0003",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_no_tools",
        "module_name": null,
        "function_name": "initialize",
        "object_name": "agent",
        "call_type": "method",
        "source": "method_call",
        "line_number": 69
      },
      {
        "test_id": "test_0003",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_no_tools",
        "module_name": "agent.langgraph_agent",
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0003",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_no_tools",
        "module_name": "agent.langgraph_agent",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0003",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_no_tools",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_langgraph_state_to_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0003",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_no_tools",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_mcp_tools_to_langchain",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0003",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_no_tools",
        "module_name": "agent.langgraph_agent",
        "function_name": "create_langgraph_initial_state",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0003",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_no_tools",
        "module_name": "agent.langgraph_agent",
        "function_name": "load_system_prompt",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0003",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_no_tools",
        "module_name": "agent.langgraph_nodes",
        "function_name": "get_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0004",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke",
        "module_name": null,
        "function_name": "AIMessage",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 80
      },
      {
        "test_id": "test_0004",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke",
        "module_name": null,
        "function_name": "assert_called_once",
        "object_name": "mock_graph",
        "call_type": "method",
        "source": "method_call",
        "line_number": 107
      },
      {
        "test_id": "test_0004",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke",
        "module_name": null,
        "function_name": "invoke",
        "object_name": "agent",
        "call_type": "method",
        "source": "method_call",
        "line_number": 100
      },
      {
        "test_id": "test_0004",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke",
        "module_name": "agent.langgraph_agent",
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0004",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke",
        "module_name": "agent.langgraph_agent",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0004",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_langgraph_state_to_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0004",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_mcp_tools_to_langchain",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0004",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke",
        "module_name": "agent.langgraph_agent",
        "function_name": "create_langgraph_initial_state",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0004",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke",
        "module_name": "agent.langgraph_agent",
        "function_name": "load_system_prompt",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0004",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke",
        "module_name": "agent.langgraph_nodes",
        "function_name": "get_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0005",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke_with_session_id",
        "module_name": null,
        "function_name": "AIMessage",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 114
      },
      {
        "test_id": "test_0005",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke_with_session_id",
        "module_name": null,
        "function_name": "invoke",
        "object_name": "agent",
        "call_type": "method",
        "source": "method_call",
        "line_number": 134
      },
      {
        "test_id": "test_0005",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke_with_session_id",
        "module_name": null,
        "function_name": "get",
        "object_name": "result",
        "call_type": "method",
        "source": "method_call",
        "line_number": 141
      },
      {
        "test_id": "test_0005",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke_with_session_id",
        "module_name": "agent.langgraph_agent",
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0005",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke_with_session_id",
        "module_name": "agent.langgraph_agent",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0005",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke_with_session_id",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_langgraph_state_to_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0005",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke_with_session_id",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_mcp_tools_to_langchain",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0005",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke_with_session_id",
        "module_name": "agent.langgraph_agent",
        "function_name": "create_langgraph_initial_state",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0005",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke_with_session_id",
        "module_name": "agent.langgraph_agent",
        "function_name": "load_system_prompt",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0005",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke_with_session_id",
        "module_name": "agent.langgraph_nodes",
        "function_name": "get_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0006",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke",
        "module_name": null,
        "function_name": "HumanMessage",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 165
      },
      {
        "test_id": "test_0006",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke",
        "module_name": null,
        "function_name": "stream_invoke",
        "object_name": "agent",
        "call_type": "method",
        "source": "method_call",
        "line_number": 171
      },
      {
        "test_id": "test_0006",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke",
        "module_name": null,
        "function_name": "any",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 178
      },
      {
        "test_id": "test_0006",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke",
        "module_name": null,
        "function_name": "append",
        "object_name": "stages",
        "call_type": "method",
        "source": "method_call",
        "line_number": 175
      },
      {
        "test_id": "test_0006",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke",
        "module_name": null,
        "function_name": "len",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 177
      },
      {
        "test_id": "test_0006",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke",
        "module_name": null,
        "function_name": "AIMessage",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 150
      },
      {
        "test_id": "test_0006",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke",
        "module_name": null,
        "function_name": "ToolMessage",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 151
      },
      {
        "test_id": "test_0006",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke",
        "module_name": null,
        "function_name": "AIMessage",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 153
      },
      {
        "test_id": "test_0006",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke",
        "module_name": null,
        "function_name": "get",
        "object_name": "s",
        "call_type": "method",
        "source": "method_call",
        "line_number": 178
      },
      {
        "test_id": "test_0006",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke",
        "module_name": "agent.langgraph_agent",
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0006",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke",
        "module_name": "agent.langgraph_agent",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0006",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_langgraph_state_to_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0006",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_mcp_tools_to_langchain",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0006",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke",
        "module_name": "agent.langgraph_agent",
        "function_name": "create_langgraph_initial_state",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0006",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke",
        "module_name": "agent.langgraph_agent",
        "function_name": "load_system_prompt",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0006",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke",
        "module_name": "agent.langgraph_nodes",
        "function_name": "get_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0007",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke_error",
        "module_name": null,
        "function_name": "Exception",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 184
      },
      {
        "test_id": "test_0007",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke_error",
        "module_name": null,
        "function_name": "HumanMessage",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 191
      },
      {
        "test_id": "test_0007",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke_error",
        "module_name": null,
        "function_name": "stream_invoke",
        "object_name": "agent",
        "call_type": "method",
        "source": "method_call",
        "line_number": 197
      },
      {
        "test_id": "test_0007",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke_error",
        "module_name": null,
        "function_name": "any",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 204
      },
      {
        "test_id": "test_0007",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke_error",
        "module_name": null,
        "function_name": "append",
        "object_name": "stages",
        "call_type": "method",
        "source": "method_call",
        "line_number": 201
      },
      {
        "test_id": "test_0007",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke_error",
        "module_name": null,
        "function_name": "get",
        "object_name": "s",
        "call_type": "method",
        "source": "method_call",
        "line_number": 204
      },
      {
        "test_id": "test_0007",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke_error",
        "module_name": "agent.langgraph_agent",
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0007",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke_error",
        "module_name": "agent.langgraph_agent",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0007",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke_error",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_langgraph_state_to_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0007",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke_error",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_mcp_tools_to_langchain",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0007",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke_error",
        "module_name": "agent.langgraph_agent",
        "function_name": "create_langgraph_initial_state",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0007",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke_error",
        "module_name": "agent.langgraph_agent",
        "function_name": "load_system_prompt",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0007",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke_error",
        "module_name": "agent.langgraph_nodes",
        "function_name": "get_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0008",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_close",
        "module_name": null,
        "function_name": "assert_called_once",
        "object_name": "mock_mcp_client",
        "call_type": "method",
        "source": "method_call",
        "line_number": 214
      },
      {
        "test_id": "test_0008",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_close",
        "module_name": null,
        "function_name": "close",
        "object_name": "agent",
        "call_type": "method",
        "source": "method_call",
        "line_number": 212
      },
      {
        "test_id": "test_0008",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_close",
        "module_name": "agent.langgraph_agent",
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0008",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_close",
        "module_name": "agent.langgraph_agent",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0008",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_close",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_langgraph_state_to_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0008",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_close",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_mcp_tools_to_langchain",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0008",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_close",
        "module_name": "agent.langgraph_agent",
        "function_name": "create_langgraph_initial_state",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0008",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_close",
        "module_name": "agent.langgraph_agent",
        "function_name": "load_system_prompt",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0008",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_close",
        "module_name": "agent.langgraph_nodes",
        "function_name": "get_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0009",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_context_manager",
        "module_name": null,
        "function_name": "assert_called_once",
        "object_name": "mock_mcp_client",
        "call_type": "method",
        "source": "method_call",
        "line_number": 232
      },
      {
        "test_id": "test_0009",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_context_manager",
        "module_name": "agent.langgraph_agent",
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0009",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_context_manager",
        "module_name": "agent.langgraph_agent",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0009",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_context_manager",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_langgraph_state_to_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0009",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_context_manager",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_mcp_tools_to_langchain",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0009",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_context_manager",
        "module_name": "agent.langgraph_agent",
        "function_name": "create_langgraph_initial_state",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0009",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_context_manager",
        "module_name": "agent.langgraph_agent",
        "function_name": "load_system_prompt",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0009",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_context_manager",
        "module_name": "agent.langgraph_nodes",
        "function_name": "get_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0010",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_agent_thinking",
        "module_name": null,
        "function_name": "_extract_stage_info",
        "object_name": "agent",
        "call_type": "method",
        "source": "method_call",
        "line_number": 241
      },
      {
        "test_id": "test_0010",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_agent_thinking",
        "module_name": null,
        "function_name": "AIMessage",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 238
      },
      {
        "test_id": "test_0010",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_agent_thinking",
        "module_name": "agent.langgraph_agent",
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0010",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_agent_thinking",
        "module_name": "agent.langgraph_agent",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0010",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_agent_thinking",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_langgraph_state_to_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0010",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_agent_thinking",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_mcp_tools_to_langchain",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0010",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_agent_thinking",
        "module_name": "agent.langgraph_agent",
        "function_name": "create_langgraph_initial_state",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0010",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_agent_thinking",
        "module_name": "agent.langgraph_agent",
        "function_name": "load_system_prompt",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0010",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_agent_thinking",
        "module_name": "agent.langgraph_nodes",
        "function_name": "get_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0011",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_executing",
        "module_name": null,
        "function_name": "AIMessage",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 254
      },
      {
        "test_id": "test_0011",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_executing",
        "module_name": null,
        "function_name": "_extract_stage_info",
        "object_name": "agent",
        "call_type": "method",
        "source": "method_call",
        "line_number": 259
      },
      {
        "test_id": "test_0011",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_executing",
        "module_name": "agent.langgraph_agent",
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0011",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_executing",
        "module_name": "agent.langgraph_agent",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0011",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_executing",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_langgraph_state_to_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0011",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_executing",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_mcp_tools_to_langchain",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0011",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_executing",
        "module_name": "agent.langgraph_agent",
        "function_name": "create_langgraph_initial_state",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0011",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_executing",
        "module_name": "agent.langgraph_agent",
        "function_name": "load_system_prompt",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0011",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_executing",
        "module_name": "agent.langgraph_nodes",
        "function_name": "get_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0012",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_completed",
        "module_name": null,
        "function_name": "AIMessage",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 270
      },
      {
        "test_id": "test_0012",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_completed",
        "module_name": null,
        "function_name": "ToolMessage",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 271
      },
      {
        "test_id": "test_0012",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_completed",
        "module_name": null,
        "function_name": "_extract_stage_info",
        "object_name": "agent",
        "call_type": "method",
        "source": "method_call",
        "line_number": 277
      },
      {
        "test_id": "test_0012",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_completed",
        "module_name": "agent.langgraph_agent",
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0012",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_completed",
        "module_name": "agent.langgraph_agent",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0012",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_completed",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_langgraph_state_to_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0012",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_completed",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_mcp_tools_to_langchain",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0012",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_completed",
        "module_name": "agent.langgraph_agent",
        "function_name": "create_langgraph_initial_state",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0012",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_completed",
        "module_name": "agent.langgraph_agent",
        "function_name": "load_system_prompt",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0012",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_completed",
        "module_name": "agent.langgraph_nodes",
        "function_name": "get_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0013",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization",
        "module_name": null,
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 29
      },
      {
        "test_id": "test_0013",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization",
        "module_name": "agent.langgraph_builder",
        "function_name": "StateGraph",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0013",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization",
        "module_name": "agent.langgraph_builder",
        "function_name": "ToolNode",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0013",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization",
        "module_name": "agent.langgraph_builder",
        "function_name": "call_model",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0013",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization",
        "module_name": "agent.langgraph_builder",
        "function_name": "get_settings",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0013",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization",
        "module_name": "agent.langgraph_builder",
        "function_name": "set_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0013",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization",
        "module_name": "agent.langgraph_builder",
        "function_name": "should_continue",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0014",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization_no_tools",
        "module_name": null,
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 35
      },
      {
        "test_id": "test_0014",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization_no_tools",
        "module_name": "agent.langgraph_builder",
        "function_name": "StateGraph",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0014",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization_no_tools",
        "module_name": "agent.langgraph_builder",
        "function_name": "ToolNode",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0014",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization_no_tools",
        "module_name": "agent.langgraph_builder",
        "function_name": "call_model",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0014",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization_no_tools",
        "module_name": "agent.langgraph_builder",
        "function_name": "get_settings",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0014",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization_no_tools",
        "module_name": "agent.langgraph_builder",
        "function_name": "set_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0014",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization_no_tools",
        "module_name": "agent.langgraph_builder",
        "function_name": "should_continue",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0015",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph",
        "module_name": null,
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 53
      },
      {
        "test_id": "test_0015",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph",
        "module_name": null,
        "function_name": "build",
        "object_name": "builder",
        "call_type": "method",
        "source": "method_call",
        "line_number": 54
      },
      {
        "test_id": "test_0015",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph",
        "module_name": null,
        "function_name": "assert_called_once_with",
        "object_name": "mock_set_tools",
        "call_type": "method",
        "source": "method_call",
        "line_number": 57
      },
      {
        "test_id": "test_0015",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph",
        "module_name": "agent.langgraph_builder",
        "function_name": "StateGraph",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0015",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph",
        "module_name": "agent.langgraph_builder",
        "function_name": "ToolNode",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0015",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph",
        "module_name": "agent.langgraph_builder",
        "function_name": "call_model",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0015",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph",
        "module_name": "agent.langgraph_builder",
        "function_name": "get_settings",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0015",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph",
        "module_name": "agent.langgraph_builder",
        "function_name": "set_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0015",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph",
        "module_name": "agent.langgraph_builder",
        "function_name": "should_continue",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0016",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph_no_tools",
        "module_name": null,
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 71
      },
      {
        "test_id": "test_0016",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph_no_tools",
        "module_name": null,
        "function_name": "build",
        "object_name": "builder",
        "call_type": "method",
        "source": "method_call",
        "line_number": 72
      },
      {
        "test_id": "test_0016",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph_no_tools",
        "module_name": "agent.langgraph_builder",
        "function_name": "StateGraph",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0016",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph_no_tools",
        "module_name": "agent.langgraph_builder",
        "function_name": "ToolNode",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0016",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph_no_tools",
        "module_name": "agent.langgraph_builder",
        "function_name": "call_model",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0016",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph_no_tools",
        "module_name": "agent.langgraph_builder",
        "function_name": "get_settings",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0016",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph_no_tools",
        "module_name": "agent.langgraph_builder",
        "function_name": "set_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0016",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph_no_tools",
        "module_name": "agent.langgraph_builder",
        "function_name": "should_continue",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0017",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph",
        "module_name": null,
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 88
      },
      {
        "test_id": "test_0017",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph",
        "module_name": null,
        "function_name": "build",
        "object_name": "builder",
        "call_type": "method",
        "source": "method_call",
        "line_number": 89
      },
      {
        "test_id": "test_0017",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph",
        "module_name": null,
        "function_name": "get_graph",
        "object_name": "builder",
        "call_type": "method",
        "source": "method_call",
        "line_number": 91
      },
      {
        "test_id": "test_0017",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph",
        "module_name": "agent.langgraph_builder",
        "function_name": "StateGraph",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0017",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph",
        "module_name": "agent.langgraph_builder",
        "function_name": "ToolNode",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0017",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph",
        "module_name": "agent.langgraph_builder",
        "function_name": "call_model",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0017",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph",
        "module_name": "agent.langgraph_builder",
        "function_name": "get_settings",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0017",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph",
        "module_name": "agent.langgraph_builder",
        "function_name": "set_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0017",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph",
        "module_name": "agent.langgraph_builder",
        "function_name": "should_continue",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0018",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph_not_built",
        "module_name": null,
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 96
      },
      {
        "test_id": "test_0018",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph_not_built",
        "module_name": null,
        "function_name": "get_graph",
        "object_name": "builder",
        "call_type": "method",
        "source": "method_call",
        "line_number": 97
      },
      {
        "test_id": "test_0018",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph_not_built",
        "module_name": "agent.langgraph_builder",
        "function_name": "StateGraph",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0018",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph_not_built",
        "module_name": "agent.langgraph_builder",
        "function_name": "ToolNode",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0018",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph_not_built",
        "module_name": "agent.langgraph_builder",
        "function_name": "call_model",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0018",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph_not_built",
        "module_name": "agent.langgraph_builder",
        "function_name": "get_settings",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0018",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph_not_built",
        "module_name": "agent.langgraph_builder",
        "function_name": "set_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0018",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph_not_built",
        "module_name": "agent.langgraph_builder",
        "function_name": "should_continue",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0019",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_string",
        "module_name": null,
        "function_name": "normalize_message_content",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 26
      },
      {
        "test_id": "test_0019",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_string",
        "module_name": null,
        "function_name": "isinstance",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 28
      },
      {
        "test_id": "test_0020",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_list",
        "module_name": null,
        "function_name": "normalize_message_content",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 36
      },
      {
        "test_id": "test_0021",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_list_simple",
        "module_name": null,
        "function_name": "normalize_message_content",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 43
      },
      {
        "test_id": "test_0022",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_none",
        "module_name": null,
        "function_name": "normalize_message_content",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 49
      },
      {
        "test_id": "test_0023",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_dict_with_text",
        "module_name": null,
        "function_name": "normalize_message_content",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 55
      },
      {
        "test_id": "test_0024",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_to_langchain_messages",
        "module_name": null,
        "function_name": "convert_to_langchain_messages",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 66
      },
      {
        "test_id": "test_0024",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_to_langchain_messages",
        "module_name": null,
        "function_name": "isinstance",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 69
      },
      {
        "test_id": "test_0024",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_to_langchain_messages",
        "module_name": null,
        "function_name": "isinstance",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 70
      },
      {
        "test_id": "test_0024",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_to_langchain_messages",
        "module_name": null,
        "function_name": "isinstance",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 71
      },
      {
        "test_id": "test_0024",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_to_langchain_messages",
        "module_name": null,
        "function_name": "len",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 68
      },
      {
        "test_id": "test_0025",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_to_langchain_messages_tool",
        "module_name": null,
        "function_name": "convert_to_langchain_messages",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 79
      },
      {
        "test_id": "test_0025",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_to_langchain_messages_tool",
        "module_name": null,
        "function_name": "isinstance",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 82
      },
      {
        "test_id": "test_0025",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_to_langchain_messages_tool",
        "module_name": null,
        "function_name": "len",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 81
      },
      {
        "test_id": "test_0026",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_from_langchain_messages",
        "module_name": null,
        "function_name": "convert_from_langchain_messages",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 92
      },
      {
        "test_id": "test_0026",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_from_langchain_messages",
        "module_name": null,
        "function_name": "HumanMessage",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 88
      },
      {
        "test_id": "test_0026",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_from_langchain_messages",
        "module_name": null,
        "function_name": "AIMessage",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 89
      },
      {
        "test_id": "test_0026",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_from_langchain_messages",
        "module_name": null,
        "function_name": "len",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 94
      },
      {
        "test_id": "test_0027",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_from_langchain_messages_with_tool_calls",
        "module_name": null,
        "function_name": "AIMessage",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 105
      },
      {
        "test_id": "test_0027",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_from_langchain_messages_with_tool_calls",
        "module_name": null,
        "function_name": "convert_from_langchain_messages",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 108
      },
      {
        "test_id": "test_0027",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_from_langchain_messages_with_tool_calls",
        "module_name": null,
        "function_name": "len",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 110
      },
      {
        "test_id": "test_0027",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_from_langchain_messages_with_tool_calls",
        "module_name": null,
        "function_name": "len",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 113
      },
      {
        "test_id": "test_0028",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_langgraph_state_to_agent",
        "module_name": null,
        "function_name": "convert_langgraph_state_to_agent",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 133
      },
      {
        "test_id": "test_0028",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_langgraph_state_to_agent",
        "module_name": null,
        "function_name": "len",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 139
      },
      {
        "test_id": "test_0028",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_langgraph_state_to_agent",
        "module_name": null,
        "function_name": "HumanMessage",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 119
      },
      {
        "test_id": "test_0028",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_langgraph_state_to_agent",
        "module_name": null,
        "function_name": "AIMessage",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 120
      },
      {
        "test_id": "test_0029",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_langgraph_state_to_agent_with_tool_calls",
        "module_name": null,
        "function_name": "AIMessage",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 148
      },
      {
        "test_id": "test_0029",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_langgraph_state_to_agent_with_tool_calls",
        "module_name": null,
        "function_name": "ToolMessage",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 149
      },
      {
        "test_id": "test_0029",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_langgraph_state_to_agent_with_tool_calls",
        "module_name": null,
        "function_name": "convert_langgraph_state_to_agent",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 164
      },
      {
        "test_id": "test_0029",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_langgraph_state_to_agent_with_tool_calls",
        "module_name": null,
        "function_name": "len",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 166
      },
      {
        "test_id": "test_0029",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_langgraph_state_to_agent_with_tool_calls",
        "module_name": null,
        "function_name": "len",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 167
      },
      {
        "test_id": "test_0030",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_string",
        "module_name": null,
        "function_name": "json_schema_to_pydantic",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 64
      },
      {
        "test_id": "test_0030",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_string",
        "module_name": null,
        "function_name": "model",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 67
      },
      {
        "test_id": "test_0031",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_integer",
        "module_name": null,
        "function_name": "json_schema_to_pydantic",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 85
      },
      {
        "test_id": "test_0031",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_integer",
        "module_name": null,
        "function_name": "model",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 86
      },
      {
        "test_id": "test_0031",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_integer",
        "module_name": null,
        "function_name": "isinstance",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 88
      },
      {
        "test_id": "test_0032",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_array",
        "module_name": null,
        "function_name": "json_schema_to_pydantic",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 106
      },
      {
        "test_id": "test_0032",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_array",
        "module_name": null,
        "function_name": "model",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 107
      },
      {
        "test_id": "test_0032",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_array",
        "module_name": null,
        "function_name": "isinstance",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 109
      },
      {
        "test_id": "test_0033",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_empty",
        "module_name": null,
        "function_name": "json_schema_to_pydantic",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 118
      },
      {
        "test_id": "test_0033",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_empty",
        "module_name": null,
        "function_name": "model",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 119
      },
      {
        "test_id": "test_0034",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_mcp_tool_to_langchain",
        "module_name": null,
        "function_name": "mcp_tool_to_langchain",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 131
      },
      {
        "test_id": "test_0034",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_mcp_tool_to_langchain",
        "module_name": null,
        "function_name": "ainvoke",
        "object_name": "tool",
        "call_type": "method",
        "source": "method_call",
        "line_number": 142
      },
      {
        "test_id": "test_0035",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_mcp_tool_to_langchain_error_handling",
        "module_name": null,
        "function_name": "mcp_tool_to_langchain",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 155
      },
      {
        "test_id": "test_0035",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_mcp_tool_to_langchain_error_handling",
        "module_name": null,
        "function_name": "ainvoke",
        "object_name": "tool",
        "call_type": "method",
        "source": "method_call",
        "line_number": 161
      },
      {
        "test_id": "test_0036",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_convert_mcp_tools_to_langchain",
        "module_name": null,
        "function_name": "convert_mcp_tools_to_langchain",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 174
      },
      {
        "test_id": "test_0036",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_convert_mcp_tools_to_langchain",
        "module_name": null,
        "function_name": "all",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 180
      },
      {
        "test_id": "test_0036",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_convert_mcp_tools_to_langchain",
        "module_name": null,
        "function_name": "len",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 179
      },
      {
        "test_id": "test_0036",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_convert_mcp_tools_to_langchain",
        "module_name": null,
        "function_name": "startswith",
        "object_name": "tool",
        "call_type": "method",
        "source": "method_call",
        "line_number": 180
      },
      {
        "test_id": "test_0037",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_convert_mcp_tools_to_langchain_empty",
        "module_name": null,
        "function_name": "convert_mcp_tools_to_langchain",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 189
      },
      {
        "test_id": "test_0037",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_convert_mcp_tools_to_langchain_empty",
        "module_name": null,
        "function_name": "len",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 194
      },
      {
        "test_id": "test_0038",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_convert_mcp_tools_to_langchain_conversion_error",
        "module_name": null,
        "function_name": "convert_mcp_tools_to_langchain",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 212
      },
      {
        "test_id": "test_0038",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_convert_mcp_tools_to_langchain_conversion_error",
        "module_name": null,
        "function_name": "isinstance",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 218
      },
      {
        "test_id": "test_0039",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py",
        "class_name": "TestAnalyticsAggregator",
        "method_name": "test_get_overview_stats",
        "module_name": null,
        "function_name": "get_overview_stats",
        "object_name": "aggregator",
        "call_type": "method",
        "source": "method_call",
        "line_number": 38
      },
      {
        "test_id": "test_0039",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py",
        "class_name": "TestAnalyticsAggregator",
        "method_name": "test_get_overview_stats",
        "module_name": "analytics.aggregator",
        "function_name": "get_inference_logger",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0040",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py",
        "class_name": "TestAnalyticsAggregator",
        "method_name": "test_get_tool_usage_stats",
        "module_name": null,
        "function_name": "get_tool_usage_stats",
        "object_name": "aggregator",
        "call_type": "method",
        "source": "method_call",
        "line_number": 56
      },
      {
        "test_id": "test_0040",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py",
        "class_name": "TestAnalyticsAggregator",
        "method_name": "test_get_tool_usage_stats",
        "module_name": "analytics.aggregator",
        "function_name": "get_inference_logger",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0041",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_root_endpoint",
        "module_name": null,
        "function_name": "get",
        "object_name": "client",
        "call_type": "method",
        "source": "method_call",
        "line_number": 26
      },
      {
        "test_id": "test_0041",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_root_endpoint",
        "module_name": null,
        "function_name": "json",
        "object_name": "response",
        "call_type": "method",
        "source": "method_call",
        "line_number": 28
      },
      {
        "test_id": "test_0041",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_root_endpoint",
        "module_name": "api.routes",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0041",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_root_endpoint",
        "module_name": "api.routes",
        "function_name": "get_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0042",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_health_endpoint",
        "module_name": null,
        "function_name": "get",
        "object_name": "client",
        "call_type": "method",
        "source": "method_call",
        "line_number": 35
      },
      {
        "test_id": "test_0042",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_health_endpoint",
        "module_name": null,
        "function_name": "json",
        "object_name": "response",
        "call_type": "method",
        "source": "method_call",
        "line_number": 37
      },
      {
        "test_id": "test_0042",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_health_endpoint",
        "module_name": "api.routes",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0042",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_health_endpoint",
        "module_name": "api.routes",
        "function_name": "get_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0043",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint",
        "module_name": null,
        "function_name": "post",
        "object_name": "client",
        "call_type": "method",
        "source": "method_call",
        "line_number": 60
      },
      {
        "test_id": "test_0043",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint",
        "module_name": null,
        "function_name": "json",
        "object_name": "response",
        "call_type": "method",
        "source": "method_call",
        "line_number": 70
      },
      {
        "test_id": "test_0043",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint",
        "module_name": "api.routes",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0043",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint",
        "module_name": "api.routes",
        "function_name": "get_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0044",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint_with_session",
        "module_name": null,
        "function_name": "post",
        "object_name": "client",
        "call_type": "method",
        "source": "method_call",
        "line_number": 94
      },
      {
        "test_id": "test_0044",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint_with_session",
        "module_name": null,
        "function_name": "json",
        "object_name": "response",
        "call_type": "method",
        "source": "method_call",
        "line_number": 104
      },
      {
        "test_id": "test_0044",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint_with_session",
        "module_name": null,
        "function_name": "get",
        "object_name": "data",
        "call_type": "method",
        "source": "method_call",
        "line_number": 105
      },
      {
        "test_id": "test_0044",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint_with_session",
        "module_name": "api.routes",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0044",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint_with_session",
        "module_name": "api.routes",
        "function_name": "get_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0045",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint_invalid_request",
        "module_name": null,
        "function_name": "post",
        "object_name": "client",
        "call_type": "method",
        "source": "method_call",
        "line_number": 109
      },
      {
        "test_id": "test_0045",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint_invalid_request",
        "module_name": "api.routes",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0045",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint_invalid_request",
        "module_name": "api.routes",
        "function_name": "get_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0046",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_stream_endpoint",
        "module_name": null,
        "function_name": "post",
        "object_name": "client",
        "call_type": "method",
        "source": "method_call",
        "line_number": 129
      },
      {
        "test_id": "test_0046",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_stream_endpoint",
        "module_name": "api.routes",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0046",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_stream_endpoint",
        "module_name": "api.routes",
        "function_name": "get_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0047",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_tools_endpoint",
        "module_name": null,
        "function_name": "get",
        "object_name": "client",
        "call_type": "method",
        "source": "method_call",
        "line_number": 154
      },
      {
        "test_id": "test_0047",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_tools_endpoint",
        "module_name": null,
        "function_name": "json",
        "object_name": "response",
        "call_type": "method",
        "source": "method_call",
        "line_number": 157
      },
      {
        "test_id": "test_0047",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_tools_endpoint",
        "module_name": "api.routes",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0047",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_tools_endpoint",
        "module_name": "api.routes",
        "function_name": "get_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0048",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_health_check_endpoint",
        "module_name": null,
        "function_name": "get",
        "object_name": "client",
        "call_type": "method",
        "source": "method_call",
        "line_number": 164
      },
      {
        "test_id": "test_0048",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_health_check_endpoint",
        "module_name": null,
        "function_name": "json",
        "object_name": "response",
        "call_type": "method",
        "source": "method_call",
        "line_number": 167
      },
      {
        "test_id": "test_0048",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_health_check_endpoint",
        "module_name": "api.routes",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0048",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_health_check_endpoint",
        "module_name": "api.routes",
        "function_name": "get_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0049",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_status_endpoint",
        "module_name": null,
        "function_name": "get",
        "object_name": "client",
        "call_type": "method",
        "source": "method_call",
        "line_number": 175
      },
      {
        "test_id": "test_0049",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_status_endpoint",
        "module_name": null,
        "function_name": "json",
        "object_name": "response",
        "call_type": "method",
        "source": "method_call",
        "line_number": 178
      },
      {
        "test_id": "test_0049",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_status_endpoint",
        "module_name": "api.routes",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0049",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_status_endpoint",
        "module_name": "api.routes",
        "function_name": "get_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0050",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_default_values",
        "module_name": null,
        "function_name": "dict",
        "object_name": "patch",
        "call_type": "method",
        "source": "method_call",
        "line_number": 20
      },
      {
        "test_id": "test_0050",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_default_values",
        "module_name": null,
        "function_name": "Settings",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 21
      },
      {
        "test_id": "test_0051",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_from_env",
        "module_name": null,
        "function_name": "dict",
        "object_name": "patch",
        "call_type": "method",
        "source": "method_call",
        "line_number": 38
      },
      {
        "test_id": "test_0051",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_from_env",
        "module_name": null,
        "function_name": "Settings",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 39
      },
      {
        "test_id": "test_0052",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_validate_llm_provider",
        "module_name": null,
        "function_name": "dict",
        "object_name": "patch",
        "call_type": "method",
        "source": "method_call",
        "line_number": 48
      },
      {
        "test_id": "test_0052",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_validate_llm_provider",
        "module_name": null,
        "function_name": "Settings",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 50
      },
      {
        "test_id": "test_0053",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_validate_embedding_provider",
        "module_name": null,
        "function_name": "dict",
        "object_name": "patch",
        "call_type": "method",
        "source": "method_call",
        "line_number": 54
      },
      {
        "test_id": "test_0053",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_validate_embedding_provider",
        "module_name": null,
        "function_name": "Settings",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 56
      },
      {
        "test_id": "test_0054",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_get_settings_singleton",
        "module_name": null,
        "function_name": "clear_settings_cache",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 60
      },
      {
        "test_id": "test_0054",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_get_settings_singleton",
        "module_name": null,
        "function_name": "get_settings",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 61
      },
      {
        "test_id": "test_0054",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_get_settings_singleton",
        "module_name": null,
        "function_name": "get_settings",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 62
      },
      {
        "test_id": "test_0055",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_clear_settings_cache",
        "module_name": null,
        "function_name": "clear_settings_cache",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 69
      },
      {
        "test_id": "test_0055",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_clear_settings_cache",
        "module_name": null,
        "function_name": "get_settings",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 70
      },
      {
        "test_id": "test_0055",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_clear_settings_cache",
        "module_name": null,
        "function_name": "clear_settings_cache",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 72
      },
      {
        "test_id": "test_0055",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_clear_settings_cache",
        "module_name": null,
        "function_name": "get_settings",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 73
      },
      {
        "test_id": "test_0055",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_clear_settings_cache",
        "module_name": null,
        "function_name": "isinstance",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 77
      },
      {
        "test_id": "test_0055",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_clear_settings_cache",
        "module_name": null,
        "function_name": "isinstance",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 78
      },
      {
        "test_id": "test_0056",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_optional_fields",
        "module_name": null,
        "function_name": "Settings",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 82
      },
      {
        "test_id": "test_0056",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_optional_fields",
        "module_name": null,
        "function_name": "isinstance",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 85
      },
      {
        "test_id": "test_0056",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_optional_fields",
        "module_name": null,
        "function_name": "isinstance",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 86
      },
      {
        "test_id": "test_0056",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_optional_fields",
        "module_name": null,
        "function_name": "type",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 85
      },
      {
        "test_id": "test_0057",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "class_name": "TestInferenceLogger",
        "method_name": "test_log_request",
        "module_name": null,
        "function_name": "log_request",
        "object_name": "logger",
        "call_type": "method",
        "source": "method_call",
        "line_number": 24
      },
      {
        "test_id": "test_0057",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "class_name": "TestInferenceLogger",
        "method_name": "test_log_request",
        "module_name": null,
        "function_name": "get_log",
        "object_name": "logger",
        "call_type": "method",
        "source": "method_call",
        "line_number": 33
      },
      {
        "test_id": "test_0058",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "class_name": "TestInferenceLogger",
        "method_name": "test_get_logs",
        "module_name": null,
        "function_name": "range",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 42
      },
      {
        "test_id": "test_0058",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "class_name": "TestInferenceLogger",
        "method_name": "test_get_logs",
        "module_name": null,
        "function_name": "all",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 54
      },
      {
        "test_id": "test_0058",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "class_name": "TestInferenceLogger",
        "method_name": "test_get_logs",
        "module_name": null,
        "function_name": "get_logs",
        "object_name": "logger",
        "call_type": "method",
        "source": "method_call",
        "line_number": 51
      },
      {
        "test_id": "test_0058",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "class_name": "TestInferenceLogger",
        "method_name": "test_get_logs",
        "module_name": null,
        "function_name": "len",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 53
      },
      {
        "test_id": "test_0058",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "class_name": "TestInferenceLogger",
        "method_name": "test_get_logs",
        "module_name": null,
        "function_name": "log_request",
        "object_name": "logger",
        "call_type": "method",
        "source": "method_call",
        "line_number": 43
      },
      {
        "test_id": "test_0058",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "class_name": "TestInferenceLogger",
        "method_name": "test_get_logs",
        "module_name": null,
        "function_name": "startswith",
        "object_name": null,
        "call_type": "method",
        "source": "method_call",
        "line_number": 54
      },
      {
        "test_id": "test_0059",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "class_name": "TestInferenceLogger",
        "method_name": "test_get_log_not_found",
        "module_name": null,
        "function_name": "get_log",
        "object_name": "logger",
        "call_type": "method",
        "source": "method_call",
        "line_number": 59
      },
      {
        "test_id": "test_0060",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_initialization_workflow",
        "module_name": null,
        "function_name": "LangGraphAgent",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 42
      },
      {
        "test_id": "test_0060",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_initialization_workflow",
        "module_name": null,
        "function_name": "assert_called_once",
        "object_name": "mock_client",
        "call_type": "method",
        "source": "method_call",
        "line_number": 47
      },
      {
        "test_id": "test_0060",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_initialization_workflow",
        "module_name": null,
        "function_name": "assert_called_once",
        "object_name": "mock_client",
        "call_type": "method",
        "source": "method_call",
        "line_number": 48
      },
      {
        "test_id": "test_0060",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_initialization_workflow",
        "module_name": null,
        "function_name": "assert_called_once",
        "object_name": "mock_convert",
        "call_type": "method",
        "source": "method_call",
        "line_number": 49
      },
      {
        "test_id": "test_0060",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_initialization_workflow",
        "module_name": null,
        "function_name": "assert_called_once",
        "object_name": "mock_builder",
        "call_type": "method",
        "source": "method_call",
        "line_number": 50
      },
      {
        "test_id": "test_0060",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_initialization_workflow",
        "module_name": null,
        "function_name": "initialize",
        "object_name": "agent",
        "call_type": "method",
        "source": "method_call",
        "line_number": 43
      },
      {
        "test_id": "test_0060",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_initialization_workflow",
        "module_name": "agent.langgraph_agent",
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0060",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_initialization_workflow",
        "module_name": "agent.langgraph_agent",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0060",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_initialization_workflow",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_langgraph_state_to_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0060",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_initialization_workflow",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_mcp_tools_to_langchain",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0060",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_initialization_workflow",
        "module_name": "agent.langgraph_agent",
        "function_name": "create_langgraph_initial_state",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0060",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_initialization_workflow",
        "module_name": "agent.langgraph_agent",
        "function_name": "load_system_prompt",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0060",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_initialization_workflow",
        "module_name": "agent.langgraph_nodes",
        "function_name": "get_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0061",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_invocation_workflow",
        "module_name": null,
        "function_name": "LangGraphAgent",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 100
      },
      {
        "test_id": "test_0061",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_invocation_workflow",
        "module_name": null,
        "function_name": "assert_called_once",
        "object_name": "mock_graph",
        "call_type": "method",
        "source": "method_call",
        "line_number": 111
      },
      {
        "test_id": "test_0061",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_invocation_workflow",
        "module_name": null,
        "function_name": "initialize",
        "object_name": "agent",
        "call_type": "method",
        "source": "method_call",
        "line_number": 101
      },
      {
        "test_id": "test_0061",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_invocation_workflow",
        "module_name": null,
        "function_name": "invoke",
        "object_name": "agent",
        "call_type": "method",
        "source": "method_call",
        "line_number": 103
      },
      {
        "test_id": "test_0061",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_invocation_workflow",
        "module_name": "agent.langgraph_agent",
        "function_name": "LangGraphAgentBuilder",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0061",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_invocation_workflow",
        "module_name": "agent.langgraph_agent",
        "function_name": "MCPSDKClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0061",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_invocation_workflow",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_langgraph_state_to_agent",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0061",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_invocation_workflow",
        "module_name": "agent.langgraph_agent",
        "function_name": "convert_mcp_tools_to_langchain",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0061",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_invocation_workflow",
        "module_name": "agent.langgraph_agent",
        "function_name": "create_langgraph_initial_state",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0061",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_invocation_workflow",
        "module_name": "agent.langgraph_agent",
        "function_name": "load_system_prompt",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0061",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_invocation_workflow",
        "module_name": "agent.langgraph_nodes",
        "function_name": "get_available_tools",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0062",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_gemini",
        "module_name": "llm.factory",
        "function_name": "create_provider",
        "object_name": "LLMFactory",
        "call_type": "method",
        "source": "method_call",
        "line_number": 44
      },
      {
        "test_id": "test_0062",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_gemini",
        "module_name": null,
        "function_name": "assert_called_once_with",
        "object_name": "mock_client",
        "call_type": "method",
        "source": "method_call",
        "line_number": 47
      },
      {
        "test_id": "test_0062",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_gemini",
        "module_name": "llm.factory",
        "function_name": "GeminiClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0062",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_gemini",
        "module_name": "llm.factory",
        "function_name": "OllamaClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0063",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_gemini_missing_key",
        "module_name": "llm.factory",
        "function_name": "create_provider",
        "object_name": "LLMFactory",
        "call_type": "method",
        "source": "method_call",
        "line_number": 57
      },
      {
        "test_id": "test_0063",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_gemini_missing_key",
        "module_name": "llm.factory",
        "function_name": "GeminiClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0063",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_gemini_missing_key",
        "module_name": "llm.factory",
        "function_name": "OllamaClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0064",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_ollama",
        "module_name": "llm.factory",
        "function_name": "create_provider",
        "object_name": "LLMFactory",
        "call_type": "method",
        "source": "method_call",
        "line_number": 65
      },
      {
        "test_id": "test_0064",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_ollama",
        "module_name": null,
        "function_name": "assert_called_once",
        "object_name": "mock_client",
        "call_type": "method",
        "source": "method_call",
        "line_number": 68
      },
      {
        "test_id": "test_0064",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_ollama",
        "module_name": "llm.factory",
        "function_name": "GeminiClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0064",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_ollama",
        "module_name": "llm.factory",
        "function_name": "OllamaClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0065",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_invalid",
        "module_name": "llm.factory",
        "function_name": "create_provider",
        "object_name": "LLMFactory",
        "call_type": "method",
        "source": "method_call",
        "line_number": 76
      },
      {
        "test_id": "test_0065",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_invalid",
        "module_name": "llm.factory",
        "function_name": "GeminiClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0065",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_invalid",
        "module_name": "llm.factory",
        "function_name": "OllamaClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0066",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_get_available_providers",
        "module_name": "llm.factory",
        "function_name": "get_available_providers",
        "object_name": "LLMFactory",
        "call_type": "method",
        "source": "method_call",
        "line_number": 80
      },
      {
        "test_id": "test_0066",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_get_available_providers",
        "module_name": null,
        "function_name": "isinstance",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 82
      },
      {
        "test_id": "test_0066",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_get_available_providers",
        "module_name": "llm.factory",
        "function_name": "GeminiClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0066",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_get_available_providers",
        "module_name": "llm.factory",
        "function_name": "OllamaClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0067",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_gemini",
        "module_name": "llm.factory",
        "function_name": "create_embedding_provider",
        "object_name": "LLMFactory",
        "call_type": "method",
        "source": "method_call",
        "line_number": 96
      },
      {
        "test_id": "test_0067",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_gemini",
        "module_name": null,
        "function_name": "assert_called_once",
        "object_name": "mock_client",
        "call_type": "method",
        "source": "method_call",
        "line_number": 99
      },
      {
        "test_id": "test_0067",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_gemini",
        "module_name": "llm.factory",
        "function_name": "GeminiClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0067",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_gemini",
        "module_name": "llm.factory",
        "function_name": "OllamaClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0068",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_ollama",
        "module_name": "llm.factory",
        "function_name": "create_embedding_provider",
        "object_name": "LLMFactory",
        "call_type": "method",
        "source": "method_call",
        "line_number": 109
      },
      {
        "test_id": "test_0068",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_ollama",
        "module_name": null,
        "function_name": "assert_called_once",
        "object_name": "mock_client",
        "call_type": "method",
        "source": "method_call",
        "line_number": 112
      },
      {
        "test_id": "test_0068",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_ollama",
        "module_name": "llm.factory",
        "function_name": "GeminiClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0068",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_ollama",
        "module_name": "llm.factory",
        "function_name": "OllamaClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0069",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_invalid",
        "module_name": "llm.factory",
        "function_name": "create_embedding_provider",
        "object_name": "LLMFactory",
        "call_type": "method",
        "source": "method_call",
        "line_number": 120
      },
      {
        "test_id": "test_0069",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_invalid",
        "module_name": "llm.factory",
        "function_name": "GeminiClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0069",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_invalid",
        "module_name": "llm.factory",
        "function_name": "OllamaClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0070",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "class_name": "TestCatalogServer",
        "method_name": "test_list_tables_tool",
        "module_name": null,
        "function_name": "assert_called_once",
        "object_name": "mock_catalog_manager",
        "call_type": "method",
        "source": "method_call",
        "line_number": 35
      },
      {
        "test_id": "test_0070",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "class_name": "TestCatalogServer",
        "method_name": "test_list_tables_tool",
        "module_name": null,
        "function_name": "list_tables",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 32
      },
      {
        "test_id": "test_0070",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "class_name": "TestCatalogServer",
        "method_name": "test_list_tables_tool",
        "module_name": "mcp_servers.catalog_server.tools",
        "function_name": "CatalogManager",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0071",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "class_name": "TestCatalogServer",
        "method_name": "test_describe_table_tool",
        "module_name": null,
        "function_name": "assert_called_once_with",
        "object_name": "mock_catalog_manager",
        "call_type": "method",
        "source": "method_call",
        "line_number": 46
      },
      {
        "test_id": "test_0071",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "class_name": "TestCatalogServer",
        "method_name": "test_describe_table_tool",
        "module_name": null,
        "function_name": "describe_table",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 43
      },
      {
        "test_id": "test_0071",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "class_name": "TestCatalogServer",
        "method_name": "test_describe_table_tool",
        "module_name": "mcp_servers.catalog_server.tools",
        "function_name": "CatalogManager",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0072",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "class_name": "TestCatalogServer",
        "method_name": "test_get_table_row_count_tool",
        "module_name": null,
        "function_name": "assert_called_once_with",
        "object_name": "mock_catalog_manager",
        "call_type": "method",
        "source": "method_call",
        "line_number": 57
      },
      {
        "test_id": "test_0072",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "class_name": "TestCatalogServer",
        "method_name": "test_get_table_row_count_tool",
        "module_name": null,
        "function_name": "get_table_row_count",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 54
      },
      {
        "test_id": "test_0072",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "class_name": "TestCatalogServer",
        "method_name": "test_get_table_row_count_tool",
        "module_name": "mcp_servers.catalog_server.tools",
        "function_name": "CatalogManager",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0073",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
        "class_name": "TestMLflowTracking",
        "method_name": "test_get_tracker",
        "module_name": null,
        "function_name": "get_tracker",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 28
      },
      {
        "test_id": "test_0073",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
        "class_name": "TestMLflowTracking",
        "method_name": "test_get_tracker",
        "module_name": "mlflow.tracking",
        "function_name": "MlflowClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0073",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
        "class_name": "TestMLflowTracking",
        "method_name": "test_get_tracker",
        "module_name": "mlflow.tracking",
        "function_name": "get_tracker",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0073",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
        "class_name": "TestMLflowTracking",
        "method_name": "test_get_tracker",
        "module_name": "mlflow.tracking",
        "function_name": "mlflow",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0074",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
        "class_name": "TestMLflowTracking",
        "method_name": "test_tracker_disabled_when_mlflow_not_available",
        "module_name": null,
        "function_name": "get_tracker",
        "object_name": null,
        "call_type": "direct",
        "source": "method_call",
        "line_number": 38
      },
      {
        "test_id": "test_0074",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
        "class_name": "TestMLflowTracking",
        "method_name": "test_tracker_disabled_when_mlflow_not_available",
        "module_name": "mlflow.tracking",
        "function_name": "MlflowClient",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0074",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
        "class_name": "TestMLflowTracking",
        "method_name": "test_tracker_disabled_when_mlflow_not_available",
        "module_name": "mlflow.tracking",
        "function_name": "get_tracker",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      },
      {
        "test_id": "test_0074",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
        "class_name": "TestMLflowTracking",
        "method_name": "test_tracker_disabled_when_mlflow_not_available",
        "module_name": "mlflow.tracking",
        "function_name": "mlflow",
        "object_name": null,
        "call_type": "patch_ref",
        "source": "patch_ref",
        "line_number": null
      }
    ]
  }
}
```

`test_analysis/outputs/04_static_dependencies.json`

```json
{
  "generated_at": "2026-02-26T19:34:10.778218",
  "data": {
    "total_tests": 74,
    "tests_with_dependencies": 74,
    "total_references": 575,
    "average_references_per_test": 7.77,
    "top_modules": {
      "agent": 188,
      "api": 27,
      "llm": 24,
      "AIMessage": 23,
      "HumanMessage": 23,
      "ToolMessage": 23,
      "langchain_core": 23,
      "Settings": 15,
      "config": 15,
      "LangGraphAgent": 14
    },
    "test_dependencies": [
      {
        "test_id": "test_0001",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "LangGraphAgent",
          "LangGraphAgentState",
          "ToolMessage",
          "agent.langgraph_agent",
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes.get_available_tools",
          "agent.langgraph_state",
          "create_langgraph_initial_state",
          "langchain_core.messages"
        ],
        "reference_types": {
          "agent.langgraph_state": "direct_import",
          "agent.langgraph_agent": "direct_import",
          "langchain_core.messages": "direct_import",
          "agent.langgraph_agent.LangGraphAgentBuilder": "string_ref",
          "agent.langgraph_agent.MCPSDKClient": "string_ref",
          "agent.langgraph_agent.convert_langgraph_state_to_agent": "string_ref",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain": "string_ref",
          "agent.langgraph_agent.create_langgraph_initial_state": "string_ref",
          "agent.langgraph_agent.load_system_prompt": "string_ref",
          "agent.langgraph_nodes.get_available_tools": "string_ref"
        },
        "import_count": 16
      },
      {
        "test_id": "test_0002",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_failure_no_servers",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "LangGraphAgent",
          "LangGraphAgentState",
          "ToolMessage",
          "agent.langgraph_agent",
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes.get_available_tools",
          "agent.langgraph_state",
          "create_langgraph_initial_state",
          "langchain_core.messages"
        ],
        "reference_types": {
          "agent.langgraph_state": "direct_import",
          "agent.langgraph_agent": "direct_import",
          "langchain_core.messages": "direct_import",
          "agent.langgraph_agent.LangGraphAgentBuilder": "string_ref",
          "agent.langgraph_agent.MCPSDKClient": "string_ref",
          "agent.langgraph_agent.convert_langgraph_state_to_agent": "string_ref",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain": "string_ref",
          "agent.langgraph_agent.create_langgraph_initial_state": "string_ref",
          "agent.langgraph_agent.load_system_prompt": "string_ref",
          "agent.langgraph_nodes.get_available_tools": "string_ref"
        },
        "import_count": 16
      },
      {
        "test_id": "test_0003",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_no_tools",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "LangGraphAgent",
          "LangGraphAgentState",
          "ToolMessage",
          "agent.langgraph_agent",
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes.get_available_tools",
          "agent.langgraph_state",
          "create_langgraph_initial_state",
          "langchain_core.messages"
        ],
        "reference_types": {
          "agent.langgraph_state": "direct_import",
          "agent.langgraph_agent": "direct_import",
          "langchain_core.messages": "direct_import",
          "agent.langgraph_agent.LangGraphAgentBuilder": "string_ref",
          "agent.langgraph_agent.MCPSDKClient": "string_ref",
          "agent.langgraph_agent.convert_langgraph_state_to_agent": "string_ref",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain": "string_ref",
          "agent.langgraph_agent.create_langgraph_initial_state": "string_ref",
          "agent.langgraph_agent.load_system_prompt": "string_ref",
          "agent.langgraph_nodes.get_available_tools": "string_ref"
        },
        "import_count": 16
      },
      {
        "test_id": "test_0004",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "LangGraphAgent",
          "LangGraphAgentState",
          "ToolMessage",
          "agent.langgraph_agent",
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes.get_available_tools",
          "agent.langgraph_state",
          "create_langgraph_initial_state",
          "langchain_core.messages"
        ],
        "reference_types": {
          "agent.langgraph_state": "direct_import",
          "agent.langgraph_agent": "direct_import",
          "langchain_core.messages": "direct_import",
          "agent.langgraph_agent.LangGraphAgentBuilder": "string_ref",
          "agent.langgraph_agent.MCPSDKClient": "string_ref",
          "agent.langgraph_agent.convert_langgraph_state_to_agent": "string_ref",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain": "string_ref",
          "agent.langgraph_agent.create_langgraph_initial_state": "string_ref",
          "agent.langgraph_agent.load_system_prompt": "string_ref",
          "agent.langgraph_nodes.get_available_tools": "string_ref"
        },
        "import_count": 16
      },
      {
        "test_id": "test_0005",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke_with_session_id",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "LangGraphAgent",
          "LangGraphAgentState",
          "ToolMessage",
          "agent.langgraph_agent",
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes.get_available_tools",
          "agent.langgraph_state",
          "create_langgraph_initial_state",
          "langchain_core.messages"
        ],
        "reference_types": {
          "agent.langgraph_state": "direct_import",
          "agent.langgraph_agent": "direct_import",
          "langchain_core.messages": "direct_import",
          "agent.langgraph_agent.LangGraphAgentBuilder": "string_ref",
          "agent.langgraph_agent.MCPSDKClient": "string_ref",
          "agent.langgraph_agent.convert_langgraph_state_to_agent": "string_ref",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain": "string_ref",
          "agent.langgraph_agent.create_langgraph_initial_state": "string_ref",
          "agent.langgraph_agent.load_system_prompt": "string_ref",
          "agent.langgraph_nodes.get_available_tools": "string_ref"
        },
        "import_count": 16
      },
      {
        "test_id": "test_0006",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "LangGraphAgent",
          "LangGraphAgentState",
          "ToolMessage",
          "agent.langgraph_agent",
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes.get_available_tools",
          "agent.langgraph_state",
          "create_langgraph_initial_state",
          "langchain_core.messages"
        ],
        "reference_types": {
          "agent.langgraph_state": "direct_import",
          "agent.langgraph_agent": "direct_import",
          "langchain_core.messages": "direct_import",
          "agent.langgraph_agent.LangGraphAgentBuilder": "string_ref",
          "agent.langgraph_agent.MCPSDKClient": "string_ref",
          "agent.langgraph_agent.convert_langgraph_state_to_agent": "string_ref",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain": "string_ref",
          "agent.langgraph_agent.create_langgraph_initial_state": "string_ref",
          "agent.langgraph_agent.load_system_prompt": "string_ref",
          "agent.langgraph_nodes.get_available_tools": "string_ref"
        },
        "import_count": 16
      },
      {
        "test_id": "test_0007",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke_error",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "LangGraphAgent",
          "LangGraphAgentState",
          "ToolMessage",
          "agent.langgraph_agent",
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes.get_available_tools",
          "agent.langgraph_state",
          "create_langgraph_initial_state",
          "langchain_core.messages"
        ],
        "reference_types": {
          "agent.langgraph_state": "direct_import",
          "agent.langgraph_agent": "direct_import",
          "langchain_core.messages": "direct_import",
          "agent.langgraph_agent.LangGraphAgentBuilder": "string_ref",
          "agent.langgraph_agent.MCPSDKClient": "string_ref",
          "agent.langgraph_agent.convert_langgraph_state_to_agent": "string_ref",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain": "string_ref",
          "agent.langgraph_agent.create_langgraph_initial_state": "string_ref",
          "agent.langgraph_agent.load_system_prompt": "string_ref",
          "agent.langgraph_nodes.get_available_tools": "string_ref"
        },
        "import_count": 16
      },
      {
        "test_id": "test_0008",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_close",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "LangGraphAgent",
          "LangGraphAgentState",
          "ToolMessage",
          "agent.langgraph_agent",
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes.get_available_tools",
          "agent.langgraph_state",
          "create_langgraph_initial_state",
          "langchain_core.messages"
        ],
        "reference_types": {
          "agent.langgraph_state": "direct_import",
          "agent.langgraph_agent": "direct_import",
          "langchain_core.messages": "direct_import",
          "agent.langgraph_agent.LangGraphAgentBuilder": "string_ref",
          "agent.langgraph_agent.MCPSDKClient": "string_ref",
          "agent.langgraph_agent.convert_langgraph_state_to_agent": "string_ref",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain": "string_ref",
          "agent.langgraph_agent.create_langgraph_initial_state": "string_ref",
          "agent.langgraph_agent.load_system_prompt": "string_ref",
          "agent.langgraph_nodes.get_available_tools": "string_ref"
        },
        "import_count": 16
      },
      {
        "test_id": "test_0009",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_context_manager",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "LangGraphAgent",
          "LangGraphAgentState",
          "ToolMessage",
          "agent.langgraph_agent",
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes.get_available_tools",
          "agent.langgraph_state",
          "create_langgraph_initial_state",
          "langchain_core.messages"
        ],
        "reference_types": {
          "agent.langgraph_state": "direct_import",
          "agent.langgraph_agent": "direct_import",
          "langchain_core.messages": "direct_import",
          "agent.langgraph_agent.LangGraphAgentBuilder": "string_ref",
          "agent.langgraph_agent.MCPSDKClient": "string_ref",
          "agent.langgraph_agent.convert_langgraph_state_to_agent": "string_ref",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain": "string_ref",
          "agent.langgraph_agent.create_langgraph_initial_state": "string_ref",
          "agent.langgraph_agent.load_system_prompt": "string_ref",
          "agent.langgraph_nodes.get_available_tools": "string_ref"
        },
        "import_count": 16
      },
      {
        "test_id": "test_0010",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_agent_thinking",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "LangGraphAgent",
          "LangGraphAgentState",
          "ToolMessage",
          "agent.langgraph_agent",
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes.get_available_tools",
          "agent.langgraph_state",
          "create_langgraph_initial_state",
          "langchain_core.messages"
        ],
        "reference_types": {
          "agent.langgraph_state": "direct_import",
          "agent.langgraph_agent": "direct_import",
          "langchain_core.messages": "direct_import",
          "agent.langgraph_agent.LangGraphAgentBuilder": "string_ref",
          "agent.langgraph_agent.MCPSDKClient": "string_ref",
          "agent.langgraph_agent.convert_langgraph_state_to_agent": "string_ref",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain": "string_ref",
          "agent.langgraph_agent.create_langgraph_initial_state": "string_ref",
          "agent.langgraph_agent.load_system_prompt": "string_ref",
          "agent.langgraph_nodes.get_available_tools": "string_ref"
        },
        "import_count": 16
      },
      {
        "test_id": "test_0011",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_executing",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "LangGraphAgent",
          "LangGraphAgentState",
          "ToolMessage",
          "agent.langgraph_agent",
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes.get_available_tools",
          "agent.langgraph_state",
          "create_langgraph_initial_state",
          "langchain_core.messages"
        ],
        "reference_types": {
          "agent.langgraph_state": "direct_import",
          "agent.langgraph_agent": "direct_import",
          "langchain_core.messages": "direct_import",
          "agent.langgraph_agent.LangGraphAgentBuilder": "string_ref",
          "agent.langgraph_agent.MCPSDKClient": "string_ref",
          "agent.langgraph_agent.convert_langgraph_state_to_agent": "string_ref",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain": "string_ref",
          "agent.langgraph_agent.create_langgraph_initial_state": "string_ref",
          "agent.langgraph_agent.load_system_prompt": "string_ref",
          "agent.langgraph_nodes.get_available_tools": "string_ref"
        },
        "import_count": 16
      },
      {
        "test_id": "test_0012",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_completed",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "LangGraphAgent",
          "LangGraphAgentState",
          "ToolMessage",
          "agent.langgraph_agent",
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes.get_available_tools",
          "agent.langgraph_state",
          "create_langgraph_initial_state",
          "langchain_core.messages"
        ],
        "reference_types": {
          "agent.langgraph_state": "direct_import",
          "agent.langgraph_agent": "direct_import",
          "langchain_core.messages": "direct_import",
          "agent.langgraph_agent.LangGraphAgentBuilder": "string_ref",
          "agent.langgraph_agent.MCPSDKClient": "string_ref",
          "agent.langgraph_agent.convert_langgraph_state_to_agent": "string_ref",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain": "string_ref",
          "agent.langgraph_agent.create_langgraph_initial_state": "string_ref",
          "agent.langgraph_agent.load_system_prompt": "string_ref",
          "agent.langgraph_nodes.get_available_tools": "string_ref"
        },
        "import_count": 16
      },
      {
        "test_id": "test_0013",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization",
        "referenced_classes": [
          "LangGraphAgentBuilder",
          "agent.langgraph_builder",
          "agent.langgraph_builder.StateGraph",
          "agent.langgraph_builder.ToolNode",
          "agent.langgraph_builder.call_model",
          "agent.langgraph_builder.get_settings",
          "agent.langgraph_builder.set_available_tools",
          "agent.langgraph_builder.should_continue"
        ],
        "reference_types": {
          "agent.langgraph_builder": "direct_import",
          "agent.langgraph_builder.StateGraph": "string_ref",
          "agent.langgraph_builder.ToolNode": "string_ref",
          "agent.langgraph_builder.call_model": "string_ref",
          "agent.langgraph_builder.get_settings": "string_ref",
          "agent.langgraph_builder.set_available_tools": "string_ref",
          "agent.langgraph_builder.should_continue": "string_ref"
        },
        "import_count": 8
      },
      {
        "test_id": "test_0014",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization_no_tools",
        "referenced_classes": [
          "LangGraphAgentBuilder",
          "agent.langgraph_builder",
          "agent.langgraph_builder.StateGraph",
          "agent.langgraph_builder.ToolNode",
          "agent.langgraph_builder.call_model",
          "agent.langgraph_builder.get_settings",
          "agent.langgraph_builder.set_available_tools",
          "agent.langgraph_builder.should_continue"
        ],
        "reference_types": {
          "agent.langgraph_builder": "direct_import",
          "agent.langgraph_builder.StateGraph": "string_ref",
          "agent.langgraph_builder.ToolNode": "string_ref",
          "agent.langgraph_builder.call_model": "string_ref",
          "agent.langgraph_builder.get_settings": "string_ref",
          "agent.langgraph_builder.set_available_tools": "string_ref",
          "agent.langgraph_builder.should_continue": "string_ref"
        },
        "import_count": 8
      },
      {
        "test_id": "test_0015",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph",
        "referenced_classes": [
          "LangGraphAgentBuilder",
          "agent.langgraph_builder",
          "agent.langgraph_builder.StateGraph",
          "agent.langgraph_builder.ToolNode",
          "agent.langgraph_builder.call_model",
          "agent.langgraph_builder.get_settings",
          "agent.langgraph_builder.set_available_tools",
          "agent.langgraph_builder.should_continue"
        ],
        "reference_types": {
          "agent.langgraph_builder": "direct_import",
          "agent.langgraph_builder.StateGraph": "string_ref",
          "agent.langgraph_builder.ToolNode": "string_ref",
          "agent.langgraph_builder.call_model": "string_ref",
          "agent.langgraph_builder.get_settings": "string_ref",
          "agent.langgraph_builder.set_available_tools": "string_ref",
          "agent.langgraph_builder.should_continue": "string_ref"
        },
        "import_count": 8
      },
      {
        "test_id": "test_0016",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph_no_tools",
        "referenced_classes": [
          "LangGraphAgentBuilder",
          "agent.langgraph_builder",
          "agent.langgraph_builder.StateGraph",
          "agent.langgraph_builder.ToolNode",
          "agent.langgraph_builder.call_model",
          "agent.langgraph_builder.get_settings",
          "agent.langgraph_builder.set_available_tools",
          "agent.langgraph_builder.should_continue"
        ],
        "reference_types": {
          "agent.langgraph_builder": "direct_import",
          "agent.langgraph_builder.StateGraph": "string_ref",
          "agent.langgraph_builder.ToolNode": "string_ref",
          "agent.langgraph_builder.call_model": "string_ref",
          "agent.langgraph_builder.get_settings": "string_ref",
          "agent.langgraph_builder.set_available_tools": "string_ref",
          "agent.langgraph_builder.should_continue": "string_ref"
        },
        "import_count": 8
      },
      {
        "test_id": "test_0017",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph",
        "referenced_classes": [
          "LangGraphAgentBuilder",
          "agent.langgraph_builder",
          "agent.langgraph_builder.StateGraph",
          "agent.langgraph_builder.ToolNode",
          "agent.langgraph_builder.call_model",
          "agent.langgraph_builder.get_settings",
          "agent.langgraph_builder.set_available_tools",
          "agent.langgraph_builder.should_continue"
        ],
        "reference_types": {
          "agent.langgraph_builder": "direct_import",
          "agent.langgraph_builder.StateGraph": "string_ref",
          "agent.langgraph_builder.ToolNode": "string_ref",
          "agent.langgraph_builder.call_model": "string_ref",
          "agent.langgraph_builder.get_settings": "string_ref",
          "agent.langgraph_builder.set_available_tools": "string_ref",
          "agent.langgraph_builder.should_continue": "string_ref"
        },
        "import_count": 8
      },
      {
        "test_id": "test_0018",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph_not_built",
        "referenced_classes": [
          "LangGraphAgentBuilder",
          "agent.langgraph_builder",
          "agent.langgraph_builder.StateGraph",
          "agent.langgraph_builder.ToolNode",
          "agent.langgraph_builder.call_model",
          "agent.langgraph_builder.get_settings",
          "agent.langgraph_builder.set_available_tools",
          "agent.langgraph_builder.should_continue"
        ],
        "reference_types": {
          "agent.langgraph_builder": "direct_import",
          "agent.langgraph_builder.StateGraph": "string_ref",
          "agent.langgraph_builder.ToolNode": "string_ref",
          "agent.langgraph_builder.call_model": "string_ref",
          "agent.langgraph_builder.get_settings": "string_ref",
          "agent.langgraph_builder.set_available_tools": "string_ref",
          "agent.langgraph_builder.should_continue": "string_ref"
        },
        "import_count": 8
      },
      {
        "test_id": "test_0019",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_string",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "SystemMessage",
          "ToolMessage",
          "agent.state_converter",
          "convert_from_langchain_messages",
          "convert_langgraph_state_to_agent",
          "convert_to_langchain_messages",
          "langchain_core.messages",
          "normalize_message_content"
        ],
        "reference_types": {
          "agent.state_converter": "direct_import",
          "langchain_core.messages": "direct_import"
        },
        "import_count": 10
      },
      {
        "test_id": "test_0020",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_list",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "SystemMessage",
          "ToolMessage",
          "agent.state_converter",
          "convert_from_langchain_messages",
          "convert_langgraph_state_to_agent",
          "convert_to_langchain_messages",
          "langchain_core.messages",
          "normalize_message_content"
        ],
        "reference_types": {
          "agent.state_converter": "direct_import",
          "langchain_core.messages": "direct_import"
        },
        "import_count": 10
      },
      {
        "test_id": "test_0021",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_list_simple",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "SystemMessage",
          "ToolMessage",
          "agent.state_converter",
          "convert_from_langchain_messages",
          "convert_langgraph_state_to_agent",
          "convert_to_langchain_messages",
          "langchain_core.messages",
          "normalize_message_content"
        ],
        "reference_types": {
          "agent.state_converter": "direct_import",
          "langchain_core.messages": "direct_import"
        },
        "import_count": 10
      },
      {
        "test_id": "test_0022",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_none",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "SystemMessage",
          "ToolMessage",
          "agent.state_converter",
          "convert_from_langchain_messages",
          "convert_langgraph_state_to_agent",
          "convert_to_langchain_messages",
          "langchain_core.messages",
          "normalize_message_content"
        ],
        "reference_types": {
          "agent.state_converter": "direct_import",
          "langchain_core.messages": "direct_import"
        },
        "import_count": 10
      },
      {
        "test_id": "test_0023",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_dict_with_text",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "SystemMessage",
          "ToolMessage",
          "agent.state_converter",
          "convert_from_langchain_messages",
          "convert_langgraph_state_to_agent",
          "convert_to_langchain_messages",
          "langchain_core.messages",
          "normalize_message_content"
        ],
        "reference_types": {
          "agent.state_converter": "direct_import",
          "langchain_core.messages": "direct_import"
        },
        "import_count": 10
      },
      {
        "test_id": "test_0024",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_to_langchain_messages",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "SystemMessage",
          "ToolMessage",
          "agent.state_converter",
          "convert_from_langchain_messages",
          "convert_langgraph_state_to_agent",
          "convert_to_langchain_messages",
          "langchain_core.messages",
          "normalize_message_content"
        ],
        "reference_types": {
          "agent.state_converter": "direct_import",
          "langchain_core.messages": "direct_import"
        },
        "import_count": 10
      },
      {
        "test_id": "test_0025",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_to_langchain_messages_tool",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "SystemMessage",
          "ToolMessage",
          "agent.state_converter",
          "convert_from_langchain_messages",
          "convert_langgraph_state_to_agent",
          "convert_to_langchain_messages",
          "langchain_core.messages",
          "normalize_message_content"
        ],
        "reference_types": {
          "agent.state_converter": "direct_import",
          "langchain_core.messages": "direct_import"
        },
        "import_count": 10
      },
      {
        "test_id": "test_0026",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_from_langchain_messages",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "SystemMessage",
          "ToolMessage",
          "agent.state_converter",
          "convert_from_langchain_messages",
          "convert_langgraph_state_to_agent",
          "convert_to_langchain_messages",
          "langchain_core.messages",
          "normalize_message_content"
        ],
        "reference_types": {
          "agent.state_converter": "direct_import",
          "langchain_core.messages": "direct_import"
        },
        "import_count": 10
      },
      {
        "test_id": "test_0027",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_from_langchain_messages_with_tool_calls",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "SystemMessage",
          "ToolMessage",
          "agent.state_converter",
          "convert_from_langchain_messages",
          "convert_langgraph_state_to_agent",
          "convert_to_langchain_messages",
          "langchain_core.messages",
          "normalize_message_content"
        ],
        "reference_types": {
          "agent.state_converter": "direct_import",
          "langchain_core.messages": "direct_import"
        },
        "import_count": 10
      },
      {
        "test_id": "test_0028",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_langgraph_state_to_agent",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "SystemMessage",
          "ToolMessage",
          "agent.state_converter",
          "convert_from_langchain_messages",
          "convert_langgraph_state_to_agent",
          "convert_to_langchain_messages",
          "langchain_core.messages",
          "normalize_message_content"
        ],
        "reference_types": {
          "agent.state_converter": "direct_import",
          "langchain_core.messages": "direct_import"
        },
        "import_count": 10
      },
      {
        "test_id": "test_0029",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_langgraph_state_to_agent_with_tool_calls",
        "referenced_classes": [
          "AIMessage",
          "HumanMessage",
          "SystemMessage",
          "ToolMessage",
          "agent.state_converter",
          "convert_from_langchain_messages",
          "convert_langgraph_state_to_agent",
          "convert_to_langchain_messages",
          "langchain_core.messages",
          "normalize_message_content"
        ],
        "reference_types": {
          "agent.state_converter": "direct_import",
          "langchain_core.messages": "direct_import"
        },
        "import_count": 10
      },
      {
        "test_id": "test_0030",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_string",
        "referenced_classes": [
          "agent.tool_converter",
          "convert_mcp_tools_to_langchain",
          "json_schema_to_pydantic",
          "mcp_tool_to_langchain"
        ],
        "reference_types": {
          "agent.tool_converter": "direct_import"
        },
        "import_count": 4
      },
      {
        "test_id": "test_0031",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_integer",
        "referenced_classes": [
          "agent.tool_converter",
          "convert_mcp_tools_to_langchain",
          "json_schema_to_pydantic",
          "mcp_tool_to_langchain"
        ],
        "reference_types": {
          "agent.tool_converter": "direct_import"
        },
        "import_count": 4
      },
      {
        "test_id": "test_0032",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_array",
        "referenced_classes": [
          "agent.tool_converter",
          "convert_mcp_tools_to_langchain",
          "json_schema_to_pydantic",
          "mcp_tool_to_langchain"
        ],
        "reference_types": {
          "agent.tool_converter": "direct_import"
        },
        "import_count": 4
      },
      {
        "test_id": "test_0033",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_empty",
        "referenced_classes": [
          "agent.tool_converter",
          "convert_mcp_tools_to_langchain",
          "json_schema_to_pydantic",
          "mcp_tool_to_langchain"
        ],
        "reference_types": {
          "agent.tool_converter": "direct_import"
        },
        "import_count": 4
      },
      {
        "test_id": "test_0034",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_mcp_tool_to_langchain",
        "referenced_classes": [
          "agent.tool_converter",
          "convert_mcp_tools_to_langchain",
          "json_schema_to_pydantic",
          "mcp_tool_to_langchain"
        ],
        "reference_types": {
          "agent.tool_converter": "direct_import"
        },
        "import_count": 4
      },
      {
        "test_id": "test_0035",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_mcp_tool_to_langchain_error_handling",
        "referenced_classes": [
          "agent.tool_converter",
          "convert_mcp_tools_to_langchain",
          "json_schema_to_pydantic",
          "mcp_tool_to_langchain"
        ],
        "reference_types": {
          "agent.tool_converter": "direct_import"
        },
        "import_count": 4
      },
      {
        "test_id": "test_0036",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_convert_mcp_tools_to_langchain",
        "referenced_classes": [
          "agent.tool_converter",
          "convert_mcp_tools_to_langchain",
          "json_schema_to_pydantic",
          "mcp_tool_to_langchain"
        ],
        "reference_types": {
          "agent.tool_converter": "direct_import"
        },
        "import_count": 4
      },
      {
        "test_id": "test_0037",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_convert_mcp_tools_to_langchain_empty",
        "referenced_classes": [
          "agent.tool_converter",
          "convert_mcp_tools_to_langchain",
          "json_schema_to_pydantic",
          "mcp_tool_to_langchain"
        ],
        "reference_types": {
          "agent.tool_converter": "direct_import"
        },
        "import_count": 4
      },
      {
        "test_id": "test_0038",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_convert_mcp_tools_to_langchain_conversion_error",
        "referenced_classes": [
          "agent.tool_converter",
          "convert_mcp_tools_to_langchain",
          "json_schema_to_pydantic",
          "mcp_tool_to_langchain"
        ],
        "reference_types": {
          "agent.tool_converter": "direct_import"
        },
        "import_count": 4
      },
      {
        "test_id": "test_0039",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py",
        "class_name": "TestAnalyticsAggregator",
        "method_name": "test_get_overview_stats",
        "referenced_classes": [
          "AnalyticsAggregator",
          "analytics.aggregator",
          "analytics.aggregator.get_inference_logger"
        ],
        "reference_types": {
          "analytics.aggregator": "direct_import",
          "analytics.aggregator.get_inference_logger": "string_ref"
        },
        "import_count": 3
      },
      {
        "test_id": "test_0040",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py",
        "class_name": "TestAnalyticsAggregator",
        "method_name": "test_get_tool_usage_stats",
        "referenced_classes": [
          "AnalyticsAggregator",
          "analytics.aggregator",
          "analytics.aggregator.get_inference_logger"
        ],
        "reference_types": {
          "analytics.aggregator": "direct_import",
          "analytics.aggregator.get_inference_logger": "string_ref"
        },
        "import_count": 3
      },
      {
        "test_id": "test_0041",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_root_endpoint",
        "referenced_classes": [
          "TestClient",
          "api.main",
          "api.routes.MCPSDKClient",
          "api.routes.get_agent",
          "create_app",
          "fastapi.testclient"
        ],
        "reference_types": {
          "fastapi.testclient": "direct_import",
          "api.main": "direct_import",
          "api.routes.MCPSDKClient": "string_ref",
          "api.routes.get_agent": "string_ref"
        },
        "import_count": 6
      },
      {
        "test_id": "test_0042",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_health_endpoint",
        "referenced_classes": [
          "TestClient",
          "api.main",
          "api.routes.MCPSDKClient",
          "api.routes.get_agent",
          "create_app",
          "fastapi.testclient"
        ],
        "reference_types": {
          "fastapi.testclient": "direct_import",
          "api.main": "direct_import",
          "api.routes.MCPSDKClient": "string_ref",
          "api.routes.get_agent": "string_ref"
        },
        "import_count": 6
      },
      {
        "test_id": "test_0043",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint",
        "referenced_classes": [
          "TestClient",
          "api.main",
          "api.routes.MCPSDKClient",
          "api.routes.get_agent",
          "create_app",
          "fastapi.testclient"
        ],
        "reference_types": {
          "fastapi.testclient": "direct_import",
          "api.main": "direct_import",
          "api.routes.MCPSDKClient": "string_ref",
          "api.routes.get_agent": "string_ref"
        },
        "import_count": 6
      },
      {
        "test_id": "test_0044",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint_with_session",
        "referenced_classes": [
          "TestClient",
          "api.main",
          "api.routes.MCPSDKClient",
          "api.routes.get_agent",
          "create_app",
          "fastapi.testclient"
        ],
        "reference_types": {
          "fastapi.testclient": "direct_import",
          "api.main": "direct_import",
          "api.routes.MCPSDKClient": "string_ref",
          "api.routes.get_agent": "string_ref"
        },
        "import_count": 6
      },
      {
        "test_id": "test_0045",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint_invalid_request",
        "referenced_classes": [
          "TestClient",
          "api.main",
          "api.routes.MCPSDKClient",
          "api.routes.get_agent",
          "create_app",
          "fastapi.testclient"
        ],
        "reference_types": {
          "fastapi.testclient": "direct_import",
          "api.main": "direct_import",
          "api.routes.MCPSDKClient": "string_ref",
          "api.routes.get_agent": "string_ref"
        },
        "import_count": 6
      },
      {
        "test_id": "test_0046",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_stream_endpoint",
        "referenced_classes": [
          "TestClient",
          "api.main",
          "api.routes.MCPSDKClient",
          "api.routes.get_agent",
          "create_app",
          "fastapi.testclient"
        ],
        "reference_types": {
          "fastapi.testclient": "direct_import",
          "api.main": "direct_import",
          "api.routes.MCPSDKClient": "string_ref",
          "api.routes.get_agent": "string_ref"
        },
        "import_count": 6
      },
      {
        "test_id": "test_0047",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_tools_endpoint",
        "referenced_classes": [
          "TestClient",
          "api.main",
          "api.routes.MCPSDKClient",
          "api.routes.get_agent",
          "create_app",
          "fastapi.testclient"
        ],
        "reference_types": {
          "fastapi.testclient": "direct_import",
          "api.main": "direct_import",
          "api.routes.MCPSDKClient": "string_ref",
          "api.routes.get_agent": "string_ref"
        },
        "import_count": 6
      },
      {
        "test_id": "test_0048",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_health_check_endpoint",
        "referenced_classes": [
          "TestClient",
          "api.main",
          "api.routes.MCPSDKClient",
          "api.routes.get_agent",
          "create_app",
          "fastapi.testclient"
        ],
        "reference_types": {
          "fastapi.testclient": "direct_import",
          "api.main": "direct_import",
          "api.routes.MCPSDKClient": "string_ref",
          "api.routes.get_agent": "string_ref"
        },
        "import_count": 6
      },
      {
        "test_id": "test_0049",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_status_endpoint",
        "referenced_classes": [
          "TestClient",
          "api.main",
          "api.routes.MCPSDKClient",
          "api.routes.get_agent",
          "create_app",
          "fastapi.testclient"
        ],
        "reference_types": {
          "fastapi.testclient": "direct_import",
          "api.main": "direct_import",
          "api.routes.MCPSDKClient": "string_ref",
          "api.routes.get_agent": "string_ref"
        },
        "import_count": 6
      },
      {
        "test_id": "test_0050",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_default_values",
        "referenced_classes": [
          "Settings",
          "clear_settings_cache",
          "config.settings",
          "get_settings"
        ],
        "reference_types": {
          "config.settings": "direct_import"
        },
        "import_count": 4
      },
      {
        "test_id": "test_0051",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_from_env",
        "referenced_classes": [
          "Settings",
          "clear_settings_cache",
          "config.settings",
          "get_settings"
        ],
        "reference_types": {
          "config.settings": "direct_import"
        },
        "import_count": 4
      },
      {
        "test_id": "test_0052",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_validate_llm_provider",
        "referenced_classes": [
          "Settings",
          "clear_settings_cache",
          "config.settings",
          "get_settings"
        ],
        "reference_types": {
          "config.settings": "direct_import"
        },
        "import_count": 4
      },
      {
        "test_id": "test_0053",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_validate_embedding_provider",
        "referenced_classes": [
          "Settings",
          "clear_settings_cache",
          "config.settings",
          "get_settings"
        ],
        "reference_types": {
          "config.settings": "direct_import"
        },
        "import_count": 4
      },
      {
        "test_id": "test_0054",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_get_settings_singleton",
        "referenced_classes": [
          "Settings",
          "clear_settings_cache",
          "config.settings",
          "get_settings"
        ],
        "reference_types": {
          "config.settings": "direct_import"
        },
        "import_count": 4
      },
      {
        "test_id": "test_0055",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_clear_settings_cache",
        "referenced_classes": [
          "Settings",
          "clear_settings_cache",
          "config.settings",
          "get_settings"
        ],
        "reference_types": {
          "config.settings": "direct_import"
        },
        "import_count": 4
      },
      {
        "test_id": "test_0056",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_optional_fields",
        "referenced_classes": [
          "Settings",
          "clear_settings_cache",
          "config.settings",
          "get_settings"
        ],
        "reference_types": {
          "config.settings": "direct_import"
        },
        "import_count": 4
      },
      {
        "test_id": "test_0057",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "class_name": "TestInferenceLogger",
        "method_name": "test_log_request",
        "referenced_classes": [
          "InferenceLogger",
          "inference_logging.logger"
        ],
        "reference_types": {
          "inference_logging.logger": "direct_import"
        },
        "import_count": 2
      },
      {
        "test_id": "test_0058",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "class_name": "TestInferenceLogger",
        "method_name": "test_get_logs",
        "referenced_classes": [
          "InferenceLogger",
          "inference_logging.logger"
        ],
        "reference_types": {
          "inference_logging.logger": "direct_import"
        },
        "import_count": 2
      },
      {
        "test_id": "test_0059",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "class_name": "TestInferenceLogger",
        "method_name": "test_get_log_not_found",
        "referenced_classes": [
          "InferenceLogger",
          "inference_logging.logger"
        ],
        "reference_types": {
          "inference_logging.logger": "direct_import"
        },
        "import_count": 2
      },
      {
        "test_id": "test_0060",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_initialization_workflow",
        "referenced_classes": [
          "LangGraphAgent",
          "agent.langgraph_agent",
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes",
          "agent.langgraph_nodes.get_available_tools",
          "get_available_tools"
        ],
        "reference_types": {
          "agent.langgraph_agent": "direct_import",
          "agent.langgraph_nodes": "direct_import",
          "agent.langgraph_agent.LangGraphAgentBuilder": "string_ref",
          "agent.langgraph_agent.MCPSDKClient": "string_ref",
          "agent.langgraph_agent.convert_langgraph_state_to_agent": "string_ref",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain": "string_ref",
          "agent.langgraph_agent.create_langgraph_initial_state": "string_ref",
          "agent.langgraph_agent.load_system_prompt": "string_ref",
          "agent.langgraph_nodes.get_available_tools": "string_ref"
        },
        "import_count": 11
      },
      {
        "test_id": "test_0061",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_invocation_workflow",
        "referenced_classes": [
          "LangGraphAgent",
          "agent.langgraph_agent",
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes",
          "agent.langgraph_nodes.get_available_tools",
          "get_available_tools"
        ],
        "reference_types": {
          "agent.langgraph_agent": "direct_import",
          "agent.langgraph_nodes": "direct_import",
          "agent.langgraph_agent.LangGraphAgentBuilder": "string_ref",
          "agent.langgraph_agent.MCPSDKClient": "string_ref",
          "agent.langgraph_agent.convert_langgraph_state_to_agent": "string_ref",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain": "string_ref",
          "agent.langgraph_agent.create_langgraph_initial_state": "string_ref",
          "agent.langgraph_agent.load_system_prompt": "string_ref",
          "agent.langgraph_nodes.get_available_tools": "string_ref"
        },
        "import_count": 11
      },
      {
        "test_id": "test_0062",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_gemini",
        "referenced_classes": [
          "LLMFactory",
          "Settings",
          "config.settings",
          "llm.factory",
          "llm.factory.GeminiClient",
          "llm.factory.OllamaClient"
        ],
        "reference_types": {
          "llm.factory": "direct_import",
          "config.settings": "direct_import",
          "llm.factory.GeminiClient": "string_ref",
          "llm.factory.OllamaClient": "string_ref"
        },
        "import_count": 6
      },
      {
        "test_id": "test_0063",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_gemini_missing_key",
        "referenced_classes": [
          "LLMFactory",
          "Settings",
          "config.settings",
          "llm.factory",
          "llm.factory.GeminiClient",
          "llm.factory.OllamaClient"
        ],
        "reference_types": {
          "llm.factory": "direct_import",
          "config.settings": "direct_import",
          "llm.factory.GeminiClient": "string_ref",
          "llm.factory.OllamaClient": "string_ref"
        },
        "import_count": 6
      },
      {
        "test_id": "test_0064",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_ollama",
        "referenced_classes": [
          "LLMFactory",
          "Settings",
          "config.settings",
          "llm.factory",
          "llm.factory.GeminiClient",
          "llm.factory.OllamaClient"
        ],
        "reference_types": {
          "llm.factory": "direct_import",
          "config.settings": "direct_import",
          "llm.factory.GeminiClient": "string_ref",
          "llm.factory.OllamaClient": "string_ref"
        },
        "import_count": 6
      },
      {
        "test_id": "test_0065",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_invalid",
        "referenced_classes": [
          "LLMFactory",
          "Settings",
          "config.settings",
          "llm.factory",
          "llm.factory.GeminiClient",
          "llm.factory.OllamaClient"
        ],
        "reference_types": {
          "llm.factory": "direct_import",
          "config.settings": "direct_import",
          "llm.factory.GeminiClient": "string_ref",
          "llm.factory.OllamaClient": "string_ref"
        },
        "import_count": 6
      },
      {
        "test_id": "test_0066",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_get_available_providers",
        "referenced_classes": [
          "LLMFactory",
          "Settings",
          "config.settings",
          "llm.factory",
          "llm.factory.GeminiClient",
          "llm.factory.OllamaClient"
        ],
        "reference_types": {
          "llm.factory": "direct_import",
          "config.settings": "direct_import",
          "llm.factory.GeminiClient": "string_ref",
          "llm.factory.OllamaClient": "string_ref"
        },
        "import_count": 6
      },
      {
        "test_id": "test_0067",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_gemini",
        "referenced_classes": [
          "LLMFactory",
          "Settings",
          "config.settings",
          "llm.factory",
          "llm.factory.GeminiClient",
          "llm.factory.OllamaClient"
        ],
        "reference_types": {
          "llm.factory": "direct_import",
          "config.settings": "direct_import",
          "llm.factory.GeminiClient": "string_ref",
          "llm.factory.OllamaClient": "string_ref"
        },
        "import_count": 6
      },
      {
        "test_id": "test_0068",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_ollama",
        "referenced_classes": [
          "LLMFactory",
          "Settings",
          "config.settings",
          "llm.factory",
          "llm.factory.GeminiClient",
          "llm.factory.OllamaClient"
        ],
        "reference_types": {
          "llm.factory": "direct_import",
          "config.settings": "direct_import",
          "llm.factory.GeminiClient": "string_ref",
          "llm.factory.OllamaClient": "string_ref"
        },
        "import_count": 6
      },
      {
        "test_id": "test_0069",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_invalid",
        "referenced_classes": [
          "LLMFactory",
          "Settings",
          "config.settings",
          "llm.factory",
          "llm.factory.GeminiClient",
          "llm.factory.OllamaClient"
        ],
        "reference_types": {
          "llm.factory": "direct_import",
          "config.settings": "direct_import",
          "llm.factory.GeminiClient": "string_ref",
          "llm.factory.OllamaClient": "string_ref"
        },
        "import_count": 6
      },
      {
        "test_id": "test_0070",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "class_name": "TestCatalogServer",
        "method_name": "test_list_tables_tool",
        "referenced_classes": [
          "describe_table",
          "get_table_row_count",
          "list_tables",
          "mcp_servers.catalog_server.tools",
          "mcp_servers.catalog_server.tools.CatalogManager"
        ],
        "reference_types": {
          "mcp_servers.catalog_server.tools": "direct_import",
          "mcp_servers.catalog_server.tools.CatalogManager": "string_ref"
        },
        "import_count": 5
      },
      {
        "test_id": "test_0071",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "class_name": "TestCatalogServer",
        "method_name": "test_describe_table_tool",
        "referenced_classes": [
          "describe_table",
          "get_table_row_count",
          "list_tables",
          "mcp_servers.catalog_server.tools",
          "mcp_servers.catalog_server.tools.CatalogManager"
        ],
        "reference_types": {
          "mcp_servers.catalog_server.tools": "direct_import",
          "mcp_servers.catalog_server.tools.CatalogManager": "string_ref"
        },
        "import_count": 5
      },
      {
        "test_id": "test_0072",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "class_name": "TestCatalogServer",
        "method_name": "test_get_table_row_count_tool",
        "referenced_classes": [
          "describe_table",
          "get_table_row_count",
          "list_tables",
          "mcp_servers.catalog_server.tools",
          "mcp_servers.catalog_server.tools.CatalogManager"
        ],
        "reference_types": {
          "mcp_servers.catalog_server.tools": "direct_import",
          "mcp_servers.catalog_server.tools.CatalogManager": "string_ref"
        },
        "import_count": 5
      },
      {
        "test_id": "test_0073",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
        "class_name": "TestMLflowTracking",
        "method_name": "test_get_tracker",
        "referenced_classes": [
          "get_tracker",
          "mlflow.tracking",
          "mlflow.tracking.MlflowClient",
          "mlflow.tracking.get_tracker",
          "mlflow.tracking.mlflow"
        ],
        "reference_types": {
          "mlflow.tracking": "direct_import",
          "mlflow.tracking.MlflowClient": "string_ref",
          "mlflow.tracking.get_tracker": "string_ref",
          "mlflow.tracking.mlflow": "string_ref"
        },
        "import_count": 5
      },
      {
        "test_id": "test_0074",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
        "class_name": "TestMLflowTracking",
        "method_name": "test_tracker_disabled_when_mlflow_not_available",
        "referenced_classes": [
          "get_tracker",
          "mlflow.tracking",
          "mlflow.tracking.MlflowClient",
          "mlflow.tracking.get_tracker",
          "mlflow.tracking.mlflow"
        ],
        "reference_types": {
          "mlflow.tracking": "direct_import",
          "mlflow.tracking.MlflowClient": "string_ref",
          "mlflow.tracking.get_tracker": "string_ref",
          "mlflow.tracking.mlflow": "string_ref"
        },
        "import_count": 5
      }
    ],
    "file_dependencies": {
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py": {
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "imports": [
          "unittest.mock",
          "pathlib",
          "llm.factory",
          "config.settings",
          "sys",
          "pytest"
        ],
        "production_imports": [
          "llm.factory",
          "config.settings"
        ],
        "string_references": [
          "llm.factory.GeminiClient",
          "llm.factory.OllamaClient"
        ],
        "production_string_references": [
          "llm.factory.GeminiClient",
          "llm.factory.OllamaClient"
        ],
        "from_imports": [
          [
            "unittest.mock",
            [
              "Mock",
              "MagicMock",
              "patch"
            ]
          ],
          [
            "pathlib",
            [
              "Path"
            ]
          ],
          [
            "llm.factory",
            [
              "LLMFactory"
            ]
          ],
          [
            "config.settings",
            [
              "Settings"
            ]
          ]
        ],
        "production_from_imports": [
          [
            "llm.factory",
            [
              "LLMFactory"
            ]
          ],
          [
            "config.settings",
            [
              "Settings"
            ]
          ]
        ],
        "all_production_references": [
          "config.settings",
          "llm.factory",
          "llm.factory.GeminiClient",
          "llm.factory.OllamaClient"
        ]
      },
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py": {
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "imports": [
          "unittest.mock",
          "pathlib",
          "sys",
          "pytest",
          "inference_logging.logger"
        ],
        "production_imports": [
          "inference_logging.logger"
        ],
        "string_references": [],
        "production_string_references": [],
        "from_imports": [
          [
            "unittest.mock",
            [
              "Mock",
              "AsyncMock",
              "MagicMock",
              "patch"
            ]
          ],
          [
            "pathlib",
            [
              "Path"
            ]
          ],
          [
            "inference_logging.logger",
            [
              "InferenceLogger"
            ]
          ]
        ],
        "production_from_imports": [
          [
            "inference_logging.logger",
            [
              "InferenceLogger"
            ]
          ]
        ],
        "all_production_references": [
          "inference_logging.logger"
        ]
      },
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py": {
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "imports": [
          "unittest.mock",
          "pathlib",
          "agent.langgraph_agent",
          "sys",
          "pytest",
          "agent.langgraph_nodes"
        ],
        "production_imports": [
          "agent.langgraph_agent",
          "agent.langgraph_nodes"
        ],
        "string_references": [
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes.get_available_tools"
        ],
        "production_string_references": [
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes.get_available_tools"
        ],
        "from_imports": [
          [
            "unittest.mock",
            [
              "Mock",
              "AsyncMock",
              "MagicMock",
              "patch"
            ]
          ],
          [
            "pathlib",
            [
              "Path"
            ]
          ],
          [
            "agent.langgraph_agent",
            [
              "LangGraphAgent"
            ]
          ],
          [
            "agent.langgraph_agent",
            [
              "LangGraphAgent"
            ]
          ],
          [
            "agent.langgraph_nodes",
            [
              "get_available_tools"
            ]
          ]
        ],
        "production_from_imports": [
          [
            "agent.langgraph_agent",
            [
              "LangGraphAgent"
            ]
          ],
          [
            "agent.langgraph_agent",
            [
              "LangGraphAgent"
            ]
          ],
          [
            "agent.langgraph_nodes",
            [
              "get_available_tools"
            ]
          ]
        ],
        "all_production_references": [
          "agent.langgraph_agent",
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes",
          "agent.langgraph_nodes.get_available_tools"
        ]
      },
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py": {
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "imports": [
          "unittest.mock",
          "pathlib",
          "sys",
          "pytest",
          "mcp_servers.catalog_server.tools"
        ],
        "production_imports": [
          "mcp_servers.catalog_server.tools"
        ],
        "string_references": [
          "mcp_servers.catalog_server.tools.CatalogManager"
        ],
        "production_string_references": [
          "mcp_servers.catalog_server.tools.CatalogManager"
        ],
        "from_imports": [
          [
            "unittest.mock",
            [
              "Mock",
              "AsyncMock",
              "MagicMock",
              "patch"
            ]
          ],
          [
            "pathlib",
            [
              "Path"
            ]
          ],
          [
            "mcp_servers.catalog_server.tools",
            [
              "list_tables"
            ]
          ],
          [
            "mcp_servers.catalog_server.tools",
            [
              "describe_table"
            ]
          ],
          [
            "mcp_servers.catalog_server.tools",
            [
              "get_table_row_count"
            ]
          ]
        ],
        "production_from_imports": [
          [
            "mcp_servers.catalog_server.tools",
            [
              "list_tables"
            ]
          ],
          [
            "mcp_servers.catalog_server.tools",
            [
              "describe_table"
            ]
          ],
          [
            "mcp_servers.catalog_server.tools",
            [
              "get_table_row_count"
            ]
          ]
        ],
        "all_production_references": [
          "mcp_servers.catalog_server.tools",
          "mcp_servers.catalog_server.tools.CatalogManager"
        ]
      },
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py": {
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "imports": [
          "unittest.mock",
          "pathlib",
          "agent.tool_converter",
          "sys",
          "pytest"
        ],
        "production_imports": [
          "agent.tool_converter"
        ],
        "string_references": [],
        "production_string_references": [],
        "from_imports": [
          [
            "unittest.mock",
            [
              "Mock",
              "MagicMock",
              "AsyncMock"
            ]
          ],
          [
            "pathlib",
            [
              "Path"
            ]
          ],
          [
            "agent.tool_converter",
            [
              "json_schema_to_pydantic",
              "mcp_tool_to_langchain",
              "convert_mcp_tools_to_langchain"
            ]
          ]
        ],
        "production_from_imports": [
          [
            "agent.tool_converter",
            [
              "json_schema_to_pydantic",
              "mcp_tool_to_langchain",
              "convert_mcp_tools_to_langchain"
            ]
          ]
        ],
        "all_production_references": [
          "agent.tool_converter"
        ]
      },
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py": {
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "imports": [
          "unittest.mock",
          "pathlib",
          "sys",
          "pytest",
          "agent.state_converter",
          "langchain_core.messages"
        ],
        "production_imports": [
          "agent.state_converter",
          "langchain_core.messages"
        ],
        "string_references": [],
        "production_string_references": [],
        "from_imports": [
          [
            "unittest.mock",
            [
              "Mock",
              "MagicMock"
            ]
          ],
          [
            "pathlib",
            [
              "Path"
            ]
          ],
          [
            "agent.state_converter",
            [
              "normalize_message_content",
              "convert_to_langchain_messages",
              "convert_from_langchain_messages",
              "convert_langgraph_state_to_agent"
            ]
          ],
          [
            "langchain_core.messages",
            [
              "HumanMessage",
              "AIMessage",
              "SystemMessage",
              "ToolMessage"
            ]
          ]
        ],
        "production_from_imports": [
          [
            "agent.state_converter",
            [
              "normalize_message_content",
              "convert_to_langchain_messages",
              "convert_from_langchain_messages",
              "convert_langgraph_state_to_agent"
            ]
          ],
          [
            "langchain_core.messages",
            [
              "HumanMessage",
              "AIMessage",
              "SystemMessage",
              "ToolMessage"
            ]
          ]
        ],
        "all_production_references": [
          "agent.state_converter",
          "langchain_core.messages"
        ]
      },
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py": {
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "imports": [
          "unittest.mock",
          "asyncio",
          "pathlib",
          "agent.langgraph_state",
          "sys",
          "agent.langgraph_agent",
          "pytest",
          "langchain_core.messages",
          "typing"
        ],
        "production_imports": [
          "agent.langgraph_state",
          "agent.langgraph_agent",
          "langchain_core.messages"
        ],
        "string_references": [
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes.get_available_tools"
        ],
        "production_string_references": [
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes.get_available_tools"
        ],
        "from_imports": [
          [
            "unittest.mock",
            [
              "Mock",
              "AsyncMock",
              "MagicMock",
              "patch",
              "call"
            ]
          ],
          [
            "typing",
            [
              "Dict",
              "Any"
            ]
          ],
          [
            "pathlib",
            [
              "Path"
            ]
          ],
          [
            "agent.langgraph_agent",
            [
              "LangGraphAgent"
            ]
          ],
          [
            "agent.langgraph_state",
            [
              "LangGraphAgentState",
              "create_langgraph_initial_state"
            ]
          ],
          [
            "langchain_core.messages",
            [
              "HumanMessage",
              "AIMessage",
              "ToolMessage"
            ]
          ],
          [
            "langchain_core.messages",
            [
              "AIMessage"
            ]
          ],
          [
            "langchain_core.messages",
            [
              "AIMessage",
              "ToolMessage"
            ]
          ]
        ],
        "production_from_imports": [
          [
            "agent.langgraph_agent",
            [
              "LangGraphAgent"
            ]
          ],
          [
            "agent.langgraph_state",
            [
              "LangGraphAgentState",
              "create_langgraph_initial_state"
            ]
          ],
          [
            "langchain_core.messages",
            [
              "HumanMessage",
              "AIMessage",
              "ToolMessage"
            ]
          ],
          [
            "langchain_core.messages",
            [
              "AIMessage"
            ]
          ],
          [
            "langchain_core.messages",
            [
              "AIMessage",
              "ToolMessage"
            ]
          ]
        ],
        "all_production_references": [
          "agent.langgraph_agent",
          "agent.langgraph_agent.LangGraphAgentBuilder",
          "agent.langgraph_agent.MCPSDKClient",
          "agent.langgraph_agent.convert_langgraph_state_to_agent",
          "agent.langgraph_agent.convert_mcp_tools_to_langchain",
          "agent.langgraph_agent.create_langgraph_initial_state",
          "agent.langgraph_agent.load_system_prompt",
          "agent.langgraph_nodes.get_available_tools",
          "agent.langgraph_state",
          "langchain_core.messages"
        ]
      },
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py": {
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "imports": [
          "unittest.mock",
          "pathlib",
          "fastapi.testclient",
          "api.main",
          "sys",
          "pytest"
        ],
        "production_imports": [
          "fastapi.testclient",
          "api.main"
        ],
        "string_references": [
          "api.routes.MCPSDKClient",
          "api.routes.get_agent"
        ],
        "production_string_references": [
          "api.routes.MCPSDKClient",
          "api.routes.get_agent"
        ],
        "from_imports": [
          [
            "unittest.mock",
            [
              "Mock",
              "AsyncMock",
              "MagicMock",
              "patch"
            ]
          ],
          [
            "fastapi.testclient",
            [
              "TestClient"
            ]
          ],
          [
            "pathlib",
            [
              "Path"
            ]
          ],
          [
            "api.main",
            [
              "create_app"
            ]
          ]
        ],
        "production_from_imports": [
          [
            "fastapi.testclient",
            [
              "TestClient"
            ]
          ],
          [
            "api.main",
            [
              "create_app"
            ]
          ]
        ],
        "all_production_references": [
          "api.main",
          "api.routes.MCPSDKClient",
          "api.routes.get_agent",
          "fastapi.testclient"
        ]
      },
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py": {
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "imports": [
          "unittest.mock",
          "pathlib",
          "agent.langgraph_builder",
          "sys",
          "pytest"
        ],
        "production_imports": [
          "agent.langgraph_builder"
        ],
        "string_references": [
          "agent.langgraph_builder.StateGraph",
          "agent.langgraph_builder.ToolNode",
          "agent.langgraph_builder.call_model",
          "agent.langgraph_builder.get_settings",
          "agent.langgraph_builder.set_available_tools",
          "agent.langgraph_builder.should_continue"
        ],
        "production_string_references": [
          "agent.langgraph_builder.StateGraph",
          "agent.langgraph_builder.ToolNode",
          "agent.langgraph_builder.call_model",
          "agent.langgraph_builder.get_settings",
          "agent.langgraph_builder.set_available_tools",
          "agent.langgraph_builder.should_continue"
        ],
        "from_imports": [
          [
            "unittest.mock",
            [
              "Mock",
              "MagicMock",
              "patch"
            ]
          ],
          [
            "pathlib",
            [
              "Path"
            ]
          ],
          [
            "agent.langgraph_builder",
            [
              "LangGraphAgentBuilder"
            ]
          ],
          [
            "unittest.mock",
            [
              "MagicMock"
            ]
          ]
        ],
        "production_from_imports": [
          [
            "agent.langgraph_builder",
            [
              "LangGraphAgentBuilder"
            ]
          ]
        ],
        "all_production_references": [
          "agent.langgraph_builder",
          "agent.langgraph_builder.StateGraph",
          "agent.langgraph_builder.ToolNode",
          "agent.langgraph_builder.call_model",
          "agent.langgraph_builder.get_settings",
          "agent.langgraph_builder.set_available_tools",
          "agent.langgraph_builder.should_continue"
        ]
      },
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py": {
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py",
        "imports": [
          "unittest.mock",
          "pathlib",
          "analytics.aggregator",
          "sys",
          "pytest"
        ],
        "production_imports": [
          "analytics.aggregator"
        ],
        "string_references": [
          "analytics.aggregator.get_inference_logger"
        ],
        "production_string_references": [
          "analytics.aggregator.get_inference_logger"
        ],
        "from_imports": [
          [
            "unittest.mock",
            [
              "Mock",
              "AsyncMock",
              "MagicMock",
              "patch"
            ]
          ],
          [
            "pathlib",
            [
              "Path"
            ]
          ],
          [
            "analytics.aggregator",
            [
              "AnalyticsAggregator"
            ]
          ]
        ],
        "production_from_imports": [
          [
            "analytics.aggregator",
            [
              "AnalyticsAggregator"
            ]
          ]
        ],
        "all_production_references": [
          "analytics.aggregator",
          "analytics.aggregator.get_inference_logger"
        ]
      },
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py": {
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
        "imports": [
          "unittest.mock",
          "pathlib",
          "mlflow.tracking",
          "sys",
          "pytest"
        ],
        "production_imports": [
          "mlflow.tracking"
        ],
        "string_references": [
          "mlflow.tracking.MlflowClient",
          "mlflow.tracking.get_tracker",
          "mlflow.tracking.mlflow"
        ],
        "production_string_references": [
          "mlflow.tracking.MlflowClient",
          "mlflow.tracking.get_tracker",
          "mlflow.tracking.mlflow"
        ],
        "from_imports": [
          [
            "unittest.mock",
            [
              "Mock",
              "MagicMock",
              "patch"
            ]
          ],
          [
            "pathlib",
            [
              "Path"
            ]
          ],
          [
            "mlflow.tracking",
            [
              "get_tracker"
            ]
          ],
          [
            "mlflow.tracking",
            [
              "get_tracker"
            ]
          ]
        ],
        "production_from_imports": [
          [
            "mlflow.tracking",
            [
              "get_tracker"
            ]
          ],
          [
            "mlflow.tracking",
            [
              "get_tracker"
            ]
          ]
        ],
        "all_production_references": [
          "mlflow.tracking",
          "mlflow.tracking.MlflowClient",
          "mlflow.tracking.get_tracker",
          "mlflow.tracking.mlflow"
        ]
      },
      "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py": {
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "imports": [
          "unittest.mock",
          "pathlib",
          "config.settings",
          "sys",
          "pytest",
          "os"
        ],
        "production_imports": [
          "config.settings"
        ],
        "string_references": [],
        "production_string_references": [],
        "from_imports": [
          [
            "unittest.mock",
            [
              "patch",
              "MagicMock"
            ]
          ],
          [
            "pathlib",
            [
              "Path"
            ]
          ],
          [
            "config.settings",
            [
              "Settings",
              "get_settings",
              "clear_settings_cache"
            ]
          ]
        ],
        "production_from_imports": [
          [
            "config.settings",
            [
              "Settings",
              "get_settings",
              "clear_settings_cache"
            ]
          ]
        ],
        "all_production_references": [
          "config.settings"
        ]
      }
    }
  }
}
```

`test_analysis/outputs/05_test_metadata.json`

```json
{
  "generated_at": "2026-02-26T19:34:12.412703",
  "data": {
    "total_tests": 74,
    "tests_with_descriptions": 74,
    "tests_with_markers": 30,
    "async_tests": 30,
    "parameterized_tests": 42,
    "marker_counts": {
      "asyncio": 30
    },
    "pattern_counts": {
      "test_prefix": 73,
      "bdd_pattern": 1
    },
    "test_metadata": [
      {
        "test_id": "test_0001",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization",
        "name": "test_agent_initialization",
        "description": "Test agent initialization.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 28
      },
      {
        "test_id": "test_0002",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_failure_no_servers",
        "name": "test_agent_initialization_failure_no_servers",
        "description": "Test agent initialization failure when MCP servers are not available.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 46
      },
      {
        "test_id": "test_0003",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_initialization_no_tools",
        "name": "test_agent_initialization_no_tools",
        "description": "Test agent initialization when no tools are available.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 58
      },
      {
        "test_id": "test_0004",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke",
        "name": "test_agent_invoke",
        "description": "Test agent invocation.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 75
      },
      {
        "test_id": "test_0005",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_invoke_with_session_id",
        "name": "test_agent_invoke_with_session_id",
        "description": "Test agent invocation with session ID.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 110
      },
      {
        "test_id": "test_0006",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke",
        "name": "test_agent_stream_invoke",
        "description": "Test agent streaming invocation.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 144
      },
      {
        "test_id": "test_0007",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_stream_invoke_error",
        "name": "test_agent_stream_invoke_error",
        "description": "Test agent streaming invocation with error.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 181
      },
      {
        "test_id": "test_0008",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_close",
        "name": "test_agent_close",
        "description": "Test agent cleanup.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 207
      },
      {
        "test_id": "test_0009",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_context_manager",
        "name": "test_agent_context_manager",
        "description": "Test agent as async context manager.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 218
      },
      {
        "test_id": "test_0010",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_agent_thinking",
        "name": "test_agent_extract_stage_info_agent_thinking",
        "description": "Test stage info extraction for agent thinking.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 235
      },
      {
        "test_id": "test_0011",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_executing",
        "name": "test_agent_extract_stage_info_tool_executing",
        "description": "Test stage info extraction for tool execution.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 247
      },
      {
        "test_id": "test_0012",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
        "class_name": "TestLangGraphAgent",
        "method_name": "test_agent_extract_stage_info_tool_completed",
        "name": "test_agent_extract_stage_info_tool_completed",
        "description": "Test stage info extraction for tool completion.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 266
      },
      {
        "test_id": "test_0013",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization",
        "name": "test_builder_initialization",
        "description": "Test builder initialization.",
        "markers": [],
        "is_async": false,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 27
      },
      {
        "test_id": "test_0014",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_builder_initialization_no_tools",
        "name": "test_builder_initialization_no_tools",
        "description": "Test builder initialization with no tools.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 33
      },
      {
        "test_id": "test_0015",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph",
        "name": "test_build_graph",
        "description": "Test building the graph.",
        "markers": [],
        "is_async": false,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 38
      },
      {
        "test_id": "test_0016",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_build_graph_no_tools",
        "name": "test_build_graph_no_tools",
        "description": "Test building graph with no tools.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 59
      },
      {
        "test_id": "test_0017",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph",
        "name": "test_get_graph",
        "description": "Test getting the compiled graph.",
        "markers": [],
        "is_async": false,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 76
      },
      {
        "test_id": "test_0018",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
        "class_name": "TestLangGraphBuilder",
        "method_name": "test_get_graph_not_built",
        "name": "test_get_graph_not_built",
        "description": "Test getting graph before building.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 94
      },
      {
        "test_id": "test_0019",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_string",
        "name": "test_normalize_message_content_string",
        "description": "Test normalizing string content.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 23
      },
      {
        "test_id": "test_0020",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_list",
        "name": "test_normalize_message_content_list",
        "description": "Test normalizing list content (Gemini format).",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 30
      },
      {
        "test_id": "test_0021",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_list_simple",
        "name": "test_normalize_message_content_list_simple",
        "description": "Test normalizing simple list content.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 40
      },
      {
        "test_id": "test_0022",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_none",
        "name": "test_normalize_message_content_none",
        "description": "Test normalizing None content.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 47
      },
      {
        "test_id": "test_0023",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_normalize_message_content_dict_with_text",
        "name": "test_normalize_message_content_dict_with_text",
        "description": "Test normalizing dict with text key.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 52
      },
      {
        "test_id": "test_0024",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_to_langchain_messages",
        "name": "test_convert_to_langchain_messages",
        "description": "Test converting custom messages to LangChain messages.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 58
      },
      {
        "test_id": "test_0025",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_to_langchain_messages_tool",
        "name": "test_convert_to_langchain_messages_tool",
        "description": "Test converting tool messages.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 73
      },
      {
        "test_id": "test_0026",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_from_langchain_messages",
        "name": "test_convert_from_langchain_messages",
        "description": "Test converting LangChain messages to custom format.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 85
      },
      {
        "test_id": "test_0027",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_from_langchain_messages_with_tool_calls",
        "name": "test_convert_from_langchain_messages_with_tool_calls",
        "description": "Test converting AIMessage with tool calls.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 98
      },
      {
        "test_id": "test_0028",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_langgraph_state_to_agent",
        "name": "test_convert_langgraph_state_to_agent",
        "description": "Test converting LangGraph state to agent state.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 115
      },
      {
        "test_id": "test_0029",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
        "class_name": "TestStateConverter",
        "method_name": "test_convert_langgraph_state_to_agent_with_tool_calls",
        "name": "test_convert_langgraph_state_to_agent_with_tool_calls",
        "description": "Test converting state with tool calls.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 141
      },
      {
        "test_id": "test_0030",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_string",
        "name": "test_json_schema_to_pydantic_string",
        "description": "Test JSON schema to Pydantic conversion for string type.",
        "markers": [],
        "is_async": false,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 62
      },
      {
        "test_id": "test_0031",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_integer",
        "name": "test_json_schema_to_pydantic_integer",
        "description": "Test JSON schema to Pydantic conversion for integer type.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 72
      },
      {
        "test_id": "test_0032",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_array",
        "name": "test_json_schema_to_pydantic_array",
        "description": "Test JSON schema to Pydantic conversion for array type.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 90
      },
      {
        "test_id": "test_0033",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_json_schema_to_pydantic_empty",
        "name": "test_json_schema_to_pydantic_empty",
        "description": "Test JSON schema to Pydantic conversion for empty schema.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 111
      },
      {
        "test_id": "test_0034",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_mcp_tool_to_langchain",
        "name": "test_mcp_tool_to_langchain",
        "description": "Test MCP tool to LangChain conversion.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 123
      },
      {
        "test_id": "test_0035",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_mcp_tool_to_langchain_error_handling",
        "name": "test_mcp_tool_to_langchain_error_handling",
        "description": "Test MCP tool to LangChain conversion with error handling.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 146
      },
      {
        "test_id": "test_0036",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_convert_mcp_tools_to_langchain",
        "name": "test_convert_mcp_tools_to_langchain",
        "description": "Test conversion of multiple MCP tools.",
        "markers": [],
        "is_async": false,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 164
      },
      {
        "test_id": "test_0037",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_convert_mcp_tools_to_langchain_empty",
        "name": "test_convert_mcp_tools_to_langchain_empty",
        "description": "Test conversion with empty tool list.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 182
      },
      {
        "test_id": "test_0038",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
        "class_name": "TestToolConverter",
        "method_name": "test_convert_mcp_tools_to_langchain_conversion_error",
        "name": "test_convert_mcp_tools_to_langchain_conversion_error",
        "description": "Test conversion with tool conversion error.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 196
      },
      {
        "test_id": "test_0039",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py",
        "class_name": "TestAnalyticsAggregator",
        "method_name": "test_get_overview_stats",
        "name": "test_get_overview_stats",
        "description": "Test getting overview statistics.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 22
      },
      {
        "test_id": "test_0040",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py",
        "class_name": "TestAnalyticsAggregator",
        "method_name": "test_get_tool_usage_stats",
        "name": "test_get_tool_usage_stats",
        "description": "Test getting tool usage statistics.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 45
      },
      {
        "test_id": "test_0041",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_root_endpoint",
        "name": "test_root_endpoint",
        "description": "Test root endpoint.",
        "markers": [],
        "is_async": false,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 24
      },
      {
        "test_id": "test_0042",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_health_endpoint",
        "name": "test_health_endpoint",
        "description": "Test health check endpoint.",
        "markers": [],
        "is_async": false,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 33
      },
      {
        "test_id": "test_0043",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint",
        "name": "test_chat_endpoint",
        "description": "Test chat endpoint.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 41
      },
      {
        "test_id": "test_0044",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint_with_session",
        "name": "test_chat_endpoint_with_session",
        "description": "Test chat endpoint with session ID.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 75
      },
      {
        "test_id": "test_0045",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_endpoint_invalid_request",
        "name": "test_chat_endpoint_invalid_request",
        "description": "Test chat endpoint with invalid request.",
        "markers": [],
        "is_async": false,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 107
      },
      {
        "test_id": "test_0046",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_chat_stream_endpoint",
        "name": "test_chat_stream_endpoint",
        "description": "Test chat stream endpoint.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 117
      },
      {
        "test_id": "test_0047",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_tools_endpoint",
        "name": "test_tools_endpoint",
        "description": "Test tools listing endpoint.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 141
      },
      {
        "test_id": "test_0048",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_health_check_endpoint",
        "name": "test_health_check_endpoint",
        "description": "Test health check endpoint with system info.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 162
      },
      {
        "test_id": "test_0049",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
        "class_name": "TestAPIRoutes",
        "method_name": "test_status_endpoint",
        "name": "test_status_endpoint",
        "description": "Test status endpoint.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 173
      },
      {
        "test_id": "test_0050",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_default_values",
        "name": "test_settings_default_values",
        "description": "Test default settings values.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 18
      },
      {
        "test_id": "test_0051",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_from_env",
        "name": "test_settings_from_env",
        "description": "Test loading settings from environment variables.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 29
      },
      {
        "test_id": "test_0052",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_validate_llm_provider",
        "name": "test_settings_validate_llm_provider",
        "description": "Test LLM provider validation.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 46
      },
      {
        "test_id": "test_0053",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_validate_embedding_provider",
        "name": "test_settings_validate_embedding_provider",
        "description": "Test embedding provider validation.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 52
      },
      {
        "test_id": "test_0054",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_get_settings_singleton",
        "name": "test_get_settings_singleton",
        "description": "Test get_settings returns singleton.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 58
      },
      {
        "test_id": "test_0055",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_clear_settings_cache",
        "name": "test_clear_settings_cache",
        "description": "Test clearing settings cache.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 67
      },
      {
        "test_id": "test_0056",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
        "class_name": "TestSettings",
        "method_name": "test_settings_optional_fields",
        "name": "test_settings_optional_fields",
        "description": "Test optional settings fields.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 80
      },
      {
        "test_id": "test_0057",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "class_name": "TestInferenceLogger",
        "method_name": "test_log_request",
        "name": "test_log_request",
        "description": "Test logging a request.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 22
      },
      {
        "test_id": "test_0058",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "class_name": "TestInferenceLogger",
        "method_name": "test_get_logs",
        "name": "test_get_logs",
        "description": "Test getting logs.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 39
      },
      {
        "test_id": "test_0059",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
        "class_name": "TestInferenceLogger",
        "method_name": "test_get_log_not_found",
        "name": "test_get_log_not_found",
        "description": "Test getting non-existent log.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 57
      },
      {
        "test_id": "test_0060",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_initialization_workflow",
        "name": "test_agent_initialization_workflow",
        "description": "Test complete agent initialization workflow.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 16
      },
      {
        "test_id": "test_0061",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
        "class_name": "TestAgentWorkflow",
        "method_name": "test_agent_invocation_workflow",
        "name": "test_agent_invocation_workflow",
        "description": "Test complete agent invocation workflow.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 53
      },
      {
        "test_id": "test_0062",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_gemini",
        "name": "test_create_provider_gemini",
        "description": "Test creating Gemini provider.",
        "markers": [],
        "is_async": false,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 38
      },
      {
        "test_id": "test_0063",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_gemini_missing_key",
        "name": "test_create_provider_gemini_missing_key",
        "description": "Test creating Gemini provider without API key.",
        "markers": [],
        "is_async": false,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 52
      },
      {
        "test_id": "test_0064",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_ollama",
        "name": "test_create_provider_ollama",
        "description": "Test creating Ollama provider.",
        "markers": [],
        "is_async": false,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 59
      },
      {
        "test_id": "test_0065",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_provider_invalid",
        "name": "test_create_provider_invalid",
        "description": "Test creating provider with invalid name.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 70
      },
      {
        "test_id": "test_0066",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_get_available_providers",
        "name": "test_get_available_providers",
        "description": "Test getting list of available providers.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 78
      },
      {
        "test_id": "test_0067",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_gemini",
        "name": "test_create_embedding_provider_gemini",
        "description": "Test creating Gemini embedding provider.",
        "markers": [],
        "is_async": false,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 88
      },
      {
        "test_id": "test_0068",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_ollama",
        "name": "test_create_embedding_provider_ollama",
        "description": "Test creating Ollama embedding provider.",
        "markers": [],
        "is_async": false,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 101
      },
      {
        "test_id": "test_0069",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
        "class_name": "TestLLMFactory",
        "method_name": "test_create_embedding_provider_invalid",
        "name": "test_create_embedding_provider_invalid",
        "description": "Test creating embedding provider with invalid name.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "test_prefix",
        "line_number": 114
      },
      {
        "test_id": "test_0070",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "class_name": "TestCatalogServer",
        "method_name": "test_list_tables_tool",
        "name": "test_list_tables_tool",
        "description": "Test list_tables tool.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 27
      },
      {
        "test_id": "test_0071",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "class_name": "TestCatalogServer",
        "method_name": "test_describe_table_tool",
        "name": "test_describe_table_tool",
        "description": "Test describe_table tool.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 38
      },
      {
        "test_id": "test_0072",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
        "class_name": "TestCatalogServer",
        "method_name": "test_get_table_row_count_tool",
        "name": "test_get_table_row_count_tool",
        "description": "Test get_table_row_count tool.",
        "markers": [
          "asyncio"
        ],
        "is_async": true,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 49
      },
      {
        "test_id": "test_0073",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
        "class_name": "TestMLflowTracking",
        "method_name": "test_get_tracker",
        "name": "test_get_tracker",
        "description": "Test getting MLflow tracker.",
        "markers": [],
        "is_async": false,
        "is_parameterized": true,
        "pattern": "test_prefix",
        "line_number": 24
      },
      {
        "test_id": "test_0074",
        "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
        "class_name": "TestMLflowTracking",
        "method_name": "test_tracker_disabled_when_mlflow_not_available",
        "name": "test_tracker_disabled_when_mlflow_not_available",
        "description": "Test tracker is disabled when MLflow is not available.",
        "markers": [],
        "is_async": false,
        "is_parameterized": false,
        "pattern": "bdd_pattern",
        "line_number": 33
      }
    ]
  }
}
```

`test_analysis/outputs/06_reverse_index.json`

```json
{
  "generated_at": "2026-02-26T19:34:13.128846",
  "data": {
    "total_production_classes": 65,
    "total_mappings": 575,
    "average_tests_per_class": 8.85,
    "most_referenced_classes": [
      {
        "class": "AIMessage",
        "test_count": 23,
        "tests": [
          {
            "test_id": "test_0001",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0002",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_failure_no_servers",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0003",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_no_tools",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0004",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0005",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke_with_session_id",
            "reference_type": "direct_import"
          }
        ]
      },
      {
        "class": "HumanMessage",
        "test_count": 23,
        "tests": [
          {
            "test_id": "test_0001",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0002",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_failure_no_servers",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0003",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_no_tools",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0004",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0005",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke_with_session_id",
            "reference_type": "direct_import"
          }
        ]
      },
      {
        "class": "ToolMessage",
        "test_count": 23,
        "tests": [
          {
            "test_id": "test_0001",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0002",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_failure_no_servers",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0003",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_no_tools",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0004",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0005",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke_with_session_id",
            "reference_type": "direct_import"
          }
        ]
      },
      {
        "class": "langchain_core.messages",
        "test_count": 23,
        "tests": [
          {
            "test_id": "test_0001",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0002",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_failure_no_servers",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0003",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_no_tools",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0004",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0005",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke_with_session_id",
            "reference_type": "direct_import"
          }
        ]
      },
      {
        "class": "Settings",
        "test_count": 15,
        "tests": [
          {
            "test_id": "test_0050",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
            "class_name": "TestSettings",
            "method_name": "test_settings_default_values",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0051",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
            "class_name": "TestSettings",
            "method_name": "test_settings_from_env",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0052",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
            "class_name": "TestSettings",
            "method_name": "test_settings_validate_llm_provider",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0053",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
            "class_name": "TestSettings",
            "method_name": "test_settings_validate_embedding_provider",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0054",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
            "class_name": "TestSettings",
            "method_name": "test_get_settings_singleton",
            "reference_type": "direct_import"
          }
        ]
      },
      {
        "class": "config.settings",
        "test_count": 15,
        "tests": [
          {
            "test_id": "test_0050",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
            "class_name": "TestSettings",
            "method_name": "test_settings_default_values",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0051",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
            "class_name": "TestSettings",
            "method_name": "test_settings_from_env",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0052",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
            "class_name": "TestSettings",
            "method_name": "test_settings_validate_llm_provider",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0053",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
            "class_name": "TestSettings",
            "method_name": "test_settings_validate_embedding_provider",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0054",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
            "class_name": "TestSettings",
            "method_name": "test_get_settings_singleton",
            "reference_type": "direct_import"
          }
        ]
      },
      {
        "class": "LangGraphAgent",
        "test_count": 14,
        "tests": [
          {
            "test_id": "test_0001",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0002",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_failure_no_servers",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0003",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_no_tools",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0004",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0005",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke_with_session_id",
            "reference_type": "direct_import"
          }
        ]
      },
      {
        "class": "agent.langgraph_agent",
        "test_count": 14,
        "tests": [
          {
            "test_id": "test_0001",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0002",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_failure_no_servers",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0003",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_no_tools",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0004",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0005",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke_with_session_id",
            "reference_type": "direct_import"
          }
        ]
      },
      {
        "class": "agent.langgraph_agent.LangGraphAgentBuilder",
        "test_count": 14,
        "tests": [
          {
            "test_id": "test_0001",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0002",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_failure_no_servers",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0003",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_no_tools",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0004",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0005",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke_with_session_id",
            "reference_type": "string_ref"
          }
        ]
      },
      {
        "class": "agent.langgraph_agent.MCPSDKClient",
        "test_count": 14,
        "tests": [
          {
            "test_id": "test_0001",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0002",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_failure_no_servers",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0003",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_no_tools",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0004",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0005",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke_with_session_id",
            "reference_type": "string_ref"
          }
        ]
      },
      {
        "class": "agent.langgraph_agent.convert_langgraph_state_to_agent",
        "test_count": 14,
        "tests": [
          {
            "test_id": "test_0001",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0002",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_failure_no_servers",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0003",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_no_tools",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0004",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0005",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke_with_session_id",
            "reference_type": "string_ref"
          }
        ]
      },
      {
        "class": "agent.langgraph_agent.convert_mcp_tools_to_langchain",
        "test_count": 14,
        "tests": [
          {
            "test_id": "test_0001",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0002",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_failure_no_servers",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0003",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_no_tools",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0004",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0005",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke_with_session_id",
            "reference_type": "string_ref"
          }
        ]
      },
      {
        "class": "agent.langgraph_agent.create_langgraph_initial_state",
        "test_count": 14,
        "tests": [
          {
            "test_id": "test_0001",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0002",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_failure_no_servers",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0003",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_no_tools",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0004",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0005",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke_with_session_id",
            "reference_type": "string_ref"
          }
        ]
      },
      {
        "class": "agent.langgraph_agent.load_system_prompt",
        "test_count": 14,
        "tests": [
          {
            "test_id": "test_0001",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0002",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_failure_no_servers",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0003",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_no_tools",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0004",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0005",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke_with_session_id",
            "reference_type": "string_ref"
          }
        ]
      },
      {
        "class": "agent.langgraph_nodes.get_available_tools",
        "test_count": 14,
        "tests": [
          {
            "test_id": "test_0001",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0002",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_failure_no_servers",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0003",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_no_tools",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0004",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke",
            "reference_type": "string_ref"
          },
          {
            "test_id": "test_0005",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke_with_session_id",
            "reference_type": "string_ref"
          }
        ]
      },
      {
        "class": "LangGraphAgentState",
        "test_count": 12,
        "tests": [
          {
            "test_id": "test_0001",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0002",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_failure_no_servers",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0003",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_no_tools",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0004",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0005",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke_with_session_id",
            "reference_type": "direct_import"
          }
        ]
      },
      {
        "class": "agent.langgraph_state",
        "test_count": 12,
        "tests": [
          {
            "test_id": "test_0001",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0002",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_failure_no_servers",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0003",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_no_tools",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0004",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0005",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke_with_session_id",
            "reference_type": "direct_import"
          }
        ]
      },
      {
        "class": "create_langgraph_initial_state",
        "test_count": 12,
        "tests": [
          {
            "test_id": "test_0001",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0002",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_failure_no_servers",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0003",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_initialization_no_tools",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0004",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0005",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
            "class_name": "TestLangGraphAgent",
            "method_name": "test_agent_invoke_with_session_id",
            "reference_type": "direct_import"
          }
        ]
      },
      {
        "class": "SystemMessage",
        "test_count": 11,
        "tests": [
          {
            "test_id": "test_0019",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
            "class_name": "TestStateConverter",
            "method_name": "test_normalize_message_content_string",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0020",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
            "class_name": "TestStateConverter",
            "method_name": "test_normalize_message_content_list",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0021",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
            "class_name": "TestStateConverter",
            "method_name": "test_normalize_message_content_list_simple",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0022",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
            "class_name": "TestStateConverter",
            "method_name": "test_normalize_message_content_none",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0023",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
            "class_name": "TestStateConverter",
            "method_name": "test_normalize_message_content_dict_with_text",
            "reference_type": "direct_import"
          }
        ]
      },
      {
        "class": "agent.state_converter",
        "test_count": 11,
        "tests": [
          {
            "test_id": "test_0019",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
            "class_name": "TestStateConverter",
            "method_name": "test_normalize_message_content_string",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0020",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
            "class_name": "TestStateConverter",
            "method_name": "test_normalize_message_content_list",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0021",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
            "class_name": "TestStateConverter",
            "method_name": "test_normalize_message_content_list_simple",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0022",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
            "class_name": "TestStateConverter",
            "method_name": "test_normalize_message_content_none",
            "reference_type": "direct_import"
          },
          {
            "test_id": "test_0023",
            "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
            "class_name": "TestStateConverter",
            "method_name": "test_normalize_message_content_dict_with_text",
            "reference_type": "direct_import"
          }
        ]
      }
    ],
    "module_groups": {
      "AIMessage": 1,
      "HumanMessage": 1,
      "LangGraphAgent": 1,
      "LangGraphAgentState": 1,
      "ToolMessage": 1,
      "agent": 19,
      "create_langgraph_initial_state": 1,
      "langchain_core": 1,
      "LangGraphAgentBuilder": 1,
      "SystemMessage": 1,
      "convert_from_langchain_messages": 1,
      "convert_langgraph_state_to_agent": 1,
      "convert_to_langchain_messages": 1,
      "normalize_message_content": 1,
      "convert_mcp_tools_to_langchain": 1,
      "json_schema_to_pydantic": 1,
      "mcp_tool_to_langchain": 1,
      "AnalyticsAggregator": 1,
      "analytics": 2,
      "TestClient": 1,
      "api": 3,
      "create_app": 1,
      "fastapi": 1,
      "Settings": 1,
      "clear_settings_cache": 1,
      "config": 1,
      "get_settings": 1,
      "InferenceLogger": 1,
      "inference_logging": 1,
      "get_available_tools": 1,
      "LLMFactory": 1,
      "llm": 3,
      "describe_table": 1,
      "get_table_row_count": 1,
      "list_tables": 1,
      "mcp_servers": 2,
      "get_tracker": 1,
      "mlflow": 4
    },
    "reverse_index": {
      "AIMessage": [
        {
          "test_id": "test_0001",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0002",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_failure_no_servers",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0003",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_no_tools",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0004",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0005",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke_with_session_id",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0006",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0007",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke_error",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0008",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_close",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0009",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_context_manager",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0010",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_agent_thinking",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0011",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_executing",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0012",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_completed",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0019",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_string",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0020",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0021",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list_simple",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0022",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_none",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0023",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_dict_with_text",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0024",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0025",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages_tool",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0026",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0027",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages_with_tool_calls",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0028",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0029",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent_with_tool_calls",
          "reference_type": "direct_import"
        }
      ],
      "HumanMessage": [
        {
          "test_id": "test_0001",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0002",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_failure_no_servers",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0003",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_no_tools",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0004",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0005",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke_with_session_id",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0006",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0007",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke_error",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0008",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_close",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0009",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_context_manager",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0010",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_agent_thinking",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0011",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_executing",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0012",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_completed",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0019",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_string",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0020",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0021",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list_simple",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0022",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_none",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0023",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_dict_with_text",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0024",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0025",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages_tool",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0026",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0027",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages_with_tool_calls",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0028",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0029",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent_with_tool_calls",
          "reference_type": "direct_import"
        }
      ],
      "LangGraphAgent": [
        {
          "test_id": "test_0001",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0002",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_failure_no_servers",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0003",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_no_tools",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0004",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0005",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke_with_session_id",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0006",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0007",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke_error",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0008",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_close",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0009",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_context_manager",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0010",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_agent_thinking",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0011",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_executing",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0012",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_completed",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0060",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_initialization_workflow",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0061",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_invocation_workflow",
          "reference_type": "direct_import"
        }
      ],
      "LangGraphAgentState": [
        {
          "test_id": "test_0001",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0002",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_failure_no_servers",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0003",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_no_tools",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0004",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0005",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke_with_session_id",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0006",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0007",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke_error",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0008",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_close",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0009",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_context_manager",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0010",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_agent_thinking",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0011",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_executing",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0012",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_completed",
          "reference_type": "direct_import"
        }
      ],
      "ToolMessage": [
        {
          "test_id": "test_0001",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0002",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_failure_no_servers",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0003",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_no_tools",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0004",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0005",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke_with_session_id",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0006",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0007",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke_error",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0008",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_close",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0009",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_context_manager",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0010",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_agent_thinking",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0011",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_executing",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0012",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_completed",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0019",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_string",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0020",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0021",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list_simple",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0022",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_none",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0023",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_dict_with_text",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0024",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0025",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages_tool",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0026",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0027",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages_with_tool_calls",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0028",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0029",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent_with_tool_calls",
          "reference_type": "direct_import"
        }
      ],
      "agent.langgraph_agent": [
        {
          "test_id": "test_0001",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0002",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_failure_no_servers",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0003",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_no_tools",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0004",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0005",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke_with_session_id",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0006",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0007",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke_error",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0008",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_close",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0009",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_context_manager",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0010",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_agent_thinking",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0011",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_executing",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0012",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_completed",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0060",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_initialization_workflow",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0061",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_invocation_workflow",
          "reference_type": "direct_import"
        }
      ],
      "agent.langgraph_agent.LangGraphAgentBuilder": [
        {
          "test_id": "test_0001",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0002",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_failure_no_servers",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0003",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_no_tools",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0004",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0005",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke_with_session_id",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0006",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0007",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke_error",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0008",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_close",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0009",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_context_manager",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0010",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_agent_thinking",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0011",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_executing",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0012",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_completed",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0060",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_initialization_workflow",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0061",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_invocation_workflow",
          "reference_type": "string_ref"
        }
      ],
      "agent.langgraph_agent.MCPSDKClient": [
        {
          "test_id": "test_0001",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0002",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_failure_no_servers",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0003",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_no_tools",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0004",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0005",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke_with_session_id",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0006",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0007",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke_error",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0008",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_close",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0009",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_context_manager",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0010",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_agent_thinking",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0011",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_executing",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0012",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_completed",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0060",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_initialization_workflow",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0061",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_invocation_workflow",
          "reference_type": "string_ref"
        }
      ],
      "agent.langgraph_agent.convert_langgraph_state_to_agent": [
        {
          "test_id": "test_0001",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0002",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_failure_no_servers",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0003",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_no_tools",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0004",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0005",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke_with_session_id",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0006",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0007",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke_error",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0008",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_close",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0009",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_context_manager",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0010",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_agent_thinking",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0011",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_executing",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0012",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_completed",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0060",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_initialization_workflow",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0061",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_invocation_workflow",
          "reference_type": "string_ref"
        }
      ],
      "agent.langgraph_agent.convert_mcp_tools_to_langchain": [
        {
          "test_id": "test_0001",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0002",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_failure_no_servers",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0003",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_no_tools",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0004",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0005",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke_with_session_id",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0006",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0007",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke_error",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0008",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_close",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0009",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_context_manager",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0010",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_agent_thinking",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0011",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_executing",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0012",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_completed",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0060",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_initialization_workflow",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0061",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_invocation_workflow",
          "reference_type": "string_ref"
        }
      ],
      "agent.langgraph_agent.create_langgraph_initial_state": [
        {
          "test_id": "test_0001",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0002",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_failure_no_servers",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0003",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_no_tools",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0004",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0005",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke_with_session_id",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0006",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0007",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke_error",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0008",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_close",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0009",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_context_manager",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0010",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_agent_thinking",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0011",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_executing",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0012",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_completed",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0060",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_initialization_workflow",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0061",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_invocation_workflow",
          "reference_type": "string_ref"
        }
      ],
      "agent.langgraph_agent.load_system_prompt": [
        {
          "test_id": "test_0001",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0002",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_failure_no_servers",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0003",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_no_tools",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0004",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0005",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke_with_session_id",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0006",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0007",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke_error",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0008",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_close",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0009",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_context_manager",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0010",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_agent_thinking",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0011",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_executing",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0012",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_completed",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0060",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_initialization_workflow",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0061",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_invocation_workflow",
          "reference_type": "string_ref"
        }
      ],
      "agent.langgraph_nodes.get_available_tools": [
        {
          "test_id": "test_0001",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0002",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_failure_no_servers",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0003",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_no_tools",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0004",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0005",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke_with_session_id",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0006",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0007",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke_error",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0008",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_close",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0009",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_context_manager",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0010",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_agent_thinking",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0011",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_executing",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0012",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_completed",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0060",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_initialization_workflow",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0061",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_invocation_workflow",
          "reference_type": "string_ref"
        }
      ],
      "agent.langgraph_state": [
        {
          "test_id": "test_0001",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0002",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_failure_no_servers",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0003",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_no_tools",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0004",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0005",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke_with_session_id",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0006",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0007",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke_error",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0008",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_close",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0009",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_context_manager",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0010",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_agent_thinking",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0011",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_executing",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0012",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_completed",
          "reference_type": "direct_import"
        }
      ],
      "create_langgraph_initial_state": [
        {
          "test_id": "test_0001",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0002",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_failure_no_servers",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0003",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_no_tools",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0004",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0005",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke_with_session_id",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0006",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0007",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke_error",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0008",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_close",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0009",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_context_manager",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0010",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_agent_thinking",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0011",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_executing",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0012",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_completed",
          "reference_type": "direct_import"
        }
      ],
      "langchain_core.messages": [
        {
          "test_id": "test_0001",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0002",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_failure_no_servers",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0003",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_initialization_no_tools",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0004",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0005",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_invoke_with_session_id",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0006",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0007",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_stream_invoke_error",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0008",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_close",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0009",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_context_manager",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0010",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_agent_thinking",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0011",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_executing",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0012",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_agent.py",
          "class_name": "TestLangGraphAgent",
          "method_name": "test_agent_extract_stage_info_tool_completed",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0019",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_string",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0020",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0021",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list_simple",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0022",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_none",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0023",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_dict_with_text",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0024",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0025",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages_tool",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0026",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0027",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages_with_tool_calls",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0028",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0029",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent_with_tool_calls",
          "reference_type": "direct_import"
        }
      ],
      "LangGraphAgentBuilder": [
        {
          "test_id": "test_0013",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_builder_initialization",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0014",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_builder_initialization_no_tools",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0015",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_build_graph",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0016",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_build_graph_no_tools",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0017",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_get_graph",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0018",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_get_graph_not_built",
          "reference_type": "direct_import"
        }
      ],
      "agent.langgraph_builder": [
        {
          "test_id": "test_0013",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_builder_initialization",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0014",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_builder_initialization_no_tools",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0015",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_build_graph",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0016",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_build_graph_no_tools",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0017",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_get_graph",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0018",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_get_graph_not_built",
          "reference_type": "direct_import"
        }
      ],
      "agent.langgraph_builder.StateGraph": [
        {
          "test_id": "test_0013",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_builder_initialization",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0014",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_builder_initialization_no_tools",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0015",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_build_graph",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0016",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_build_graph_no_tools",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0017",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_get_graph",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0018",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_get_graph_not_built",
          "reference_type": "string_ref"
        }
      ],
      "agent.langgraph_builder.ToolNode": [
        {
          "test_id": "test_0013",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_builder_initialization",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0014",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_builder_initialization_no_tools",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0015",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_build_graph",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0016",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_build_graph_no_tools",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0017",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_get_graph",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0018",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_get_graph_not_built",
          "reference_type": "string_ref"
        }
      ],
      "agent.langgraph_builder.call_model": [
        {
          "test_id": "test_0013",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_builder_initialization",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0014",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_builder_initialization_no_tools",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0015",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_build_graph",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0016",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_build_graph_no_tools",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0017",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_get_graph",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0018",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_get_graph_not_built",
          "reference_type": "string_ref"
        }
      ],
      "agent.langgraph_builder.get_settings": [
        {
          "test_id": "test_0013",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_builder_initialization",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0014",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_builder_initialization_no_tools",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0015",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_build_graph",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0016",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_build_graph_no_tools",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0017",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_get_graph",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0018",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_get_graph_not_built",
          "reference_type": "string_ref"
        }
      ],
      "agent.langgraph_builder.set_available_tools": [
        {
          "test_id": "test_0013",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_builder_initialization",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0014",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_builder_initialization_no_tools",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0015",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_build_graph",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0016",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_build_graph_no_tools",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0017",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_get_graph",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0018",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_get_graph_not_built",
          "reference_type": "string_ref"
        }
      ],
      "agent.langgraph_builder.should_continue": [
        {
          "test_id": "test_0013",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_builder_initialization",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0014",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_builder_initialization_no_tools",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0015",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_build_graph",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0016",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_build_graph_no_tools",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0017",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_get_graph",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0018",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_langgraph_builder.py",
          "class_name": "TestLangGraphBuilder",
          "method_name": "test_get_graph_not_built",
          "reference_type": "string_ref"
        }
      ],
      "SystemMessage": [
        {
          "test_id": "test_0019",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_string",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0020",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0021",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list_simple",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0022",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_none",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0023",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_dict_with_text",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0024",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0025",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages_tool",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0026",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0027",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages_with_tool_calls",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0028",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0029",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent_with_tool_calls",
          "reference_type": "direct_import"
        }
      ],
      "agent.state_converter": [
        {
          "test_id": "test_0019",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_string",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0020",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0021",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list_simple",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0022",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_none",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0023",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_dict_with_text",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0024",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0025",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages_tool",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0026",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0027",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages_with_tool_calls",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0028",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0029",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent_with_tool_calls",
          "reference_type": "direct_import"
        }
      ],
      "convert_from_langchain_messages": [
        {
          "test_id": "test_0019",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_string",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0020",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0021",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list_simple",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0022",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_none",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0023",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_dict_with_text",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0024",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0025",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages_tool",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0026",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0027",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages_with_tool_calls",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0028",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0029",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent_with_tool_calls",
          "reference_type": "direct_import"
        }
      ],
      "convert_langgraph_state_to_agent": [
        {
          "test_id": "test_0019",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_string",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0020",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0021",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list_simple",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0022",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_none",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0023",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_dict_with_text",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0024",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0025",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages_tool",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0026",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0027",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages_with_tool_calls",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0028",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0029",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent_with_tool_calls",
          "reference_type": "direct_import"
        }
      ],
      "convert_to_langchain_messages": [
        {
          "test_id": "test_0019",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_string",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0020",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0021",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list_simple",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0022",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_none",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0023",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_dict_with_text",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0024",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0025",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages_tool",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0026",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0027",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages_with_tool_calls",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0028",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0029",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent_with_tool_calls",
          "reference_type": "direct_import"
        }
      ],
      "normalize_message_content": [
        {
          "test_id": "test_0019",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_string",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0020",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0021",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_list_simple",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0022",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_none",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0023",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_normalize_message_content_dict_with_text",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0024",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0025",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_to_langchain_messages_tool",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0026",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0027",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_from_langchain_messages_with_tool_calls",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0028",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0029",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_state_converter.py",
          "class_name": "TestStateConverter",
          "method_name": "test_convert_langgraph_state_to_agent_with_tool_calls",
          "reference_type": "direct_import"
        }
      ],
      "agent.tool_converter": [
        {
          "test_id": "test_0030",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_json_schema_to_pydantic_string",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0031",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_json_schema_to_pydantic_integer",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0032",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_json_schema_to_pydantic_array",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0033",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_json_schema_to_pydantic_empty",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0034",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_mcp_tool_to_langchain",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0035",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_mcp_tool_to_langchain_error_handling",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0036",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_convert_mcp_tools_to_langchain",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0037",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_convert_mcp_tools_to_langchain_empty",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0038",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_convert_mcp_tools_to_langchain_conversion_error",
          "reference_type": "direct_import"
        }
      ],
      "convert_mcp_tools_to_langchain": [
        {
          "test_id": "test_0030",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_json_schema_to_pydantic_string",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0031",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_json_schema_to_pydantic_integer",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0032",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_json_schema_to_pydantic_array",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0033",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_json_schema_to_pydantic_empty",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0034",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_mcp_tool_to_langchain",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0035",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_mcp_tool_to_langchain_error_handling",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0036",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_convert_mcp_tools_to_langchain",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0037",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_convert_mcp_tools_to_langchain_empty",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0038",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_convert_mcp_tools_to_langchain_conversion_error",
          "reference_type": "direct_import"
        }
      ],
      "json_schema_to_pydantic": [
        {
          "test_id": "test_0030",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_json_schema_to_pydantic_string",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0031",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_json_schema_to_pydantic_integer",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0032",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_json_schema_to_pydantic_array",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0033",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_json_schema_to_pydantic_empty",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0034",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_mcp_tool_to_langchain",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0035",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_mcp_tool_to_langchain_error_handling",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0036",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_convert_mcp_tools_to_langchain",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0037",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_convert_mcp_tools_to_langchain_empty",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0038",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_convert_mcp_tools_to_langchain_conversion_error",
          "reference_type": "direct_import"
        }
      ],
      "mcp_tool_to_langchain": [
        {
          "test_id": "test_0030",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_json_schema_to_pydantic_string",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0031",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_json_schema_to_pydantic_integer",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0032",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_json_schema_to_pydantic_array",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0033",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_json_schema_to_pydantic_empty",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0034",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_mcp_tool_to_langchain",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0035",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_mcp_tool_to_langchain_error_handling",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0036",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_convert_mcp_tools_to_langchain",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0037",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_convert_mcp_tools_to_langchain_empty",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0038",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\agent\\test_tool_converter.py",
          "class_name": "TestToolConverter",
          "method_name": "test_convert_mcp_tools_to_langchain_conversion_error",
          "reference_type": "direct_import"
        }
      ],
      "AnalyticsAggregator": [
        {
          "test_id": "test_0039",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py",
          "class_name": "TestAnalyticsAggregator",
          "method_name": "test_get_overview_stats",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0040",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py",
          "class_name": "TestAnalyticsAggregator",
          "method_name": "test_get_tool_usage_stats",
          "reference_type": "direct_import"
        }
      ],
      "analytics.aggregator": [
        {
          "test_id": "test_0039",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py",
          "class_name": "TestAnalyticsAggregator",
          "method_name": "test_get_overview_stats",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0040",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py",
          "class_name": "TestAnalyticsAggregator",
          "method_name": "test_get_tool_usage_stats",
          "reference_type": "direct_import"
        }
      ],
      "analytics.aggregator.get_inference_logger": [
        {
          "test_id": "test_0039",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py",
          "class_name": "TestAnalyticsAggregator",
          "method_name": "test_get_overview_stats",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0040",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\analytics\\test_aggregator.py",
          "class_name": "TestAnalyticsAggregator",
          "method_name": "test_get_tool_usage_stats",
          "reference_type": "string_ref"
        }
      ],
      "TestClient": [
        {
          "test_id": "test_0041",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_root_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0042",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_health_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0043",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0044",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_endpoint_with_session",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0045",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_endpoint_invalid_request",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0046",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_stream_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0047",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_tools_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0048",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_health_check_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0049",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_status_endpoint",
          "reference_type": "direct_import"
        }
      ],
      "api.main": [
        {
          "test_id": "test_0041",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_root_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0042",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_health_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0043",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0044",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_endpoint_with_session",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0045",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_endpoint_invalid_request",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0046",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_stream_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0047",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_tools_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0048",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_health_check_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0049",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_status_endpoint",
          "reference_type": "direct_import"
        }
      ],
      "api.routes.MCPSDKClient": [
        {
          "test_id": "test_0041",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_root_endpoint",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0042",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_health_endpoint",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0043",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_endpoint",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0044",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_endpoint_with_session",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0045",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_endpoint_invalid_request",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0046",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_stream_endpoint",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0047",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_tools_endpoint",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0048",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_health_check_endpoint",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0049",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_status_endpoint",
          "reference_type": "string_ref"
        }
      ],
      "api.routes.get_agent": [
        {
          "test_id": "test_0041",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_root_endpoint",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0042",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_health_endpoint",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0043",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_endpoint",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0044",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_endpoint_with_session",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0045",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_endpoint_invalid_request",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0046",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_stream_endpoint",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0047",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_tools_endpoint",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0048",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_health_check_endpoint",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0049",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_status_endpoint",
          "reference_type": "string_ref"
        }
      ],
      "create_app": [
        {
          "test_id": "test_0041",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_root_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0042",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_health_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0043",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0044",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_endpoint_with_session",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0045",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_endpoint_invalid_request",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0046",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_stream_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0047",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_tools_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0048",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_health_check_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0049",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_status_endpoint",
          "reference_type": "direct_import"
        }
      ],
      "fastapi.testclient": [
        {
          "test_id": "test_0041",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_root_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0042",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_health_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0043",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0044",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_endpoint_with_session",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0045",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_endpoint_invalid_request",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0046",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_chat_stream_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0047",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_tools_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0048",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_health_check_endpoint",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0049",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\api\\test_routes.py",
          "class_name": "TestAPIRoutes",
          "method_name": "test_status_endpoint",
          "reference_type": "direct_import"
        }
      ],
      "Settings": [
        {
          "test_id": "test_0050",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_default_values",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0051",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_from_env",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0052",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_validate_llm_provider",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0053",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_validate_embedding_provider",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0054",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_get_settings_singleton",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0055",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_clear_settings_cache",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0056",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_optional_fields",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0062",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_gemini",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0063",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_gemini_missing_key",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0064",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_ollama",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0065",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_invalid",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0066",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_get_available_providers",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0067",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_embedding_provider_gemini",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0068",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_embedding_provider_ollama",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0069",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_embedding_provider_invalid",
          "reference_type": "direct_import"
        }
      ],
      "clear_settings_cache": [
        {
          "test_id": "test_0050",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_default_values",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0051",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_from_env",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0052",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_validate_llm_provider",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0053",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_validate_embedding_provider",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0054",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_get_settings_singleton",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0055",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_clear_settings_cache",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0056",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_optional_fields",
          "reference_type": "direct_import"
        }
      ],
      "config.settings": [
        {
          "test_id": "test_0050",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_default_values",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0051",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_from_env",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0052",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_validate_llm_provider",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0053",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_validate_embedding_provider",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0054",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_get_settings_singleton",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0055",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_clear_settings_cache",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0056",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_optional_fields",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0062",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_gemini",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0063",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_gemini_missing_key",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0064",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_ollama",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0065",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_invalid",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0066",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_get_available_providers",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0067",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_embedding_provider_gemini",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0068",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_embedding_provider_ollama",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0069",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_embedding_provider_invalid",
          "reference_type": "direct_import"
        }
      ],
      "get_settings": [
        {
          "test_id": "test_0050",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_default_values",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0051",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_from_env",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0052",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_validate_llm_provider",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0053",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_validate_embedding_provider",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0054",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_get_settings_singleton",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0055",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_clear_settings_cache",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0056",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\config\\test_settings.py",
          "class_name": "TestSettings",
          "method_name": "test_settings_optional_fields",
          "reference_type": "direct_import"
        }
      ],
      "InferenceLogger": [
        {
          "test_id": "test_0057",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
          "class_name": "TestInferenceLogger",
          "method_name": "test_log_request",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0058",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
          "class_name": "TestInferenceLogger",
          "method_name": "test_get_logs",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0059",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
          "class_name": "TestInferenceLogger",
          "method_name": "test_get_log_not_found",
          "reference_type": "direct_import"
        }
      ],
      "inference_logging.logger": [
        {
          "test_id": "test_0057",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
          "class_name": "TestInferenceLogger",
          "method_name": "test_log_request",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0058",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
          "class_name": "TestInferenceLogger",
          "method_name": "test_get_logs",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0059",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\inference_logging\\test_logger.py",
          "class_name": "TestInferenceLogger",
          "method_name": "test_get_log_not_found",
          "reference_type": "direct_import"
        }
      ],
      "agent.langgraph_nodes": [
        {
          "test_id": "test_0060",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_initialization_workflow",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0061",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_invocation_workflow",
          "reference_type": "direct_import"
        }
      ],
      "get_available_tools": [
        {
          "test_id": "test_0060",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_initialization_workflow",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0061",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\integration\\test_agent_workflow.py",
          "class_name": "TestAgentWorkflow",
          "method_name": "test_agent_invocation_workflow",
          "reference_type": "direct_import"
        }
      ],
      "LLMFactory": [
        {
          "test_id": "test_0062",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_gemini",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0063",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_gemini_missing_key",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0064",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_ollama",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0065",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_invalid",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0066",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_get_available_providers",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0067",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_embedding_provider_gemini",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0068",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_embedding_provider_ollama",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0069",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_embedding_provider_invalid",
          "reference_type": "direct_import"
        }
      ],
      "llm.factory": [
        {
          "test_id": "test_0062",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_gemini",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0063",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_gemini_missing_key",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0064",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_ollama",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0065",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_invalid",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0066",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_get_available_providers",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0067",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_embedding_provider_gemini",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0068",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_embedding_provider_ollama",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0069",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_embedding_provider_invalid",
          "reference_type": "direct_import"
        }
      ],
      "llm.factory.GeminiClient": [
        {
          "test_id": "test_0062",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_gemini",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0063",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_gemini_missing_key",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0064",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_ollama",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0065",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_invalid",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0066",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_get_available_providers",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0067",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_embedding_provider_gemini",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0068",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_embedding_provider_ollama",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0069",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_embedding_provider_invalid",
          "reference_type": "string_ref"
        }
      ],
      "llm.factory.OllamaClient": [
        {
          "test_id": "test_0062",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_gemini",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0063",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_gemini_missing_key",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0064",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_ollama",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0065",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_provider_invalid",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0066",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_get_available_providers",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0067",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_embedding_provider_gemini",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0068",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_embedding_provider_ollama",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0069",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\llm\\test_factory.py",
          "class_name": "TestLLMFactory",
          "method_name": "test_create_embedding_provider_invalid",
          "reference_type": "string_ref"
        }
      ],
      "describe_table": [
        {
          "test_id": "test_0070",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
          "class_name": "TestCatalogServer",
          "method_name": "test_list_tables_tool",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0071",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
          "class_name": "TestCatalogServer",
          "method_name": "test_describe_table_tool",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0072",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
          "class_name": "TestCatalogServer",
          "method_name": "test_get_table_row_count_tool",
          "reference_type": "direct_import"
        }
      ],
      "get_table_row_count": [
        {
          "test_id": "test_0070",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
          "class_name": "TestCatalogServer",
          "method_name": "test_list_tables_tool",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0071",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
          "class_name": "TestCatalogServer",
          "method_name": "test_describe_table_tool",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0072",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
          "class_name": "TestCatalogServer",
          "method_name": "test_get_table_row_count_tool",
          "reference_type": "direct_import"
        }
      ],
      "list_tables": [
        {
          "test_id": "test_0070",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
          "class_name": "TestCatalogServer",
          "method_name": "test_list_tables_tool",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0071",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
          "class_name": "TestCatalogServer",
          "method_name": "test_describe_table_tool",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0072",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
          "class_name": "TestCatalogServer",
          "method_name": "test_get_table_row_count_tool",
          "reference_type": "direct_import"
        }
      ],
      "mcp_servers.catalog_server.tools": [
        {
          "test_id": "test_0070",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
          "class_name": "TestCatalogServer",
          "method_name": "test_list_tables_tool",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0071",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
          "class_name": "TestCatalogServer",
          "method_name": "test_describe_table_tool",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0072",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
          "class_name": "TestCatalogServer",
          "method_name": "test_get_table_row_count_tool",
          "reference_type": "direct_import"
        }
      ],
      "mcp_servers.catalog_server.tools.CatalogManager": [
        {
          "test_id": "test_0070",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
          "class_name": "TestCatalogServer",
          "method_name": "test_list_tables_tool",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0071",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
          "class_name": "TestCatalogServer",
          "method_name": "test_describe_table_tool",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0072",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mcp_servers\\test_catalog_server.py",
          "class_name": "TestCatalogServer",
          "method_name": "test_get_table_row_count_tool",
          "reference_type": "string_ref"
        }
      ],
      "get_tracker": [
        {
          "test_id": "test_0073",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
          "class_name": "TestMLflowTracking",
          "method_name": "test_get_tracker",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0074",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
          "class_name": "TestMLflowTracking",
          "method_name": "test_tracker_disabled_when_mlflow_not_available",
          "reference_type": "direct_import"
        }
      ],
      "mlflow.tracking": [
        {
          "test_id": "test_0073",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
          "class_name": "TestMLflowTracking",
          "method_name": "test_get_tracker",
          "reference_type": "direct_import"
        },
        {
          "test_id": "test_0074",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
          "class_name": "TestMLflowTracking",
          "method_name": "test_tracker_disabled_when_mlflow_not_available",
          "reference_type": "direct_import"
        }
      ],
      "mlflow.tracking.MlflowClient": [
        {
          "test_id": "test_0073",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
          "class_name": "TestMLflowTracking",
          "method_name": "test_get_tracker",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0074",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
          "class_name": "TestMLflowTracking",
          "method_name": "test_tracker_disabled_when_mlflow_not_available",
          "reference_type": "string_ref"
        }
      ],
      "mlflow.tracking.get_tracker": [
        {
          "test_id": "test_0073",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
          "class_name": "TestMLflowTracking",
          "method_name": "test_get_tracker",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0074",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
          "class_name": "TestMLflowTracking",
          "method_name": "test_tracker_disabled_when_mlflow_not_available",
          "reference_type": "string_ref"
        }
      ],
      "mlflow.tracking.mlflow": [
        {
          "test_id": "test_0073",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
          "class_name": "TestMLflowTracking",
          "method_name": "test_get_tracker",
          "reference_type": "string_ref"
        },
        {
          "test_id": "test_0074",
          "file_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository\\mlflow\\test_tracking.py",
          "class_name": "TestMLflowTracking",
          "method_name": "test_tracker_disabled_when_mlflow_not_available",
          "reference_type": "string_ref"
        }
      ]
    }
  }
}
```

`test_analysis/outputs/07_test_structure.json`

```json
{
  "generated_at": "2026-02-26T19:34:13.853881",
  "data": {
    "directory_structure": {
      "root_path": "C:\\Users\\SakethN.IDEYALABS\\OneDrive - IdeyaLabs\\Documents\\Planon\\sample_workflow\\test_repository",
      "directories": {
        "unit": {
          "file_count": 12,
          "total_lines": 1580
        },
        "integration": {
          "file_count": 2,
          "total_lines": 112
        }
      },
      "files_by_directory": {
        "unit": [
          {
            "path": "agent\\test_langgraph_agent.py",
            "name": "test_langgraph_agent.py",
            "line_count": 280
          },
          {
            "path": "agent\\test_langgraph_builder.py",
            "name": "test_langgraph_builder.py",
            "line_count": 98
          },
          {
            "path": "agent\\test_state_converter.py",
            "name": "test_state_converter.py",
            "line_count": 167
          },
          {
            "path": "agent\\test_tool_converter.py",
            "name": "test_tool_converter.py",
            "line_count": 218
          },
          {
            "path": "analytics\\test_aggregator.py",
            "name": "test_aggregator.py",
            "line_count": 59
          },
          {
            "path": "api\\test_routes.py",
            "name": "test_routes.py",
            "line_count": 180
          },
          {
            "path": "config\\test_settings.py",
            "name": "test_settings.py",
            "line_count": 86
          },
          {
            "path": "conftest.py",
            "name": "conftest.py",
            "line_count": 214
          },
          {
            "path": "inference_logging\\test_logger.py",
            "name": "test_logger.py",
            "line_count": 60
          },
          {
            "path": "llm\\test_factory.py",
            "name": "test_factory.py",
            "line_count": 120
          },
          {
            "path": "mcp_servers\\test_catalog_server.py",
            "name": "test_catalog_server.py",
            "line_count": 57
          },
          {
            "path": "mlflow\\test_tracking.py",
            "name": "test_tracking.py",
            "line_count": 41
          }
        ],
        "integration": [
          {
            "path": "integration\\__init__.py",
            "name": "__init__.py",
            "line_count": 1
          },
          {
            "path": "integration\\test_agent_workflow.py",
            "name": "test_agent_workflow.py",
            "line_count": 111
          }
        ]
      },
      "package_structure": {
        "agent": {
          "files": [
            {
              "name": "test_langgraph_agent.py",
              "path": "agent\\test_langgraph_agent.py",
              "category": "unit"
            },
            {
              "name": "test_langgraph_builder.py",
              "path": "agent\\test_langgraph_builder.py",
              "category": "unit"
            },
            {
              "name": "test_state_converter.py",
              "path": "agent\\test_state_converter.py",
              "category": "unit"
            },
            {
              "name": "test_tool_converter.py",
              "path": "agent\\test_tool_converter.py",
              "category": "unit"
            }
          ],
          "subdirectories": []
        },
        "analytics": {
          "files": [
            {
              "name": "test_aggregator.py",
              "path": "analytics\\test_aggregator.py",
              "category": "unit"
            }
          ],
          "subdirectories": []
        },
        "api": {
          "files": [
            {
              "name": "test_routes.py",
              "path": "api\\test_routes.py",
              "category": "unit"
            }
          ],
          "subdirectories": []
        },
        "config": {
          "files": [
            {
              "name": "test_settings.py",
              "path": "config\\test_settings.py",
              "category": "unit"
            }
          ],
          "subdirectories": []
        },
        "inference_logging": {
          "files": [
            {
              "name": "test_logger.py",
              "path": "inference_logging\\test_logger.py",
              "category": "unit"
            }
          ],
          "subdirectories": []
        },
        "integration": {
          "files": [
            {
              "name": "__init__.py",
              "path": "integration\\__init__.py",
              "category": "integration"
            },
            {
              "name": "test_agent_workflow.py",
              "path": "integration\\test_agent_workflow.py",
              "category": "integration"
            }
          ],
          "subdirectories": []
        },
        "llm": {
          "files": [
            {
              "name": "test_factory.py",
              "path": "llm\\test_factory.py",
              "category": "unit"
            }
          ],
          "subdirectories": []
        },
        "mcp_servers": {
          "files": [
            {
              "name": "test_catalog_server.py",
              "path": "mcp_servers\\test_catalog_server.py",
              "category": "unit"
            }
          ],
          "subdirectories": []
        },
        "mlflow": {
          "files": [
            {
              "name": "test_tracking.py",
              "path": "mlflow\\test_tracking.py",
              "category": "unit"
            }
          ],
          "subdirectories": []
        }
      }
    },
    "shared_utilities": {
      "conftest_files": [
        {
          "path": "conftest.py",
          "directory": "."
        }
      ],
      "fixture_directories": [
        {
          "path": "fixtures",
          "files": []
        }
      ],
      "shared_modules": []
    },
    "summary": {
      "total_directories": 9,
      "total_files": 14,
      "categories": [
        "unit",
        "integration"
      ]
    }
  }
}
```

`test_analysis/outputs/08_summary_report.json`

```json
{
  "generated_at": "2026-02-26T19:34:16.414578",
  "data": {
    "generated_at": "2026-02-26T19:34:16.402993",
    "test_repository_overview": {
      "total_test_files": 14,
      "total_lines_of_code": 1692,
      "test_framework": "pytest",
      "framework_confidence": "high"
    },
    "test_inventory": {
      "total_tests": 74,
      "total_test_classes": 12,
      "tests_by_type": {
        "unit": 72,
        "integration": 2
      }
    },
    "dependencies": {
      "total_production_classes_referenced": 65,
      "total_dependency_mappings": 575,
      "average_tests_per_class": 8.85,
      "tests_with_dependencies": 74
    },
    "metadata": {
      "tests_with_descriptions": 74,
      "tests_with_markers": 30,
      "async_tests": 30,
      "parameterized_tests": 42
    },
    "structure": {
      "test_categories": [
        "unit",
        "integration"
      ],
      "package_count": 9
    },
    "key_insights": [
      {
        "type": "coverage",
        "message": "Average of 1.1 tests per production class",
        "value": 1.1384615384615384
      },
      {
        "type": "framework",
        "message": "Using pytest framework",
        "confidence": "high"
      },
      {
        "type": "organization",
        "message": "Tests organized into 2 categories",
        "categories": [
          "unit",
          "integration"
        ]
      },
      {
        "type": "documentation",
        "message": "100.0% of tests have descriptions",
        "value": 100.0
      }
    ]
  }
}
```

`test_analysis/README.md`

```markdown
# Test Repository Analysis System

A step-by-step analysis tool for processing test repositories. Each step is implemented as a separate, runnable file with clear outputs.

## Overview

This system analyzes test repositories to extract:
- Test file inventory
- Framework detection
- Test registry (classes and methods)
- Static dependencies (test → production code)
- Test metadata (descriptions, markers, patterns)
- Reverse index (production code → tests)
- Test structure mapping
- Comprehensive summary report

## Project Structure

```
test_analysis/
├── __init__.py
├── 01_scan_test_files.py          # Step 1: Discover all test files
├── 02_detect_framework.py         # Step 2: Identify test framework
├── 03_build_test_registry.py       # Step 3: Create test inventory
├── 04_extract_static_dependencies.py  # Step 4: Extract imports/dependencies
├── 05_extract_test_metadata.py    # Step 5: Extract test metadata
├── 06_build_reverse_index.py      # Step 6: Create reverse index
├── 07_map_test_structure.py       # Step 7: Map directory structure
├── 08_generate_summary.py          # Step 8: Generate summary report
├── utils/
│   ├── __init__.py
│   ├── file_scanner.py            # File scanning utilities
│   ├── ast_parser.py              # AST parsing utilities
│   └── output_formatter.py        # Output formatting utilities
├── outputs/                        # Generated output files
│   ├── 01_test_files.json
│   ├── 02_framework_detection.json
│   ├── 03_test_registry.json
│   ├── 04_static_dependencies.json
│   ├── 05_test_metadata.json
│   ├── 06_reverse_index.json
│   ├── 07_test_structure.json
│   └── 08_summary_report.json
└── requirements.txt
```

## Usage

### Running Individual Steps

Each step can be run independently:

```bash
# Step 1: Scan test files
python test_analysis/01_scan_test_files.py

# Step 2: Detect framework
python test_analysis/02_detect_framework.py

# Step 3: Build test registry
python test_analysis/03_build_test_registry.py

# Step 4: Extract static dependencies
python test_analysis/04_extract_static_dependencies.py

# Step 5: Extract test metadata
python test_analysis/05_extract_test_metadata.py

# Step 6: Build reverse index
python test_analysis/06_build_reverse_index.py

# Step 7: Map test structure
python test_analysis/07_map_test_structure.py

# Step 8: Generate summary
python test_analysis/08_generate_summary.py
```

### Running All Steps

You can run all steps sequentially:

```bash
python test_analysis/01_scan_test_files.py
python test_analysis/02_detect_framework.py
python test_analysis/03_build_test_registry.py
python test_analysis/04_extract_static_dependencies.py
python test_analysis/05_extract_test_metadata.py
python test_analysis/06_build_reverse_index.py
python test_analysis/07_map_test_structure.py
python test_analysis/08_generate_summary.py
```

## Step Descriptions

### Step 1: Scan Test Files
- Recursively scans the test repository
- Identifies test files (test_*.py, *_test.py patterns)
- Extracts file metadata (size, line count)
- Categorizes by directory (unit, integration, e2e)
- **Output**: `01_test_files.json`

### Step 2: Detect Framework
- Checks for pytest.ini, setup.cfg, pyproject.toml
- Analyzes conftest.py
- Scans test files for framework patterns
- Determines primary framework (pytest/unittest)
- **Output**: `02_framework_detection.json`

### Step 3: Build Test Registry
- Parses each test file using AST
- Extracts test classes (class Test*)
- Extracts test methods (def test_*)
- Generates unique test IDs
- **Output**: `03_test_registry.json`

### Step 4: Extract Static Dependencies
- Parses imports from test files
- Filters out test framework imports
- Identifies production code references
- Builds test → production_code mapping
- **Output**: `04_static_dependencies.json`

### Step 5: Extract Test Metadata
- Extracts test names and descriptions
- Identifies pytest markers
- Detects async tests
- Identifies parameterized tests
- Analyzes test naming patterns
- **Output**: `05_test_metadata.json`

### Step 6: Build Reverse Index
- Inverts dependency mapping
- Creates production_code → [tests] index
- Groups by production module
- **Output**: `06_reverse_index.json`

### Step 7: Map Test Structure
- Analyzes directory hierarchy
- Maps package organization
- Identifies shared utilities (conftest.py, fixtures)
- **Output**: `07_test_structure.json`

### Step 8: Generate Summary
- Combines all previous outputs
- Generates statistics and insights
- Creates comprehensive report
- **Output**: `08_summary_report.json`

## Output Format

Each step produces:
1. **Console Output**: Human-readable progress and results
2. **JSON File**: Structured data saved to `outputs/` directory

### JSON Structure

All JSON files follow this structure:
```json
{
  "generated_at": "2024-01-01T12:00:00",
  "data": {
    // Step-specific data
  }
}
```

## Requirements

- Python 3.7+
- Standard library only (no external dependencies required)

Optional (for enhanced features):
- `ast-comments` for advanced docstring extraction

## Features

- **Beginner-Friendly**: Extensive comments explaining each step
- **Modular Design**: Each step is independent and can run separately
- **Clear Output**: Both console and JSON outputs for each step
- **Progress Indicators**: Shows progress for long-running operations
- **Error Handling**: Graceful error handling with clear messages
- **Data Validation**: Validates outputs before saving

## Example Output

### Step 1 Example:
```
==================================================
Step 1: Scanning Test Files
==================================================

  Scanning directory: test_repository
  Found test files: 15
  
  Summary:
    Total Files: 15
    Total Lines: 2809
    Unit Tests: 11
    Integration Tests: 2
    E2E Tests: 1
```

### Step 8 Example:
```
==================================================
ALL ANALYSIS STEPS COMPLETE!
==================================================

Summary:
  - 15 test files analyzed
  - 95 tests registered
  - 49 production classes mapped
  - Framework: pytest (high confidence)
```

## Next Steps

After completing all 8 steps, you have:
- Complete test inventory
- Test-to-code dependency mappings
- Reverse index for fast lookups
- Test metadata and characteristics
- Repository structure analysis

This foundation is ready for:
- Embedding generation (for semantic search)
- Coverage data integration
- Git history analysis
- Vector database population
- Test selection algorithms

## Troubleshooting

### Import Errors
If you encounter import errors, ensure you're running from the project root:
```bash
cd /path/to/project
python test_analysis/01_scan_test_files.py
```

### Missing Step Outputs
Some steps depend on previous steps:
- Step 3 needs Step 1 (or will scan directly)
- Step 4 needs Step 3
- Step 5 needs Step 3
- Step 6 needs Step 4
- Step 8 needs all previous steps

### File Not Found Errors
Ensure the test repository exists at:
```
/path/to/project/test_repository/
```

## Contributing

When adding new analysis steps:
1. Create a new numbered file (e.g., `09_new_analysis.py`)
2. Use utilities from `utils/` directory
3. Follow the output format (console + JSON)
4. Add extensive comments for beginners
5. Update this README

## License

This is part of the Test Impact Analysis platform project.

```

`test_analysis/requirements.txt`

```
# Requirements for Test Repository Analysis
# Minimal dependencies - most functionality uses Python standard library

# Enhanced AST parsing (optional, for better docstring extraction)
# ast-comments>=1.1.3  # Uncomment if needed for advanced parsing

# Note: This project primarily uses Python's built-in modules:
# - ast (Abstract Syntax Tree parsing)
# - pathlib (File path handling)
# - json (JSON file operations)
# - re (Regular expressions)
# - datetime (Timestamp generation)

# All other dependencies are part of Python standard library (3.7+)

```

`test_analysis/utils/ast_parser.py`

```python
"""
AST (Abstract Syntax Tree) parsing utilities.

This module provides functions to:
- Parse Python files into AST
- Extract imports and dependencies
- Extract class and method definitions
- Extract test-related information from AST nodes
"""

import ast
from pathlib import Path
from typing import List, Dict, Set, Optional, Any
import re
import time


def parse_file(filepath: Path, max_retries: int = 3, retry_delay: float = 0.5) -> Optional[ast.Module]:
    """
    Parse a Python file into an AST (Abstract Syntax Tree).
    
    Handles OneDrive file locking issues with retry logic.
    
    Args:
        filepath: Path to the Python file
        max_retries: Maximum number of retry attempts (default: 3)
        retry_delay: Initial delay between retries in seconds (default: 0.5)
    
    Returns:
        AST Module node, or None if parsing fails
    
    Example:
        tree = parse_file(Path("test_agent.py"))
        # Returns an ast.Module object if successful
    """
    # Try to read and parse with retries (for OneDrive file locking)
    for attempt in range(max_retries):
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            return ast.parse(content, filename=str(filepath))
        except PermissionError as e:
            if attempt < max_retries - 1:
                # Wait before retrying (exponential backoff)
                wait_time = retry_delay * (2 ** attempt)
                time.sleep(wait_time)
                continue
            else:
                # Final attempt failed - file is likely locked by OneDrive
                print(f"Warning: Permission denied after {max_retries} attempts: {filepath}")
                print(f"  This may be due to OneDrive syncing. Try closing OneDrive or waiting for sync to complete.")
                return None
        except SyntaxError as e:
            print(f"Warning: Could not parse {filepath}: {e}")
            return None
        except Exception as e:
            if attempt < max_retries - 1:
                # Wait before retrying for other errors too
                wait_time = retry_delay * (2 ** attempt)
                time.sleep(wait_time)
                continue
            else:
                print(f"Warning: Error reading {filepath} after {max_retries} attempts: {e}")
                return None
    
    return None


def extract_imports(tree: ast.Module) -> Dict[str, List[str]]:
    """
    Extract all import statements from an AST.
    
    Args:
        tree: AST Module node
    
    Returns:
        Dictionary with:
        - 'imports': List of module names imported (import X)
        - 'from_imports': List of (module, [names]) tuples (from X import Y)
        - 'all_imports': Combined list of all imported modules
    
    Example:
        # Parse a file with imports
        # tree = ast.parse("import os\\nfrom pathlib import Path")
        # imports = extract_imports(tree)
        # 'os' will be in imports['imports']
    """
    imports = []
    from_imports = []
    
    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            # Handle: import module
            for alias in node.names:
                imports.append(alias.name)
        
        elif isinstance(node, ast.ImportFrom):
            # Handle: from module import name
            if node.module:  # module can be None for relative imports
                imported_names = [alias.name for alias in node.names]
                from_imports.append((node.module, imported_names))
                imports.append(node.module)  # Also add the module itself
    
    # Combine all unique imports
    all_imports = list(set(imports))
    
    return {
        'imports': imports,
        'from_imports': from_imports,
        'all_imports': all_imports
    }


def extract_classes(tree: ast.Module) -> List[Dict[str, Any]]:
    """
    Extract all class definitions from an AST.
    
    Args:
        tree: AST Module node
    
    Returns:
        List of dictionaries with class information:
        - name: Class name
        - bases: List of base class names
        - methods: List of method names
        - line_number: Line where class is defined
    
    Example:
        # Parse a file with a class
        # tree = ast.parse("class TestAgent:\\n    def test_method(self): pass")
        # classes = extract_classes(tree)
        # classes[0]['name'] will be 'TestAgent'
    """
    classes = []
    
    for node in ast.walk(tree):
        if isinstance(node, ast.ClassDef):
            # Get base class names
            bases = []
            for base in node.bases:
                if isinstance(base, ast.Name):
                    bases.append(base.id)
                elif isinstance(base, ast.Attribute):
                    # Handle dotted names like unittest.TestCase
                    bases.append(_get_attr_name(base))
            
            # Get method names (including async methods)
            methods = []
            for item in node.body:
                if isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    methods.append(item.name)
            
            classes.append({
                'name': node.name,
                'bases': bases,
                'methods': methods,
                'line_number': node.lineno
            })
    
    return classes


def extract_functions(tree: ast.Module) -> List[Dict[str, Any]]:
    """
    Extract all function definitions from an AST.
    
    Args:
        tree: AST Module node
    
    Returns:
        List of dictionaries with function information:
        - name: Function name
        - is_async: Whether function is async
        - parameters: List of parameter names
        - line_number: Line where function is defined
        - decorators: List of decorator names
    
    Example:
        # Parse a file with a decorated function
        # tree = ast.parse("@pytest.mark.asyncio\\ndef test_func(): pass")
        # funcs = extract_functions(tree)
        # funcs[0]['name'] will be 'test_func'
    """
    functions = []
    
    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef) or isinstance(node, ast.AsyncFunctionDef):
            is_async = isinstance(node, ast.AsyncFunctionDef)
            
            # Get parameter names
            parameters = [arg.arg for arg in node.args.args]
            
            # Get decorator names
            decorators = []
            for decorator in node.decorator_list:
                if isinstance(decorator, ast.Name):
                    decorators.append(decorator.id)
                elif isinstance(decorator, ast.Attribute):
                    decorators.append(_get_attr_name(decorator))
                elif isinstance(decorator, ast.Call):
                    # Handle @pytest.mark.asyncio() style
                    if isinstance(decorator.func, ast.Attribute):
                        decorators.append(_get_attr_name(decorator.func))
            
            functions.append({
                'name': node.name,
                'is_async': is_async,
                'parameters': parameters,
                'line_number': node.lineno,
                'decorators': decorators
            })
    
    return functions


def extract_test_classes(tree: ast.Module) -> List[Dict[str, Any]]:
    """
    Extract test classes (classes that start with 'Test' or inherit from TestCase).
    
    Args:
        tree: AST Module node
    
    Returns:
        List of test class dictionaries (same format as extract_classes)
    
    Example:
        # Parse a file with a test class
        # tree = ast.parse("class TestAgent:\\n    def test_method(self): pass")
        # test_classes = extract_test_classes(tree)
        # len(test_classes) will be 1
    """
    all_classes = extract_classes(tree)
    
    # Filter for test classes
    test_classes = []
    for cls in all_classes:
        # Check if class name starts with 'Test'
        if cls['name'].startswith('Test'):
            test_classes.append(cls)
        # Check if it inherits from TestCase or similar
        elif any('Test' in base for base in cls['bases']):
            test_classes.append(cls)
    
    return test_classes


def extract_test_methods(tree: ast.Module) -> List[Dict[str, Any]]:
    """
    Extract test methods (methods that start with 'test_').
    
    Args:
        tree: AST Module node
    
    Returns:
        List of test method dictionaries (same format as extract_functions)
    
    Example:
        # Parse a file with a test function
        # tree = ast.parse("def test_agent(): pass")
        # test_methods = extract_test_methods(tree)
        # len(test_methods) will be 1
    """
    all_functions = extract_functions(tree)
    
    # Filter for test methods
    test_methods = []
    for func in all_functions:
        if func['name'].startswith('test_'):
            test_methods.append(func)
    
    return test_methods


def extract_docstrings(tree: ast.Module) -> Dict[str, str]:
    """
    Extract docstrings from module, classes, and functions.
    
    Args:
        tree: AST Module node
    
    Returns:
        Dictionary mapping names to docstrings:
        - 'module': Module-level docstring
        - 'classes': Dict of class_name -> docstring
        - 'functions': Dict of function_name -> docstring
    
    Example:
        # Parse a file with a class that has a docstring
        # tree = ast.parse('class Test: pass')  # Class with docstring
        # docs = extract_docstrings(tree)
        # docs['classes']['Test'] will contain the docstring if present
    """
    result = {
        'module': ast.get_docstring(tree),
        'classes': {},
        'functions': {}
    }
    
    for node in ast.walk(tree):
        if isinstance(node, ast.ClassDef):
            docstring = ast.get_docstring(node)
            if docstring:
                result['classes'][node.name] = docstring
        
        elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            docstring = ast.get_docstring(node)
            if docstring:
                result['functions'][node.name] = docstring
    
    return result


def _get_attr_name(node: ast.Attribute) -> str:
    """
    Helper function to get full attribute name (e.g., 'pytest.mark.asyncio').
    
    Args:
        node: AST Attribute node
    
    Returns:
        Full attribute name as string
    """
    parts = []
    current = node
    
    while isinstance(current, ast.Attribute):
        parts.append(current.attr)
        current = current.value
    
    if isinstance(current, ast.Name):
        parts.append(current.id)
    
    return '.'.join(reversed(parts))


def extract_string_references(tree: ast.Module) -> List[str]:
    """
    Extract string-based references from function calls like:
    - patch('agent.agent_pool.LangGraphAgent')
    - mock.patch('module.Class')
    - @patch('module.function')
    - unittest.mock.patch('module.method')
    
    This is important because many tests use string-based references
    in patch() calls that aren't captured by regular import analysis.
    
    Args:
        tree: AST Module node
    
    Returns:
        List of module/class names found in string literals
    
    Example:
        # Code: @patch('agent.agent_pool.LangGraphAgent')
        # tree = ast.parse("...")
        # refs = extract_string_references(tree)
        # refs will contain 'agent.agent_pool.LangGraphAgent'
    """
    string_refs = []
    
    def is_patch_function(node) -> bool:
        """Check if a node represents a patch/mock function call."""
        if isinstance(node, ast.Name):
            return node.id in ('patch', 'Mock', 'MagicMock', 'PropertyMock', 'AsyncMock')
        elif isinstance(node, ast.Attribute):
            # Handle mock.patch, unittest.mock.patch, etc.
            return node.attr in ('patch', 'Mock', 'MagicMock', 'PropertyMock', 'AsyncMock')
        return False
    
    def extract_string_from_node(node) -> Optional[str]:
        """Extract string value from various AST node types."""
        if isinstance(node, ast.Constant):
            if isinstance(node.value, str):
                return node.value
        elif isinstance(node, ast.Str):  # Python < 3.8 compatibility
            return node.s
        return None
    
    for node in ast.walk(tree):
        # Check function calls (patch('module.Class'))
        if isinstance(node, ast.Call):
            if is_patch_function(node.func):
                # Extract string arguments
                for arg in node.args:
                    ref = extract_string_from_node(arg)
                    if ref and '.' in ref and not ref.startswith('http'):
                        # Filter out URLs and non-module strings
                        # Module paths typically have dots and don't start with http
                        if not ref.startswith('/') and not ref.startswith('\\'):
                            string_refs.append(ref)
        
        # Check decorators with string arguments (@patch('module.Class'))
        if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):
            for decorator in node.decorator_list:
                if isinstance(decorator, ast.Call):
                    if is_patch_function(decorator.func):
                        for arg in decorator.args:
                            ref = extract_string_from_node(arg)
                            if ref and '.' in ref and not ref.startswith('http'):
                                if not ref.startswith('/') and not ref.startswith('\\'):
                                    string_refs.append(ref)
    
    # Remove duplicates and return
    return sorted(list(set(string_refs)))


def extract_function_calls(tree: ast.Module) -> List[Dict[str, Any]]:
    """
    Extract all function calls made inside each test method.
    
    For each test method, finds:
    - What functions it directly calls (e.g., initialize())
    - What methods it calls on objects (e.g., agent.initialize())
    - Filters out test framework calls (assert, patch, Mock, etc.)
    
    Args:
        tree: AST Module node
    
    Returns:
        List of dictionaries, one per test method:
        - test_method: Name of the test method
        - calls: List of call dictionaries with:
            - function: Function/method name
            - object: Object name (if method call, e.g., 'agent')
            - type: 'direct' or 'method'
            - line_number: Line where call occurs
    
    Example:
        # Code: 
        # def test_agent_initialization(self, agent):
        #     await agent.initialize()
        #     assert agent._initialized is True
        # 
        # tree = ast.parse("...")
        # calls = extract_function_calls(tree)
        # calls[0]['calls'] will contain {'function': 'initialize', 'object': 'agent', 'type': 'method'}
    """
    test_function_calls = []
    
    # Test framework functions to exclude
    TEST_FRAMEWORK_FUNCTIONS = {
        'assert', 'assertEqual', 'assertNotEqual', 'assertTrue', 'assertFalse',
        'assertIn', 'assertNotIn', 'assertIs', 'assertIsNot', 'assertIsNone',
        'assertIsNotNone', 'assertRaises', 'assertRaisesRegex',
        'patch', 'Mock', 'MagicMock', 'AsyncMock', 'PropertyMock',
        'pytest', 'fixture', 'mark', 'raises', 'parametrize',
        'setUp', 'tearDown', 'setUpClass', 'tearDownClass'
    }
    
    def get_object_name(node) -> Optional[str]:
        """Extract object name from an AST node."""
        if isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, ast.Attribute):
            # For chained attributes like mock_client.initialize, get the base
            current = node
            while isinstance(current, ast.Attribute):
                current = current.value
            if isinstance(current, ast.Name):
                return current.id
        return None
    
    # Walk through all nodes to find test methods
    for node in ast.walk(tree):
        # Only look at test methods (functions starting with 'test_')
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            if not node.name.startswith('test_'):
                continue
            
            calls = []
            
            # Walk inside this specific test method to find calls
            for child in ast.walk(node):
                if isinstance(child, ast.Call):
                    func_name = None
                    object_name = None
                    call_type = 'direct'
                    
                    # Extract function name and object
                    if isinstance(child.func, ast.Name):
                        # Direct call: initialize()
                        func_name = child.func.id
                        call_type = 'direct'
                    elif isinstance(child.func, ast.Attribute):
                        # Method call: agent.initialize()
                        func_name = child.func.attr
                        object_name = get_object_name(child.func.value)
                        call_type = 'method'
                    
                    # Skip test framework calls
                    if func_name and func_name not in TEST_FRAMEWORK_FUNCTIONS:
                        # Also skip if it's a method on a mock object (common pattern)
                        if object_name and ('mock' in object_name.lower() or 'Mock' in object_name):
                            # Still include it, but mark it
                            pass
                        
                        calls.append({
                            'function': func_name,
                            'object': object_name,
                            'type': call_type,
                            'line_number': child.lineno
                        })
            
            if calls:
                test_function_calls.append({
                    'test_method': node.name,
                    'calls': calls
                })
    
    return test_function_calls
```

`test_analysis/utils/file_scanner.py`

```python
"""
File scanning utilities for discovering test files.

This module provides functions to:
- Recursively scan directories for test files
- Match test file patterns (test_*.py, *_test.py)
- Extract file metadata (size, line count)
- Categorize files by directory structure
"""

from pathlib import Path
from typing import List, Dict, Optional
import re


# Common test file patterns
TEST_FILE_PATTERNS = [
    r'test_.*\.py$',      # test_*.py
    r'.*_test\.py$',      # *_test.py
    r'.*Test\.py$',       # *Test.py (Java-style, but sometimes used in Python)
]


def is_test_file(filepath: Path) -> bool:
    """
    Check if a file matches test file patterns.
    
    Args:
        filepath: Path to the file to check
    
    Returns:
        True if the file matches test patterns, False otherwise
    
    Example:
        >>> is_test_file(Path("test_agent.py"))
        True
        >>> is_test_file(Path("agent.py"))
        False
    """
    filename = filepath.name
    
    # Check against all test patterns
    for pattern in TEST_FILE_PATTERNS:
        if re.match(pattern, filename, re.IGNORECASE):
            return True
    
    return False


def scan_directory(root_dir: Path, exclude_dirs: Optional[List[str]] = None) -> List[Path]:
    """
    Recursively scan a directory for test files.
    
    Enhanced version that finds ALL test files using multiple strategies.
    
    Args:
        root_dir: Root directory to scan
        exclude_dirs: List of directory names to exclude (e.g., ['__pycache__', '.git'])
    
    Returns:
        List of Path objects for all test files found
    
    Example:
        >>> files = scan_directory(Path("test_repository"))
        >>> len(files)
        15
    """
    if exclude_dirs is None:
        exclude_dirs = ['__pycache__', '.git', '.pytest_cache', 'node_modules', '.venv', 'venv', 'env', '.env']
    
    test_files = []
    seen_files = set()  # Track files to avoid duplicates
    
    # Strategy 1: Standard test file patterns (test_*.py, *_test.py)
    for item in root_dir.rglob('*.py'):
        # Skip excluded directories
        if any(excluded in item.parts for excluded in exclude_dirs):
            continue
        
        # Skip .pyc files
        if item.suffix == '.pyc':
            continue
        
        # Check if it's a test file
        if is_test_file(item):
            file_str = str(item.resolve())
            if file_str not in seen_files:
                seen_files.add(file_str)
                test_files.append(item)
    
    # Strategy 2: Check common test directories explicitly (even if not matching patterns)
    common_test_dirs = ['unit', 'integration', 'e2e', 'tests', 'test', 'end_to_end', 'endtoend']
    for dir_name in common_test_dirs:
        test_dir = root_dir / dir_name
        if test_dir.exists() and test_dir.is_dir():
            for item in test_dir.rglob('*.py'):
                # Skip excluded directories
                if any(excluded in item.parts for excluded in exclude_dirs):
                    continue
                
                # Skip .pyc files
                if item.suffix == '.pyc':
                    continue
                
                file_str = str(item.resolve())
                if file_str not in seen_files:
                    seen_files.add(file_str)
                    test_files.append(item)
    
    # Strategy 3: Look for files in test/test directories even if they don't match patterns
    # This catches files that might be tests but don't follow naming conventions
    for item in root_dir.rglob('*.py'):
        if any(excluded in item.parts for excluded in exclude_dirs):
            continue
        
        if item.suffix == '.pyc':
            continue
        
        # If file is in a test directory, include it even if name doesn't match pattern
        path_str = str(item).lower()
        if any(test_dir in path_str for test_dir in ['/test/', '/tests/', '\\test\\', '\\tests\\']):
            file_str = str(item.resolve())
            if file_str not in seen_files:
                seen_files.add(file_str)
                test_files.append(item)
    
    return sorted(test_files)  # Return sorted list for consistency


def scan_directory_comprehensive(root_dir: Path, exclude_dirs: Optional[List[str]] = None) -> List[Path]:
    """
    Comprehensive test file scanner that finds ALL test files.
    
    This is an alias for scan_directory (which is now enhanced).
    Kept for backward compatibility and clarity.
    
    Args:
        root_dir: Root directory to scan
        exclude_dirs: List of directory names to exclude
    
    Returns:
        List of Path objects for all test files found
    """
    return scan_directory(root_dir, exclude_dirs)


def get_file_metadata(filepath: Path) -> Dict[str, any]:
    """
    Extract metadata from a file.
    
    Args:
        filepath: Path to the file
    
    Returns:
        Dictionary with file metadata:
        - path: Relative path as string
        - absolute_path: Absolute path as string
        - size_bytes: File size in bytes
        - line_count: Number of lines in file
        - directory: Directory name (e.g., 'unit', 'integration')
    
    Example:
        >>> metadata = get_file_metadata(Path("test_repository/unit/test_agent.py"))
        >>> metadata['line_count']
        127
    """
    try:
        # Read file to count lines
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
            line_count = len(lines)
        
        # Get file size
        size_bytes = filepath.stat().st_size
        
        # Determine directory category
        directory = _categorize_directory(filepath)
        
        return {
            "path": str(filepath),
            "absolute_path": str(filepath.absolute()),
            "size_bytes": size_bytes,
            "line_count": line_count,
            "directory": directory
        }
    except Exception as e:
        # Return minimal metadata if file can't be read
        return {
            "path": str(filepath),
            "absolute_path": str(filepath.absolute()),
            "size_bytes": 0,
            "line_count": 0,
            "directory": "unknown",
            "error": str(e)
        }


def _categorize_directory(filepath: Path) -> str:
    """
    Categorize a test file by its directory structure.
    
    Enhanced version with better detection for integration/e2e tests.
    
    Args:
        filepath: Path to the test file
    
    Returns:
        Category string: 'unit', 'integration', 'e2e', or 'other'
    
    Example:
        >>> _categorize_directory(Path("test_repository/unit/test_agent.py"))
        'unit'
    """
    parts = filepath.parts
    path_str = str(filepath).lower()
    
    # Check for e2e/end-to-end first (most specific)
    if 'e2e' in path_str or 'end_to_end' in path_str or 'endtoend' in path_str or 'end-to-end' in path_str:
        return 'e2e'
    
    # Check for integration
    if 'integration' in path_str:
        return 'integration'
    
    # Check for unit
    if 'unit' in path_str:
        return 'unit'
    
    # Check parent directory name
    if len(parts) > 1:
        parent_dir = parts[-2].lower()
        if parent_dir in ['e2e', 'end_to_end', 'endtoend', 'end-to-end']:
            return 'e2e'
        elif parent_dir == 'integration':
            return 'integration'
        elif parent_dir == 'unit':
            return 'unit'
    
    # Default to unit if in test_repository or tests directory
    if 'test_repository' in path_str or 'tests' in path_str:
        return 'unit'  # Default fallback
    
    return 'other'


def group_files_by_category(files: List[Path]) -> Dict[str, List[Path]]:
    """
    Group test files by their category (unit, integration, e2e, other).
    
    Args:
        files: List of test file paths
    
    Returns:
        Dictionary mapping category to list of files
    
    Example:
        >>> files = [Path("test_repository/unit/test_a.py"), Path("test_repository/integration/test_b.py")]
        >>> grouped = group_files_by_category(files)
        >>> len(grouped['unit'])
        1
    """
    grouped = {
        'unit': [],
        'integration': [],
        'e2e': [],
        'other': []
    }
    
    for filepath in files:
        category = _categorize_directory(filepath)
        grouped[category].append(filepath)
    
    return grouped

```

`test_analysis/utils/output_formatter.py`

```python
"""
Output formatting utilities for console and JSON output.

This module provides functions to:
- Format console output with clear headers and sections
- Save data to JSON files with proper formatting
- Display progress indicators
- Print structured data in a readable format
"""

import json
from pathlib import Path
from typing import Any, Dict, List
from datetime import datetime


def print_header(title: str, width: int = 50) -> None:
    """
    Print a formatted header for console output.
    
    Args:
        title: The title text to display
        width: Width of the header line (default: 50)
    
    Example:
        >>> print_header("Step 1: Scanning Test Files")
        ==================================================
        Step 1: Scanning Test Files
        ==================================================
    """
    print("=" * width)
    print(title)
    print("=" * width)


def print_section(title: str, indent: int = 2) -> None:
    """
    Print a section header with indentation.
    
    Args:
        title: The section title
        indent: Number of spaces to indent (default: 2)
    
    Example:
        >>> print_section("Found Test Files:")
          Found Test Files:
    """
    print(" " * indent + title)


def print_item(label: str, value: Any, indent: int = 4) -> None:
    """
    Print a labeled item with indentation.
    
    Args:
        label: The label text
        value: The value to display
        indent: Number of spaces to indent (default: 4)
    
    Example:
        >>> print_item("Total files:", 15)
            Total files: 15
    """
    print(" " * indent + f"{label} {value}")


def print_list(items: List[Any], label: str = "", max_items: int = 10, indent: int = 4) -> None:
    """
    Print a list of items, limiting the number shown.
    
    Args:
        items: List of items to print
        label: Optional label for the list
        max_items: Maximum number of items to show (default: 10)
        indent: Number of spaces to indent (default: 4)
    
    Example:
        >>> print_list(["file1.py", "file2.py"], "Files:", max_items=5)
            Files:
              - file1.py
              - file2.py
    """
    if label:
        print(" " * indent + label)
    
    for i, item in enumerate(items[:max_items]):
        print(" " * (indent + 2) + f"- {item}")
    
    if len(items) > max_items:
        remaining = len(items) - max_items
        print(" " * (indent + 2) + f"... and {remaining} more")


def save_json(data: Dict[str, Any], filepath: Path, pretty: bool = True) -> None:
    """
    Save data to a JSON file with optional pretty printing.
    
    Args:
        data: Dictionary to save as JSON
        filepath: Path where to save the file
        pretty: Whether to format JSON with indentation (default: True)
    
    Example:
        >>> data = {"total": 10, "items": [1, 2, 3]}
        >>> save_json(data, Path("output.json"))
        # Creates output.json with formatted JSON
    """
    # Ensure the directory exists
    filepath.parent.mkdir(parents=True, exist_ok=True)
    
    # Add metadata
    output_data = {
        "generated_at": datetime.now().isoformat(),
        "data": data
    }
    
    # Write JSON file
    with open(filepath, 'w', encoding='utf-8') as f:
        if pretty:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        else:
            json.dump(output_data, f, ensure_ascii=False)
    
    print(f"Saved to: {filepath}")


def print_summary(stats: Dict[str, Any], indent: int = 2) -> None:
    """
    Print a summary of statistics in a formatted way.
    
    Args:
        stats: Dictionary of statistics to display
        indent: Number of spaces to indent (default: 2)
    
    Example:
        >>> stats = {"total_files": 15, "total_tests": 50}
        >>> print_summary(stats)
          Summary:
            Total files: 15
            Total tests: 50
    """
    print(" " * indent + "Summary:")
    for key, value in stats.items():
        # Convert key from snake_case to Title Case
        label = key.replace('_', ' ').title()
        print(" " * (indent + 2) + f"{label}: {value}")


def print_progress(current: int, total: int, item_name: str = "items") -> None:
    """
    Print a progress indicator.
    
    Args:
        current: Current item number
        total: Total number of items
        item_name: Name of the items being processed (default: "items")
    
    Example:
        >>> print_progress(5, 10, "files")
        Processing: 5/10 files (50%)
    """
    percentage = (current / total * 100) if total > 0 else 0
    print(f"Processing: {current}/{total} {item_name} ({percentage:.1f}%)", end='\r')
    
    # Print newline when complete
    if current == total:
        print()  # Move to next line

```

`test_analysis/utils/__init__.py`

```python
"""
Utility modules for test repository analysis.
"""

```

`test_analysis/__init__.py`

```python
"""
Test Repository Analysis Package

This package contains step-by-step analysis tools for processing test repositories.
Each module is designed to be run independently and produces both console and JSON outputs.
"""

__version__ = "1.0.0"

```

`test_repository/agent/test_langgraph_agent.py`

```python
"""Tests for LangGraph agent implementation."""

import pytest
import asyncio
from unittest.mock import Mock, AsyncMock, MagicMock, patch, call
from typing import Dict, Any

# Import agent modules
import sys
from pathlib import Path
backend_path = Path(__file__).parent.parent.parent / "backend"
sys.path.insert(0, str(backend_path))

from agent.langgraph_agent import LangGraphAgent
from agent.langgraph_state import LangGraphAgentState, create_langgraph_initial_state
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage


class TestLangGraphAgent:
    """Test suite for LangGraphAgent."""
    
    @pytest.fixture
    def agent(self):
        """Create a LangGraphAgent instance."""
        return LangGraphAgent()
    
    @pytest.mark.asyncio
    async def test_agent_initialization(self, agent, mock_mcp_client):
        """Test agent initialization."""
        with patch('agent.langgraph_agent.MCPSDKClient', return_value=mock_mcp_client):
            with patch('agent.langgraph_agent.convert_mcp_tools_to_langchain', return_value=[]):
                with patch('agent.langgraph_agent.load_system_prompt', return_value="System prompt"):
                    with patch('agent.langgraph_agent.LangGraphAgentBuilder') as mock_builder:
                        mock_graph = MagicMock()
                        mock_builder_instance = MagicMock()
                        mock_builder_instance.build.return_value = mock_graph
                        mock_builder.return_value = mock_builder_instance
                        
                        await agent.initialize()
                        
                        assert agent._initialized is True
                        assert agent.graph is not None
                        mock_mcp_client.initialize.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_agent_initialization_failure_no_servers(self, agent):
        """Test agent initialization failure when MCP servers are not available."""
        with patch('agent.langgraph_agent.MCPSDKClient') as mock_client_class:
            mock_client = MagicMock()
            mock_client.initialize = AsyncMock()
            mock_client.discover_all_tools = AsyncMock(side_effect=RuntimeError("No servers available"))
            mock_client_class.return_value = mock_client
            
            with pytest.raises(RuntimeError, match="Agent initialization failed"):
                await agent.initialize()
    
    @pytest.mark.asyncio
    async def test_agent_initialization_no_tools(self, agent, mock_mcp_client):
        """Test agent initialization when no tools are available."""
        with patch('agent.langgraph_agent.MCPSDKClient', return_value=mock_mcp_client):
            with patch('agent.langgraph_agent.convert_mcp_tools_to_langchain', return_value=[]):
                with patch('agent.langgraph_agent.load_system_prompt', return_value="System prompt"):
                    with patch('agent.langgraph_agent.LangGraphAgentBuilder') as mock_builder:
                        mock_graph = MagicMock()
                        mock_builder_instance = MagicMock()
                        mock_builder_instance.build.return_value = mock_graph
                        mock_builder.return_value = mock_builder_instance
                        
                        await agent.initialize()
                        
                        # Should still initialize even with no tools
                        assert agent._initialized is True
    
    @pytest.mark.asyncio
    async def test_agent_invoke(self, agent, mock_mcp_client):
        """Test agent invocation."""
        # Setup mocks
        mock_graph = MagicMock()
        mock_final_state = {
            "messages": [AIMessage(content="Test response")],
            "request_id": "test_123",
            "session_id": None,
            "tool_calls": [],
            "tool_results": [],
            "current_step": 1,
            "finished": True,
            "error": None,
            "prompt_version": "v1",
            "model_name": "gemini-2.5-flash"
        }
        mock_graph.ainvoke = AsyncMock(return_value=mock_final_state)
        agent.graph = mock_graph
        agent._initialized = True
        agent._has_checkpointer = False
        
        with patch('agent.langgraph_agent.create_langgraph_initial_state', return_value=mock_final_state):
            with patch('agent.langgraph_agent.convert_langgraph_state_to_agent', return_value=mock_final_state):
                with patch('agent.langgraph_agent.load_system_prompt', return_value="System prompt"):
                    with patch('agent.langgraph_nodes.get_available_tools', return_value=[]):
                        result = await agent.invoke(
                            user_message="Test message",
                            request_id="test_123"
                        )
                        
                        assert result is not None
                        assert "messages" in result
                        mock_graph.ainvoke.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_agent_invoke_with_session_id(self, agent, mock_mcp_client):
        """Test agent invocation with session ID."""
        mock_graph = MagicMock()
        mock_final_state = {
            "messages": [AIMessage(content="Test response")],
            "request_id": "test_123",
            "session_id": "session_123",
            "tool_calls": [],
            "tool_results": [],
            "current_step": 1,
            "finished": True,
            "error": None,
            "prompt_version": "v1",
            "model_name": "gemini-2.5-flash"
        }
        mock_graph.ainvoke = AsyncMock(return_value=mock_final_state)
        agent.graph = mock_graph
        agent._initialized = True
        agent._has_checkpointer = False
        
        with patch('agent.langgraph_agent.create_langgraph_initial_state', return_value=mock_final_state):
            with patch('agent.langgraph_agent.convert_langgraph_state_to_agent', return_value=mock_final_state):
                with patch('agent.langgraph_agent.load_system_prompt', return_value="System prompt"):
                    with patch('agent.langgraph_nodes.get_available_tools', return_value=[]):
                        result = await agent.invoke(
                            user_message="Test message",
                            request_id="test_123",
                            session_id="session_123"
                        )
                        
                        assert result is not None
                        assert result.get("session_id") == "session_123"
    
    @pytest.mark.asyncio
    async def test_agent_stream_invoke(self, agent, mock_mcp_client):
        """Test agent streaming invocation."""
        mock_graph = MagicMock()
        
        # Create mock stream events
        async def mock_stream(*args, **kwargs):
            yield {"agent": {"messages": [AIMessage(content="Thinking...")]}}
            yield {"tools": {"messages": [ToolMessage(content="Tool result", tool_call_id="tc_1")]}}
            yield {"agent": {
                "messages": [AIMessage(content="Final response")],
                "request_id": "test_123",
                "finished": True
            }}
        
        mock_graph.astream = mock_stream
        agent.graph = mock_graph
        agent._initialized = True
        agent._has_checkpointer = False
        
        with patch('agent.langgraph_agent.create_langgraph_initial_state') as mock_init_state:
            mock_init_state.return_value = {
                "messages": [HumanMessage(content="Test")],
                "request_id": "test_123"
            }
            with patch('agent.langgraph_agent.load_system_prompt', return_value="System prompt"):
                with patch('agent.langgraph_nodes.get_available_tools', return_value=[]):
                    stages = []
                    async for stage in agent.stream_invoke(
                        user_message="Test message",
                        request_id="test_123"
                    ):
                        stages.append(stage)
                    
                    assert len(stages) > 0
                    assert any(s.get("stage") == "completed" for s in stages)
    
    @pytest.mark.asyncio
    async def test_agent_stream_invoke_error(self, agent, mock_mcp_client):
        """Test agent streaming invocation with error."""
        mock_graph = MagicMock()
        mock_graph.astream = AsyncMock(side_effect=Exception("Test error"))
        agent.graph = mock_graph
        agent._initialized = True
        agent._has_checkpointer = False
        
        with patch('agent.langgraph_agent.create_langgraph_initial_state') as mock_init_state:
            mock_init_state.return_value = {
                "messages": [HumanMessage(content="Test")],
                "request_id": "test_123"
            }
            with patch('agent.langgraph_agent.load_system_prompt', return_value="System prompt"):
                with patch('agent.langgraph_nodes.get_available_tools', return_value=[]):
                    stages = []
                    async for stage in agent.stream_invoke(
                        user_message="Test message",
                        request_id="test_123"
                    ):
                        stages.append(stage)
                    
                    # Should yield error stage
                    assert any(s.get("stage") == "error" for s in stages)
    
    @pytest.mark.asyncio
    async def test_agent_close(self, agent, mock_mcp_client):
        """Test agent cleanup."""
        agent.mcp_client = mock_mcp_client
        agent._initialized = True
        
        await agent.close()
        
        mock_mcp_client.close.assert_called_once()
        assert agent._initialized is False
    
    @pytest.mark.asyncio
    async def test_agent_context_manager(self, agent, mock_mcp_client):
        """Test agent as async context manager."""
        with patch('agent.langgraph_agent.MCPSDKClient', return_value=mock_mcp_client):
            with patch('agent.langgraph_agent.convert_mcp_tools_to_langchain', return_value=[]):
                with patch('agent.langgraph_agent.load_system_prompt', return_value="System prompt"):
                    with patch('agent.langgraph_agent.LangGraphAgentBuilder') as mock_builder:
                        mock_graph = MagicMock()
                        mock_builder_instance = MagicMock()
                        mock_builder_instance.build.return_value = mock_graph
                        mock_builder.return_value = mock_builder_instance
                        
                        async with agent:
                            assert agent._initialized is True
                        
                        mock_mcp_client.close.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_agent_extract_stage_info_agent_thinking(self, agent):
        """Test stage info extraction for agent thinking."""
        state = {
            "messages": [AIMessage(content="Thinking...")]
        }
        
        stage_info = agent._extract_stage_info("agent", state, 1)
        
        assert stage_info is not None
        assert stage_info["stage"] == "agent_thinking"
    
    @pytest.mark.asyncio
    async def test_agent_extract_stage_info_tool_executing(self, agent):
        """Test stage info extraction for tool execution."""
        from langchain_core.messages import AIMessage
        
        mock_tool_call = MagicMock()
        mock_tool_call.name = "test_tool"
        
        ai_message = AIMessage(content="", tool_calls=[mock_tool_call])
        state = {
            "messages": [ai_message]
        }
        
        stage_info = agent._extract_stage_info("agent", state, 1)
        
        assert stage_info is not None
        assert stage_info["stage"] == "tool_executing"
        assert "test_tool" in stage_info["data"]["tools"]
    
    @pytest.mark.asyncio
    async def test_agent_extract_stage_info_tool_completed(self, agent):
        """Test stage info extraction for tool completion."""
        from langchain_core.messages import AIMessage, ToolMessage
        
        ai_message = AIMessage(content="", tool_calls=[MagicMock(name="test_tool")])
        tool_message = ToolMessage(content="Result", tool_call_id="tc_1")
        
        state = {
            "messages": [ai_message, tool_message]
        }
        
        stage_info = agent._extract_stage_info("tools", state, 1)
        
        assert stage_info is not None
        assert stage_info["stage"] == "tool_completed"

```

`test_repository/agent/test_langgraph_builder.py`

```python
"""Tests for LangGraph builder module."""

import pytest
from unittest.mock import Mock, MagicMock, patch
import sys
from pathlib import Path

backend_path = Path(__file__).parent.parent.parent / "backend"
sys.path.insert(0, str(backend_path))

from agent.langgraph_builder import LangGraphAgentBuilder


class TestLangGraphBuilder:
    """Test suite for LangGraphAgentBuilder."""
    
    @pytest.fixture
    def mock_tools(self):
        """Create mock tools."""
        from unittest.mock import MagicMock
        tool1 = MagicMock()
        tool1.name = "tool1"
        tool2 = MagicMock()
        tool2.name = "tool2"
        return [tool1, tool2]
    
    def test_builder_initialization(self, mock_tools):
        """Test builder initialization."""
        builder = LangGraphAgentBuilder(tools=mock_tools)
        assert builder.tools == mock_tools
        assert builder._graph is None
    
    def test_builder_initialization_no_tools(self):
        """Test builder initialization with no tools."""
        builder = LangGraphAgentBuilder(tools=[])
        assert builder.tools == []
    
    def test_build_graph(self, mock_tools):
        """Test building the graph."""
        with patch('agent.langgraph_builder.StateGraph') as mock_graph_class:
            with patch('agent.langgraph_builder.set_available_tools') as mock_set_tools:
                with patch('agent.langgraph_builder.call_model') as mock_call_model:
                    with patch('agent.langgraph_builder.should_continue') as mock_should_continue:
                        with patch('agent.langgraph_builder.ToolNode') as mock_tool_node:
                            with patch('agent.langgraph_builder.get_settings') as mock_settings:
                                mock_settings.return_value.checkpoint_db_path = "test.db"
                                
                                mock_graph = MagicMock()
                                mock_graph_instance = MagicMock()
                                mock_graph_instance.compile.return_value = mock_graph
                                mock_graph_class.return_value = mock_graph_instance
                                
                                builder = LangGraphAgentBuilder(tools=mock_tools)
                                result = builder.build()
                                
                                assert result is not None
                                mock_set_tools.assert_called_once_with(mock_tools)
    
    def test_build_graph_no_tools(self):
        """Test building graph with no tools."""
        with patch('agent.langgraph_builder.StateGraph') as mock_graph_class:
            with patch('agent.langgraph_builder.set_available_tools') as mock_set_tools:
                with patch('agent.langgraph_builder.get_settings') as mock_settings:
                    mock_settings.return_value.checkpoint_db_path = "test.db"
                    
                    mock_graph = MagicMock()
                    mock_graph_instance = MagicMock()
                    mock_graph_instance.compile.return_value = mock_graph
                    mock_graph_class.return_value = mock_graph_instance
                    
                    builder = LangGraphAgentBuilder(tools=[])
                    result = builder.build()
                    
                    assert result is not None
    
    def test_get_graph(self, mock_tools):
        """Test getting the compiled graph."""
        with patch('agent.langgraph_builder.StateGraph') as mock_graph_class:
            with patch('agent.langgraph_builder.set_available_tools'):
                with patch('agent.langgraph_builder.get_settings') as mock_settings:
                    mock_settings.return_value.checkpoint_db_path = "test.db"
                    
                    mock_graph = MagicMock()
                    mock_graph_instance = MagicMock()
                    mock_graph_instance.compile.return_value = mock_graph
                    mock_graph_class.return_value = mock_graph_instance
                    
                    builder = LangGraphAgentBuilder(tools=mock_tools)
                    builder.build()
                    
                    result = builder.get_graph()
                    assert result == mock_graph
    
    def test_get_graph_not_built(self):
        """Test getting graph before building."""
        builder = LangGraphAgentBuilder(tools=[])
        result = builder.get_graph()
        assert result is None

```

`test_repository/agent/test_state_converter.py`

```python
"""Tests for state converter module."""

import pytest
from unittest.mock import Mock, MagicMock
import sys
from pathlib import Path

backend_path = Path(__file__).parent.parent.parent / "backend"
sys.path.insert(0, str(backend_path))

from agent.state_converter import (
    normalize_message_content,
    convert_to_langchain_messages,
    convert_from_langchain_messages,
    convert_langgraph_state_to_agent
)
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage


class TestStateConverter:
    """Test suite for state converter."""
    
    def test_normalize_message_content_string(self):
        """Test normalizing string content."""
        content = "Simple string content"
        result = normalize_message_content(content)
        assert result == "Simple string content"
        assert isinstance(result, str)
    
    def test_normalize_message_content_list(self):
        """Test normalizing list content (Gemini format)."""
        content = [
            {"type": "text", "text": "First part"},
            {"type": "text", "text": "Second part"}
        ]
        result = normalize_message_content(content)
        assert "First part" in result
        assert "Second part" in result
    
    def test_normalize_message_content_list_simple(self):
        """Test normalizing simple list content."""
        content = ["part1", "part2"]
        result = normalize_message_content(content)
        assert "part1" in result
        assert "part2" in result
    
    def test_normalize_message_content_none(self):
        """Test normalizing None content."""
        result = normalize_message_content(None)
        assert result == ""
    
    def test_normalize_message_content_dict_with_text(self):
        """Test normalizing dict with text key."""
        content = {"text": "Dict text content"}
        result = normalize_message_content(content)
        assert "Dict text content" in result
    
    def test_convert_to_langchain_messages(self):
        """Test converting custom messages to LangChain messages."""
        messages = [
            {"role": "user", "content": "Hello"},
            {"role": "assistant", "content": "Hi there"},
            {"role": "system", "content": "System message"}
        ]
        
        langchain_messages = convert_to_langchain_messages(messages)
        
        assert len(langchain_messages) == 3
        assert isinstance(langchain_messages[0], HumanMessage)
        assert isinstance(langchain_messages[1], AIMessage)
        assert isinstance(langchain_messages[2], SystemMessage)
    
    def test_convert_to_langchain_messages_tool(self):
        """Test converting tool messages."""
        messages = [
            {"role": "tool", "content": "Tool result", "tool_call_id": "tc_1"}
        ]
        
        langchain_messages = convert_to_langchain_messages(messages)
        
        assert len(langchain_messages) == 1
        assert isinstance(langchain_messages[0], ToolMessage)
        assert langchain_messages[0].tool_call_id == "tc_1"
    
    def test_convert_from_langchain_messages(self):
        """Test converting LangChain messages to custom format."""
        langchain_messages = [
            HumanMessage(content="Hello"),
            AIMessage(content="Hi there")
        ]
        
        custom_messages = convert_from_langchain_messages(langchain_messages)
        
        assert len(custom_messages) == 2
        assert custom_messages[0]["role"] == "user"
        assert custom_messages[1]["role"] == "assistant"
    
    def test_convert_from_langchain_messages_with_tool_calls(self):
        """Test converting AIMessage with tool calls."""
        mock_tool_call = MagicMock()
        mock_tool_call.name = "test_tool"
        mock_tool_call.id = "tc_1"
        mock_tool_call.args = {"param": "value"}
        
        ai_message = AIMessage(content="", tool_calls=[mock_tool_call])
        langchain_messages = [ai_message]
        
        custom_messages = convert_from_langchain_messages(langchain_messages)
        
        assert len(custom_messages) == 1
        assert custom_messages[0]["role"] == "assistant"
        assert "tool_calls" in custom_messages[0]
        assert len(custom_messages[0]["tool_calls"]) == 1
    
    def test_convert_langgraph_state_to_agent(self):
        """Test converting LangGraph state to agent state."""
        langgraph_state = {
            "messages": [
                HumanMessage(content="Hello"),
                AIMessage(content="Hi")
            ],
            "tool_calls": [],
            "tool_results": [],
            "request_id": "test_123",
            "session_id": "session_123",
            "current_step": 1,
            "error": None,
            "finished": True,
            "prompt_version": "v1",
            "model_name": "gemini-2.5-flash"
        }
        
        agent_state = convert_langgraph_state_to_agent(langgraph_state)
        
        assert agent_state["request_id"] == "test_123"
        assert agent_state["session_id"] == "session_123"
        assert agent_state["current_step"] == 1
        assert agent_state["finished"] is True
        assert len(agent_state["messages"]) == 2
    
    def test_convert_langgraph_state_to_agent_with_tool_calls(self):
        """Test converting state with tool calls."""
        mock_tool_call = MagicMock()
        mock_tool_call.name = "test_tool"
        mock_tool_call.id = "tc_1"
        mock_tool_call.args = {"param": "value"}
        
        ai_message = AIMessage(content="", tool_calls=[mock_tool_call])
        tool_message = ToolMessage(content="Result", tool_call_id="tc_1")
        
        langgraph_state = {
            "messages": [ai_message, tool_message],
            "tool_calls": [],
            "tool_results": [],
            "request_id": "test_123",
            "session_id": None,
            "current_step": 0,
            "error": None,
            "finished": False,
            "prompt_version": "v1",
            "model_name": "gemini-2.5-flash"
        }
        
        agent_state = convert_langgraph_state_to_agent(langgraph_state)
        
        assert len(agent_state["tool_calls"]) > 0
        assert len(agent_state["tool_results"]) > 0

```

`test_repository/agent/test_tool_converter.py`

```python
"""Tests for tool converter module."""

import pytest
from unittest.mock import Mock, MagicMock, AsyncMock
import sys
from pathlib import Path

backend_path = Path(__file__).parent.parent.parent / "backend"
sys.path.insert(0, str(backend_path))

from agent.tool_converter import (
    json_schema_to_pydantic,
    mcp_tool_to_langchain,
    convert_mcp_tools_to_langchain
)


class TestToolConverter:
    """Test suite for tool converter."""
    
    @pytest.fixture
    def sample_json_schema(self):
        """Sample JSON schema for testing."""
        return {
            "type": "object",
            "properties": {
                "param1": {
                    "type": "string",
                    "description": "First parameter"
                },
                "param2": {
                    "type": "integer",
                    "description": "Second parameter",
                    "default": 10
                },
                "param3": {
                    "type": "boolean",
                    "description": "Third parameter"
                }
            },
            "required": ["param1"]
        }
    
    @pytest.fixture
    def sample_mcp_tool(self):
        """Sample MCP tool for testing."""
        tool = MagicMock()
        tool.name = "test_tool"
        tool.description = "Test tool description"
        tool.inputSchema = {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "Search query"
                }
            },
            "required": ["query"]
        }
        return tool
    
    def test_json_schema_to_pydantic_string(self, sample_json_schema):
        """Test JSON schema to Pydantic conversion for string type."""
        model = json_schema_to_pydantic(sample_json_schema)
        
        # Test that model can be instantiated
        instance = model(param1="test")
        assert instance.param1 == "test"
        assert instance.param2 == 10  # Default value
        assert instance.param3 is None  # Optional field
    
    def test_json_schema_to_pydantic_integer(self):
        """Test JSON schema to Pydantic conversion for integer type."""
        schema = {
            "type": "object",
            "properties": {
                "count": {
                    "type": "integer",
                    "description": "Count value"
                }
            },
            "required": ["count"]
        }
        
        model = json_schema_to_pydantic(schema)
        instance = model(count=5)
        assert instance.count == 5
        assert isinstance(instance.count, int)
    
    def test_json_schema_to_pydantic_array(self):
        """Test JSON schema to Pydantic conversion for array type."""
        schema = {
            "type": "object",
            "properties": {
                "items": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of items"
                }
            },
            "required": ["items"]
        }
        
        model = json_schema_to_pydantic(schema)
        instance = model(items=["item1", "item2"])
        assert instance.items == ["item1", "item2"]
        assert isinstance(instance.items, list)
    
    def test_json_schema_to_pydantic_empty(self):
        """Test JSON schema to Pydantic conversion for empty schema."""
        schema = {
            "type": "object",
            "properties": {}
        }
        
        model = json_schema_to_pydantic(schema)
        instance = model()
        assert instance is not None
    
    @pytest.mark.asyncio
    async def test_mcp_tool_to_langchain(self, sample_mcp_tool):
        """Test MCP tool to LangChain conversion."""
        async def mock_tool_executor(server_name, tool_name, arguments):
            return {
                "result": '{"status": "success"}',
                "isError": False
            }
        
        tool = mcp_tool_to_langchain(
            sample_mcp_tool,
            "test_server",
            mock_tool_executor
        )
        
        assert tool.name == "test_server_test_tool"
        assert tool.description == "Test tool description"
        assert tool.args_schema is not None
        
        # Test tool execution
        result = await tool.ainvoke(query="test query")
        assert "status" in result or "success" in result
    
    @pytest.mark.asyncio
    async def test_mcp_tool_to_langchain_error_handling(self, sample_mcp_tool):
        """Test MCP tool to LangChain conversion with error handling."""
        async def mock_tool_executor(server_name, tool_name, arguments):
            return {
                "result": "",
                "isError": True,
                "error": "Test error"
            }
        
        tool = mcp_tool_to_langchain(
            sample_mcp_tool,
            "test_server",
            mock_tool_executor
        )
        
        result = await tool.ainvoke(query="test query")
        assert "Error" in result
    
    def test_convert_mcp_tools_to_langchain(self, sample_mcp_tool):
        """Test conversion of multiple MCP tools."""
        async def mock_tool_executor(server_name, tool_name, arguments):
            return {"result": '{"status": "success"}', "isError": False}
        
        mcp_tools = {
            "server1": [sample_mcp_tool],
            "server2": [sample_mcp_tool]
        }
        
        langchain_tools = convert_mcp_tools_to_langchain(
            mcp_tools,
            mock_tool_executor
        )
        
        assert len(langchain_tools) == 2
        assert all(tool.name.startswith("server") for tool in langchain_tools)
    
    def test_convert_mcp_tools_to_langchain_empty(self):
        """Test conversion with empty tool list."""
        async def mock_tool_executor(server_name, tool_name, arguments):
            return {"result": '{}', "isError": False}
        
        mcp_tools = {}
        
        langchain_tools = convert_mcp_tools_to_langchain(
            mcp_tools,
            mock_tool_executor
        )
        
        assert len(langchain_tools) == 0
    
    def test_convert_mcp_tools_to_langchain_conversion_error(self):
        """Test conversion with tool conversion error."""
        async def mock_tool_executor(server_name, tool_name, arguments):
            return {"result": '{}', "isError": False}
        
        # Create a tool with invalid schema
        bad_tool = MagicMock()
        bad_tool.name = "bad_tool"
        bad_tool.description = "Bad tool"
        bad_tool.inputSchema = "invalid_schema"  # Not a dict
        
        mcp_tools = {
            "server1": [bad_tool]
        }
        
        # Should handle error gracefully
        langchain_tools = convert_mcp_tools_to_langchain(
            mcp_tools,
            mock_tool_executor
        )
        
        # May return empty list or handle error
        assert isinstance(langchain_tools, list)

```

`test_repository/agent/__init__.py`

```python
"""Agent module tests."""

```

`test_repository/analytics/test_aggregator.py`

```python
"""Tests for analytics aggregator."""

import pytest
from unittest.mock import Mock, AsyncMock, MagicMock, patch
import sys
from pathlib import Path

backend_path = Path(__file__).parent.parent.parent / "backend"
sys.path.insert(0, str(backend_path))


class TestAnalyticsAggregator:
    """Test suite for AnalyticsAggregator."""
    
    @pytest.fixture
    def aggregator(self):
        """Create AnalyticsAggregator instance."""
        from analytics.aggregator import AnalyticsAggregator
        return AnalyticsAggregator()
    
    @pytest.mark.asyncio
    async def test_get_overview_stats(self, aggregator):
        """Test getting overview statistics."""
        with patch('analytics.aggregator.get_inference_logger') as mock_logger:
            mock_logger_instance = MagicMock()
            mock_logger_instance.get_logs = AsyncMock(return_value=[
                {
                    "status_code": 200,
                    "duration": 1.5,
                    "path": "/api/v1/chat",
                    "method": "POST",
                    "tool_calls": [{"tool_name": "test_tool"}],
                    "iterations": 2
                }
            ])
            mock_logger.return_value = mock_logger_instance
            
            stats = await aggregator.get_overview_stats()
            
            assert "total_requests" in stats
            assert "successful_requests" in stats
            assert "avg_duration" in stats
    
    @pytest.mark.asyncio
    async def test_get_tool_usage_stats(self, aggregator):
        """Test getting tool usage statistics."""
        with patch('analytics.aggregator.get_inference_logger') as mock_logger:
            mock_logger_instance = MagicMock()
            mock_logger_instance.get_logs = AsyncMock(return_value=[
                {
                    "tool_calls": [{"tool_name": "test_tool"}]
                }
            ])
            mock_logger.return_value = mock_logger_instance
            
            stats = await aggregator.get_tool_usage_stats()
            
            assert "tools" in stats
            assert "total_tool_calls" in stats

```

`test_repository/analytics/__init__.py`

```python
"""Analytics module tests."""

```

`test_repository/api/test_routes.py`

```python
"""Tests for API routes module."""

import pytest
from unittest.mock import Mock, AsyncMock, MagicMock, patch
from fastapi.testclient import TestClient
import sys
from pathlib import Path

backend_path = Path(__file__).parent.parent.parent / "backend"
sys.path.insert(0, str(backend_path))

from api.main import create_app


class TestAPIRoutes:
    """Test suite for API routes."""
    
    @pytest.fixture
    def client(self):
        """Create test client."""
        app = create_app()
        return TestClient(app)
    
    def test_root_endpoint(self, client):
        """Test root endpoint."""
        response = client.get("/")
        assert response.status_code == 200
        data = response.json()
        assert "name" in data
        assert "version" in data
        assert "status" in data
    
    def test_health_endpoint(self, client):
        """Test health check endpoint."""
        response = client.get("/health")
        assert response.status_code == 200
        data = response.json()
        assert data["status"] == "healthy"
    
    @pytest.mark.asyncio
    async def test_chat_endpoint(self, client):
        """Test chat endpoint."""
        with patch('api.routes.get_agent') as mock_get_agent:
            mock_agent = MagicMock()
            mock_agent.invoke = AsyncMock(return_value={
                "messages": [
                    {"role": "assistant", "content": "Test response"}
                ],
                "request_id": "test_123",
                "tool_calls": [],
                "tool_results": [],
                "current_step": 1,
                "session_id": None,
                "prompt_version": "v1",
                "model_name": "gemini-2.5-flash",
                "error": None
            })
            mock_get_agent.return_value = mock_agent
            
            response = client.post(
                "/api/v1/chat",
                json={
                    "message": "Hello",
                    "session_id": None,
                    "max_iterations": 10
                }
            )
            
            assert response.status_code == 200
            data = response.json()
            assert "response" in data
            assert "request_id" in data
    
    @pytest.mark.asyncio
    async def test_chat_endpoint_with_session(self, client):
        """Test chat endpoint with session ID."""
        with patch('api.routes.get_agent') as mock_get_agent:
            mock_agent = MagicMock()
            mock_agent.invoke = AsyncMock(return_value={
                "messages": [
                    {"role": "assistant", "content": "Test response"}
                ],
                "request_id": "test_123",
                "session_id": "session_123",
                "tool_calls": [],
                "tool_results": [],
                "current_step": 1,
                "prompt_version": "v1",
                "model_name": "gemini-2.5-flash",
                "error": None
            })
            mock_get_agent.return_value = mock_agent
            
            response = client.post(
                "/api/v1/chat",
                json={
                    "message": "Hello",
                    "session_id": "session_123",
                    "max_iterations": 10
                }
            )
            
            assert response.status_code == 200
            data = response.json()
            assert data.get("session_id") == "session_123"
    
    def test_chat_endpoint_invalid_request(self, client):
        """Test chat endpoint with invalid request."""
        response = client.post(
            "/api/v1/chat",
            json={}  # Missing required fields
        )
        
        assert response.status_code == 422  # Validation error
    
    @pytest.mark.asyncio
    async def test_chat_stream_endpoint(self, client):
        """Test chat stream endpoint."""
        with patch('api.routes.get_agent') as mock_get_agent:
            mock_agent = MagicMock()
            
            async def mock_stream(*args, **kwargs):
                yield {"stage": "initializing", "data": {}, "timestamp": "2024-01-01T00:00:00"}
                yield {"stage": "completed", "data": {"response": "Test"}, "timestamp": "2024-01-01T00:00:01"}
            
            mock_agent.stream_invoke = mock_stream
            mock_get_agent.return_value = mock_agent
            
            response = client.post(
                "/api/v1/chat/stream",
                json={
                    "message": "Hello",
                    "max_iterations": 10
                }
            )
            
            assert response.status_code == 200
            assert response.headers["content-type"] == "text/event-stream; charset=utf-8"
    
    @pytest.mark.asyncio
    async def test_tools_endpoint(self, client):
        """Test tools listing endpoint."""
        with patch('api.routes.MCPSDKClient') as mock_client_class:
            mock_client = MagicMock()
            mock_client.__aenter__ = AsyncMock(return_value=mock_client)
            mock_client.__aexit__ = AsyncMock(return_value=None)
            mock_client.discover_all_tools = AsyncMock(return_value={
                "catalog": [],
                "sql_query": [],
                "vector_search": []
            })
            mock_client_class.return_value = mock_client
            
            response = client.get("/api/v1/tools")
            
            assert response.status_code == 200
            data = response.json()
            assert "tools" in data
            assert "count" in data
    
    @pytest.mark.asyncio
    async def test_health_check_endpoint(self, client):
        """Test health check endpoint with system info."""
        response = client.get("/api/v1/health")
        
        assert response.status_code == 200
        data = response.json()
        assert "status" in data
        assert "llm_provider" in data
        assert "mcp_servers" in data
    
    @pytest.mark.asyncio
    async def test_status_endpoint(self, client):
        """Test status endpoint."""
        response = client.get("/api/v1/status")
        
        assert response.status_code == 200
        data = response.json()
        assert "api_version" in data
        assert "llm_provider" in data

```

`test_repository/api/__init__.py`

```python
"""API module tests."""

```

`test_repository/config/test_settings.py`

```python
"""Tests for settings module."""

import pytest
from unittest.mock import patch, MagicMock
import os
import sys
from pathlib import Path

backend_path = Path(__file__).parent.parent.parent / "backend"
sys.path.insert(0, str(backend_path))

from config.settings import Settings, get_settings, clear_settings_cache


class TestSettings:
    """Test suite for Settings."""
    
    def test_settings_default_values(self):
        """Test default settings values."""
        with patch.dict(os.environ, {}, clear=True):
            settings = Settings()
            
            assert settings.llm_provider == "gemini"
            assert settings.embedding_provider == "ollama"
            assert settings.catalog_mcp_port == 7001
            assert settings.vector_mcp_port == 7002
            assert settings.sql_mcp_port == 7003
    
    def test_settings_from_env(self):
        """Test loading settings from environment variables."""
        env_vars = {
            "LLM_PROVIDER": "ollama",
            "EMBEDDING_PROVIDER": "gemini",
            "GEMINI_API_KEY": "test_key",
            "CATALOG_MCP_PORT": "8001"
        }
        
        with patch.dict(os.environ, env_vars, clear=False):
            settings = Settings()
            
            assert settings.llm_provider == "ollama"
            assert settings.embedding_provider == "gemini"
            assert settings.gemini_api_key == "test_key"
            assert settings.catalog_mcp_port == 8001
    
    def test_settings_validate_llm_provider(self):
        """Test LLM provider validation."""
        with patch.dict(os.environ, {"LLM_PROVIDER": "invalid"}, clear=False):
            with pytest.raises(ValueError, match="Invalid LLM provider"):
                Settings()
    
    def test_settings_validate_embedding_provider(self):
        """Test embedding provider validation."""
        with patch.dict(os.environ, {"EMBEDDING_PROVIDER": "invalid"}, clear=False):
            with pytest.raises(ValueError, match="Invalid embedding provider"):
                Settings()
    
    def test_get_settings_singleton(self):
        """Test get_settings returns singleton."""
        clear_settings_cache()
        settings1 = get_settings()
        settings2 = get_settings()
        
        # Should be the same instance (cached)
        assert settings1 is settings2
    
    def test_clear_settings_cache(self):
        """Test clearing settings cache."""
        clear_settings_cache()
        settings1 = get_settings()
        
        clear_settings_cache()
        settings2 = get_settings()
        
        # Should be different instances after clearing cache
        # (Note: In practice, they might be the same if no env changes)
        assert isinstance(settings1, Settings)
        assert isinstance(settings2, Settings)
    
    def test_settings_optional_fields(self):
        """Test optional settings fields."""
        settings = Settings()
        
        # Optional fields should have defaults or be None
        assert settings.llm_top_p is None or isinstance(settings.llm_top_p, (float, type(None)))
        assert settings.api_key is None or isinstance(settings.api_key, str)

```

`test_repository/config/__init__.py`

```python
"""Config module tests."""

```

`test_repository/conftest.py`

```python
"""Pytest configuration and shared fixtures for test suite."""

import pytest
import asyncio
import os
import sys
from pathlib import Path
from typing import AsyncGenerator, Generator
from unittest.mock import Mock, AsyncMock, MagicMock, patch
import tempfile
import shutil

# Add backend to path for imports
backend_path = Path(__file__).parent.parent / "backend"
sys.path.insert(0, str(backend_path))

# Test configuration
TEST_DB_PATH = "test_data/test.db"
TEST_VECTOR_STORE_PATH = "test_data/vector_store"
TEST_CHROMADB_PATH = "test_data/chromadb"
TEST_CHECKPOINT_DB_PATH = "test_data/checkpoints.db"
TEST_INFERENCE_LOG_DB_PATH = "test_data/inference_logs.db"


@pytest.fixture(scope="session")
def event_loop():
    """Create an instance of the default event loop for the test session."""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


@pytest.fixture(autouse=True)
def setup_test_environment(monkeypatch, tmp_path):
    """Set up test environment variables and paths."""
    # Set test database paths
    test_data_dir = tmp_path / "test_data"
    test_data_dir.mkdir()
    
    monkeypatch.setenv("DATABASE_PATH", str(test_data_dir / "test.db"))
    monkeypatch.setenv("VECTOR_STORE_PATH", str(test_data_dir / "vector_store"))
    monkeypatch.setenv("CHROMADB_DATA_PATH", str(test_data_dir / "chromadb"))
    monkeypatch.setenv("CHECKPOINT_DB_PATH", str(test_data_dir / "checkpoints.db"))
    monkeypatch.setenv("INFERENCE_LOG_DB_PATH", str(test_data_dir / "inference_logs.db"))
    monkeypatch.setenv("LLM_PROVIDER", "gemini")
    monkeypatch.setenv("EMBEDDING_PROVIDER", "ollama")
    monkeypatch.setenv("GEMINI_API_KEY", "test_gemini_key")
    monkeypatch.setenv("OLLAMA_BASE_URL", "http://localhost:11434")
    
    # Clean up after tests
    yield
    if test_data_dir.exists():
        shutil.rmtree(test_data_dir, ignore_errors=True)


@pytest.fixture
def mock_settings():
    """Create a mock settings object."""
    from unittest.mock import MagicMock
    
    settings = MagicMock()
    settings.llm_provider = "gemini"
    settings.embedding_provider = "ollama"
    settings.gemini_api_key = "test_gemini_key"
    settings.gemini_model = "gemini-2.5-flash"
    settings.ollama_base_url = "http://localhost:11434"
    settings.ollama_chat_model = "llama3"
    settings.ollama_embedding_model = "nomic-embed-text"
    settings.ollama_timeout = 300
    settings.llm_temperature = 0.7
    settings.llm_max_tokens = 500
    settings.database_path = TEST_DB_PATH
    settings.vector_store_path = TEST_VECTOR_STORE_PATH
    settings.chromadb_data_path = TEST_CHROMADB_PATH
    settings.checkpoint_db_path = TEST_CHECKPOINT_DB_PATH
    settings.inference_log_db_path = TEST_INFERENCE_LOG_DB_PATH
    settings.catalog_mcp_port = 7001
    settings.vector_mcp_port = 7002
    settings.sql_mcp_port = 7003
    settings.max_parallel_mcp_calls = 5
    settings.mcp_call_timeout = 60
    settings.mcp_connect_timeout = 10
    settings.api_port = 8000
    settings.api_host = "0.0.0.0"
    settings.enable_mlflow_tracking = False
    settings.enable_graph_visualization = False
    settings.mlflow_tracking_uri = "http://localhost:5000"
    settings.mlflow_experiment_name = "test_experiments"
    
    return settings


@pytest.fixture
def mock_mcp_tool():
    """Create a mock MCP tool."""
    from unittest.mock import MagicMock
    
    tool = MagicMock()
    tool.name = "test_tool"
    tool.description = "Test tool description"
    tool.inputSchema = {
        "type": "object",
        "properties": {
            "param1": {
                "type": "string",
                "description": "Test parameter"
            }
        },
        "required": ["param1"]
    }
    return tool


@pytest.fixture
def mock_langchain_tool():
    """Create a mock LangChain StructuredTool."""
    from unittest.mock import MagicMock
    
    tool = MagicMock()
    tool.name = "test_tool"
    tool.description = "Test tool description"
    tool.args_schema = MagicMock()
    return tool


@pytest.fixture
def sample_messages():
    """Sample messages for testing."""
    return [
        {
            "role": "user",
            "content": "Hello, how are you?"
        },
        {
            "role": "assistant",
            "content": "I'm doing well, thank you!"
        }
    ]


@pytest.fixture
def sample_langchain_messages():
    """Sample LangChain messages for testing."""
    from langchain_core.messages import HumanMessage, AIMessage
    
    return [
        HumanMessage(content="Hello, how are you?"),
        AIMessage(content="I'm doing well, thank you!")
    ]


@pytest.fixture
def mock_llm_response():
    """Create a mock LLM response."""
    from unittest.mock import MagicMock
    
    response = MagicMock()
    response.content = "Test response"
    response.tool_calls = []
    return response


@pytest.fixture
def mock_agent_state():
    """Create a mock agent state."""
    return {
        "messages": [
            {"role": "user", "content": "Test message"}
        ],
        "tool_calls": [],
        "tool_results": [],
        "request_id": "test_request_123",
        "session_id": "test_session_123",
        "current_step": 0,
        "error": None,
        "finished": False,
        "prompt_version": "v1",
        "model_name": "gemini-2.5-flash"
    }


@pytest.fixture
def mock_mcp_client():
    """Create a mock MCP SDK client."""
    from unittest.mock import AsyncMock, MagicMock
    
    client = MagicMock()
    client.initialize = AsyncMock(return_value=None)
    client.discover_all_tools = AsyncMock(return_value={
        "catalog": [],
        "sql_query": [],
        "vector_search": []
    })
    client.call_tool = AsyncMock(return_value={
        "result": '{"status": "success", "data": {}}',
        "isError": False
    })
    client.close = AsyncMock(return_value=None)
    client.list_tools = AsyncMock(return_value=[])
    
    return client


@pytest.fixture
def temp_dir(tmp_path):
    """Create a temporary directory for tests."""
    return tmp_path


@pytest.fixture
def cleanup_test_data():
    """Cleanup test data after tests."""
    yield
    # Cleanup will be handled by setup_test_environment fixture

```

`test_repository/inference_logging/test_logger.py`

```python
"""Tests for inference logger."""

import pytest
from unittest.mock import Mock, AsyncMock, MagicMock, patch
import sys
from pathlib import Path

backend_path = Path(__file__).parent.parent.parent / "backend"
sys.path.insert(0, str(backend_path))


class TestInferenceLogger:
    """Test suite for inference logger."""
    
    @pytest.fixture
    def logger(self):
        """Create inference logger instance."""
        from inference_logging.logger import InferenceLogger
        return InferenceLogger(db_path=":memory:")  # Use in-memory database for tests
    
    @pytest.mark.asyncio
    async def test_log_request(self, logger):
        """Test logging a request."""
        await logger.log_request(
            request_id="test_123",
            method="POST",
            path="/api/v1/chat",
            status_code=200,
            duration=1.5
        )
        
        # Verify log was created
        log = await logger.get_log("test_123")
        assert log is not None
        assert log["request_id"] == "test_123"
        assert log["status_code"] == 200
    
    @pytest.mark.asyncio
    async def test_get_logs(self, logger):
        """Test getting logs."""
        # Create some test logs
        for i in range(5):
            await logger.log_request(
                request_id=f"test_{i}",
                method="POST",
                path="/api/v1/chat",
                status_code=200,
                duration=1.0 + i
            )
        
        logs = await logger.get_logs(limit=10, offset=0)
        
        assert len(logs) == 5
        assert all(log["request_id"].startswith("test_") for log in logs)
    
    @pytest.mark.asyncio
    async def test_get_log_not_found(self, logger):
        """Test getting non-existent log."""
        log = await logger.get_log("nonexistent")
        assert log is None

```

`test_repository/inference_logging/__init__.py`

```python
"""Inference logging module tests."""

```

`test_repository/integration/test_agent_workflow.py`

```python
"""Integration tests for agent workflow."""

import pytest
from unittest.mock import Mock, AsyncMock, MagicMock, patch
import sys
from pathlib import Path

backend_path = Path(__file__).parent.parent.parent / "backend"
sys.path.insert(0, str(backend_path))


class TestAgentWorkflow:
    """Integration tests for agent workflow."""
    
    @pytest.mark.asyncio
    async def test_agent_initialization_workflow(self):
        """Test complete agent initialization workflow."""
        with patch('agent.langgraph_agent.MCPSDKClient') as mock_client_class:
            with patch('agent.langgraph_agent.convert_mcp_tools_to_langchain') as mock_convert:
                with patch('agent.langgraph_agent.load_system_prompt') as mock_prompt:
                    with patch('agent.langgraph_agent.LangGraphAgentBuilder') as mock_builder:
                        # Setup mocks
                        mock_client = MagicMock()
                        mock_client.initialize = AsyncMock()
                        mock_client.discover_all_tools = AsyncMock(return_value={
                            "catalog": [],
                            "sql_query": [],
                            "vector_search": []
                        })
                        mock_client_class.return_value = mock_client
                        
                        mock_convert.return_value = []
                        mock_prompt.return_value = "System prompt"
                        
                        mock_graph = MagicMock()
                        mock_builder_instance = MagicMock()
                        mock_builder_instance.build.return_value = mock_graph
                        mock_builder.return_value = mock_builder_instance
                        
                        # Test initialization
                        from agent.langgraph_agent import LangGraphAgent
                        agent = LangGraphAgent()
                        await agent.initialize()
                        
                        # Verify workflow
                        assert agent._initialized is True
                        mock_client.initialize.assert_called_once()
                        mock_client.discover_all_tools.assert_called_once()
                        mock_convert.assert_called_once()
                        mock_builder.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_agent_invocation_workflow(self):
        """Test complete agent invocation workflow."""
        with patch('agent.langgraph_agent.MCPSDKClient') as mock_client_class:
            with patch('agent.langgraph_agent.convert_mcp_tools_to_langchain') as mock_convert:
                with patch('agent.langgraph_agent.load_system_prompt') as mock_prompt:
                    with patch('agent.langgraph_agent.LangGraphAgentBuilder') as mock_builder:
                        with patch('agent.langgraph_agent.create_langgraph_initial_state') as mock_init_state:
                            with patch('agent.langgraph_agent.convert_langgraph_state_to_agent') as mock_convert_state:
                                # Setup mocks
                                mock_client = MagicMock()
                                mock_client.initialize = AsyncMock()
                                mock_client.discover_all_tools = AsyncMock(return_value={
                                    "catalog": [],
                                    "sql_query": [],
                                    "vector_search": []
                                })
                                mock_client_class.return_value = mock_client
                                
                                mock_convert.return_value = []
                                mock_prompt.return_value = "System prompt"
                                
                                mock_graph = MagicMock()
                                mock_final_state = {
                                    "messages": [{"role": "assistant", "content": "Response"}],
                                    "request_id": "test_123",
                                    "tool_calls": [],
                                    "tool_results": [],
                                    "current_step": 1,
                                    "finished": True,
                                    "error": None,
                                    "prompt_version": "v1",
                                    "model_name": "gemini-2.5-flash"
                                }
                                mock_graph.ainvoke = AsyncMock(return_value=mock_final_state)
                                
                                mock_builder_instance = MagicMock()
                                mock_builder_instance.build.return_value = mock_graph
                                mock_builder.return_value = mock_builder_instance
                                
                                mock_init_state.return_value = mock_final_state
                                mock_convert_state.return_value = mock_final_state
                                
                                # Test invocation
                                from agent.langgraph_agent import LangGraphAgent
                                from agent.langgraph_nodes import get_available_tools
                                
                                with patch('agent.langgraph_nodes.get_available_tools', return_value=[]):
                                    agent = LangGraphAgent()
                                    await agent.initialize()
                                    
                                    result = await agent.invoke(
                                        user_message="Test message",
                                        request_id="test_123"
                                    )
                                    
                                    # Verify workflow
                                    assert result is not None
                                    assert "messages" in result
                                    mock_graph.ainvoke.assert_called_once()

```

`test_repository/integration/__init__.py`

```python
"""Integration tests."""

```

`test_repository/llm/test_factory.py`

```python
"""Tests for LLM factory module."""

import pytest
from unittest.mock import Mock, MagicMock, patch
import sys
from pathlib import Path

backend_path = Path(__file__).parent.parent.parent / "backend"
sys.path.insert(0, str(backend_path))

from llm.factory import LLMFactory
from config.settings import Settings


class TestLLMFactory:
    """Test suite for LLMFactory."""
    
    @pytest.fixture
    def gemini_settings(self):
        """Create settings for Gemini provider."""
        settings = MagicMock(spec=Settings)
        settings.llm_provider = "gemini"
        settings.gemini_api_key = "test_gemini_key"
        settings.gemini_model = "gemini-2.5-flash"
        return settings
    
    @pytest.fixture
    def ollama_settings(self):
        """Create settings for Ollama provider."""
        settings = MagicMock(spec=Settings)
        settings.llm_provider = "ollama"
        settings.ollama_base_url = "http://localhost:11434"
        settings.ollama_chat_model = "llama3"
        settings.ollama_embedding_model = "nomic-embed-text"
        settings.ollama_timeout = 300
        return settings
    
    def test_create_provider_gemini(self, gemini_settings):
        """Test creating Gemini provider."""
        with patch('llm.factory.GeminiClient') as mock_client:
            mock_instance = MagicMock()
            mock_client.return_value = mock_instance
            
            provider = LLMFactory.create_provider(gemini_settings)
            
            assert provider is not None
            mock_client.assert_called_once_with(
                api_key="test_gemini_key",
                model="gemini-2.5-flash"
            )
    
    def test_create_provider_gemini_missing_key(self, gemini_settings):
        """Test creating Gemini provider without API key."""
        gemini_settings.gemini_api_key = None
        
        with pytest.raises(ValueError, match="Gemini API key is required"):
            LLMFactory.create_provider(gemini_settings)
    
    def test_create_provider_ollama(self, ollama_settings):
        """Test creating Ollama provider."""
        with patch('llm.factory.OllamaClient') as mock_client:
            mock_instance = MagicMock()
            mock_client.return_value = mock_instance
            
            provider = LLMFactory.create_provider(ollama_settings)
            
            assert provider is not None
            mock_client.assert_called_once()
    
    def test_create_provider_invalid(self):
        """Test creating provider with invalid name."""
        settings = MagicMock(spec=Settings)
        settings.llm_provider = "invalid_provider"
        
        with pytest.raises(ValueError, match="Unsupported LLM provider"):
            LLMFactory.create_provider(settings)
    
    def test_get_available_providers(self):
        """Test getting list of available providers."""
        providers = LLMFactory.get_available_providers()
        
        assert isinstance(providers, list)
        assert "gemini" in providers
        assert "ollama" in providers
        assert "openai" in providers
        assert "anthropic" in providers
    
    def test_create_embedding_provider_gemini(self, gemini_settings):
        """Test creating Gemini embedding provider."""
        gemini_settings.embedding_provider = "gemini"
        
        with patch('llm.factory.GeminiClient') as mock_client:
            mock_instance = MagicMock()
            mock_client.return_value = mock_instance
            
            provider = LLMFactory.create_embedding_provider(gemini_settings)
            
            assert provider is not None
            mock_client.assert_called_once()
    
    def test_create_embedding_provider_ollama(self, ollama_settings):
        """Test creating Ollama embedding provider."""
        ollama_settings.embedding_provider = "ollama"
        
        with patch('llm.factory.OllamaClient') as mock_client:
            mock_instance = MagicMock()
            mock_client.return_value = mock_instance
            
            provider = LLMFactory.create_embedding_provider(ollama_settings)
            
            assert provider is not None
            mock_client.assert_called_once()
    
    def test_create_embedding_provider_invalid(self):
        """Test creating embedding provider with invalid name."""
        settings = MagicMock(spec=Settings)
        settings.embedding_provider = "invalid_provider"
        
        with pytest.raises(ValueError, match="Unsupported embedding provider"):
            LLMFactory.create_embedding_provider(settings)

```

`test_repository/llm/__init__.py`

```python
"""LLM module tests."""

```

`test_repository/mcp_servers/test_catalog_server.py`

```python
"""Tests for catalog server."""

import pytest
from unittest.mock import Mock, AsyncMock, MagicMock, patch
import sys
from pathlib import Path

backend_path = Path(__file__).parent.parent.parent / "backend"
sys.path.insert(0, str(backend_path))


class TestCatalogServer:
    """Test suite for catalog server."""
    
    @pytest.fixture
    def mock_catalog_manager(self):
        """Create mock catalog manager."""
        manager = MagicMock()
        manager.list_tables = AsyncMock(return_value=["table1", "table2"])
        manager.describe_table = AsyncMock(return_value={
            "columns": [{"name": "id", "type": "INTEGER"}]
        })
        manager.get_table_row_count = AsyncMock(return_value=100)
        return manager
    
    @pytest.mark.asyncio
    async def test_list_tables_tool(self, mock_catalog_manager):
        """Test list_tables tool."""
        with patch('mcp_servers.catalog_server.tools.CatalogManager', return_value=mock_catalog_manager):
            from mcp_servers.catalog_server.tools import list_tables
            
            result = await list_tables()
            
            assert "tables" in result or "status" in result
            mock_catalog_manager.list_tables.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_describe_table_tool(self, mock_catalog_manager):
        """Test describe_table tool."""
        with patch('mcp_servers.catalog_server.tools.CatalogManager', return_value=mock_catalog_manager):
            from mcp_servers.catalog_server.tools import describe_table
            
            result = await describe_table(table_name="table1")
            
            assert "columns" in result or "status" in result
            mock_catalog_manager.describe_table.assert_called_once_with("table1")
    
    @pytest.mark.asyncio
    async def test_get_table_row_count_tool(self, mock_catalog_manager):
        """Test get_table_row_count tool."""
        with patch('mcp_servers.catalog_server.tools.CatalogManager', return_value=mock_catalog_manager):
            from mcp_servers.catalog_server.tools import get_table_row_count
            
            result = await get_table_row_count(table_name="table1")
            
            assert "row_count" in result or "status" in result
            mock_catalog_manager.get_table_row_count.assert_called_once_with("table1")

```

`test_repository/mcp_servers/__init__.py`

```python
"""MCP servers module tests."""

```

`test_repository/mlflow/test_tracking.py`

```python
"""Tests for MLflow tracking."""

import pytest
from unittest.mock import Mock, MagicMock, patch
import sys
from pathlib import Path

backend_path = Path(__file__).parent.parent.parent / "backend"
sys.path.insert(0, str(backend_path))


class TestMLflowTracking:
    """Test suite for MLflow tracking."""
    
    @pytest.fixture
    def mock_tracker(self):
        """Create mock MLflow tracker."""
        tracker = MagicMock()
        tracker.enabled = True
        tracker.tracking_uri = "http://localhost:5000"
        tracker.experiment_name = "test_experiments"
        return tracker
    
    def test_get_tracker(self, mock_tracker):
        """Test getting MLflow tracker."""
        with patch('mlflow.tracking.get_tracker', return_value=mock_tracker):
            from mlflow.tracking import get_tracker
            tracker = get_tracker()
            
            assert tracker is not None
            assert tracker.enabled is True
    
    def test_tracker_disabled_when_mlflow_not_available(self):
        """Test tracker is disabled when MLflow is not available."""
        with patch('mlflow.tracking.mlflow', None):
            with patch('mlflow.tracking.MlflowClient', None):
                from mlflow.tracking import get_tracker
                tracker = get_tracker()
                
                # Should return a tracker but with enabled=False
                assert tracker is not None

```

`test_repository/mlflow/__init__.py`

```python
"""MLflow module tests."""

```

`test_repository/README.md`

```markdown
# Test Repository

Comprehensive test suite for the Multi-Tool Orchestration system.

## Structure

```
test_repository/
├── agent/              # Agent module tests
│   ├── test_langgraph_agent.py
│   ├── test_tool_converter.py
│   ├── test_state_converter.py
│   └── test_langgraph_builder.py
├── api/                # API module tests
│   ├── test_routes.py
│   └── test_main.py
├── llm/                # LLM module tests
│   ├── test_factory.py
│   └── test_clients.py
├── mcp_servers/         # MCP server tests
│   ├── test_catalog_server.py
│   ├── test_sql_query_server.py
│   └── test_vector_search_server.py
├── config/              # Configuration tests
│   └── test_settings.py
├── analytics/           # Analytics tests
│   └── test_aggregator.py
├── mlflow/              # MLflow tests
│   ├── test_tracking.py
│   └── test_evaluation.py
├── inference_logging/   # Inference logging tests
│   └── test_logger.py
├── integration/         # Integration tests
│   ├── test_end_to_end.py
│   └── test_agent_workflow.py
├── fixtures/            # Test fixtures
│   └── sample_data.py
├── conftest.py          # Pytest configuration
└── README.md            # This file
```

## Running Tests

### Run all tests
```bash
pytest test_repository/
```

### Run specific test module
```bash
pytest test_repository/agent/test_langgraph_agent.py
```

### Run with coverage
```bash
pytest test_repository/ --cov=backend --cov-report=html
```

### Run with verbose output
```bash
pytest test_repository/ -v
```

## Test Categories

### Unit Tests
- **Agent Tests**: Test individual agent components (langgraph_agent, builder, nodes, converters)
- **LLM Tests**: Test LLM factory and client implementations
- **API Tests**: Test API routes and endpoints
- **MCP Server Tests**: Test MCP server implementations
- **Config Tests**: Test configuration loading and validation

### Integration Tests
- **End-to-End Tests**: Test complete workflows from API to agent execution
- **Agent Workflow Tests**: Test agent execution with real MCP servers

## Test Fixtures

Common fixtures are defined in `conftest.py`:
- `mock_settings`: Mock settings object
- `mock_mcp_client`: Mock MCP SDK client
- `mock_llm_response`: Mock LLM response
- `sample_messages`: Sample message data
- `temp_dir`: Temporary directory for test data

## Requirements

Tests require the following packages:
- pytest
- pytest-asyncio
- pytest-cov
- unittest.mock (built-in)

## Notes

- Tests use mocks extensively to avoid dependencies on external services
- Integration tests may require running MCP servers
- Some tests may need environment variables set (see `conftest.py`)

```

`test_repository/__init__.py`

```python
"""Test repository for multi-tool orchestration system.

This package contains comprehensive tests for all components of the system.
"""

```

